---
title: "Data processing_PRL_25.07.24"
format: html
editor: visual
---

# Package installation

```{r}
#Install packages and libraries:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot", "reshape2")

#Install packages if not already installed:
packages_to_install <- packages[!packages %in% installed.packages()]
if(length(packages_to_install)) install.packages(packages_to_install, dependencies = TRUE)

#If not already installed, install rethinking() separately:
#install.packages("rethinking", 
#                 repos=c(cran="https://cloud.r-project.org",
#                         rethinking="http://xcelab.net/R"))

#Rstan might need a bit of extra attention. If it doesn't install with the above code, remove any existing RStan via:
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")

#Set up compiler flags:
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXX17FLAGS=-O3 -march=native -mtune=native -fPIC",
    "CXX17=g++", # or clang++ but you may need a version postfix
    file = M, sep = "\n", append = TRUE)

#This code is for the development version of rstan- I've been told that this might function better than the up-to-date version:
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

#To verify your installation, you can run the RStan example/test model:
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

```{r}
#Once installed, load libraries using groundhog():
library(groundhog)

#Specify packages:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot")

#Load packages:
groundhog.library(packages, "2024-06-28", tolerate.R.version='4.4.0')
```

```{r}
#If that doesn't work, load packages manually:
library(rstan)
library(cmdstanr)
library(devtools)
library(rethinking) #Add download above
library(V8)
library(brms)
library(tidyverse) 
library(plotrix)
library(gridExtra)
library(tidybayes)
library(modelsummary)
library(bayesplot)
library(viridis)
library(patchwork)
library(reshape2)

#Add downloads above
library(missForest)
library(Hmisc)
library(mice)
#library(VIM)
library(rms)
```

# Data preparation

## Load data

```{r}
#Create a variable containing the merged summary data:
df_sum <- bind_rows(read.csv("ASC_data_summary_18.12.24.csv") %>% mutate(groupId = as.character(groupId)), 
                          read.csv("ASC_data_summary_VS_18.12.24.csv") %>% mutate(groupId = as.character(groupId)),
                    read.csv("NT_data_summary_07.01.25.csv") %>% mutate(groupId = as.character(groupId)), 
                          read.csv("NT_data_summary_VS_07.01.25.csv") %>% mutate(groupId = as.character(groupId)))


#Convert subject IDs for NT data to character:
df_sum$subjectId <- as.character(df_sum$subjectId)

#Repeat the process for raw data files, create a variable containing the merged raw data sets:
df_raw <- bind_rows(read.csv("ASC_data_raw_18.12.24.csv"), read.csv("ASC_data_raw_VS_18.12.24.csv"),
                          read.csv("NT_data_raw_07.01.25.csv"), read.csv("NT_data_raw_VS_07.01.25.csv"))

#Convert ebjectid to character variable:
df_raw$subject <- as.character(df_raw$subject)
```

Inspect structure of summary and raw data frames:

```{r}
#Check simulated data structure: 
str(df_sum) 
str(df_raw)
```

## Exclusion criteria

Participant IDs and the reasons for their exclusion are listed below.

+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Participant ID                                                | Reason for exclusion                                                                                                                                                                                                                                                      |
+===============================================================+===========================================================================================================================================================================================================================================================================+
| 5f3e8437fcbaca0b24f7e4e9                                      | Test frame                                                                                                                                                                                                                                                                |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66e57b9239b3cb31e8ca878b                                      | consecutive NR \>10                                                                                                                                                                                                                                                       |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6745bbbed5198599fec4be5d                                      | Did not respond on any experimental trials; consecutive NR \>10                                                                                                                                                                                                           |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6736167ddd3a31ee2b5fa8dd                                      | Did not respond on any experimental trials; consecutive NR \>10                                                                                                                                                                                                           |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6672d9dfb3878b6f6c5fa763                                      | Did not respond to first 53 experimental trials; consecutive NR \>10                                                                                                                                                                                                      |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6731d652c0f5542c912b8bf7 [**(LM)**]{.underline}               | After the first 17 trial responded left almost exclusively.                                                                                                                                                                                                               |
|                                                               |                                                                                                                                                                                                                                                                           |
|                                                               | (Potentially) not autistic - participant completed twice - reporting neurotypicality first time - but clarified that the first attempt was made in error.                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 673f7c392d58dfa2121c85da [**(LM)**]{.underline}               | After approx. half way, chose right or NR on every trial.                                                                                                                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66b246951b9aefec80b7158a [**(LM)**]{.underline}               | Frequently chose right consecutively.                                                                                                                                                                                                                                     |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6751b35e2ea9ab39c5c6aa77 [**(LM)**]{.underline}               | Only chose left, or NR (trials\>10)                                                                                                                                                                                                                                       |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66a250c5cfb4c9802675c786 [**(LM)**]{.underline}               | Frequently chose only left or right consecutively.                                                                                                                                                                                                                        |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6710c4cf3a160f87690ff9e8 - [**(LM)**]{.underline}             | Seemed to respond in blocks of left or right at random                                                                                                                                                                                                                    |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6715f821d59317137a6a123b                                      | Missing a lot of questionnaire responses                                                                                                                                                                                                                                  |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6738d09a032bf7224008ce7e                                      | Completed study twice; first time did not respond to many trials.                                                                                                                                                                                                         |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 669f5d4b56cd9e5a3615a8ed                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6770d14c73f6dd06e978bfcc                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 610299d092eb1df29a1c88b6                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66c9e33199e0581a3f40ec29                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 666c4699beb014a7b85ccea1                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 673333f751db5adf81499f5d                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 61501f8fdda44b1783c3556b                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66a8ed63c7a150e76f2f6c9e                                      | Not autistic, and used ctrl-B to skip ineligibility prompt                                                                                                                                                                                                                |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 675ef67a1ac1854b97ad3898                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66fc46712e0cd6ae205afc22                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 673dcafa74e1f8b7786087f4                                      | Not autistic, but ADHD                                                                                                                                                                                                                                                    |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 67713b697d8d5646684521e9                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66a4f41f0a2e0870ee52c6bf                                      | Not autistic, but dyslexic                                                                                                                                                                                                                                                |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 609e083e65eac4b7143bfc59                                      | Identifies as autistic, but does not have a diagnosis                                                                                                                                                                                                                     |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6718c98aba879c2ff2d74aa0                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 676059932932a2b96881e5fb                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6731c8479c456a18d3ce684d                                      | Identifies as autistic, but does not have a diagnosis.                                                                                                                                                                                                                    |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 61079db3e10fdd6232985023                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 666457e5527ea238e24c018f                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 663945510818036ee5453ff7                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 616934a454cf1f0e21721134                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66bcf992235bc8cbb733eaf8                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 665f469f755e84fb40af7e8f                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66ba0c9bf26db3f44b274954                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 605269ed69c7aac84a09dc72                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 67597d346b33858dd36b1987                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 671fb7f4e74935c3ed692844                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 65f29af4187cf293ea407c16                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6720fa867a2d912c050af8e8                                      | Not autistic                                                                                                                                                                                                                                                              |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 66897636a0f86915fe5e1530 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reporting neurotypicality first time - but clarified that the first attempt was made in error.                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5d60061e45714d00019ad6a4 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reporting neurotypicality first time - but clarified that the first attempt was made in error.                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6633bd3bc5ec5e52e25592a4 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reporting neurotypicality first time - but clarified that the first attempt was made in error.                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 630a8feaa8eef385f72dc1a5 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reporting neurotypicality first time - but clarified that the first attempt was made in error.                                                                                                                 |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6672e6b02916e61a88d5aa27 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reporting being unsure about a diagnosis the first time - but clarified that the wording of the questions had confused them.                                                                                   |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6694158481df62fee041a342 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reportedly not identifying as autistic the first time - but clarified that the wording of the questions had confused them; the participant does have a diagnosis of autism, but does not identify as autistic. |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 673fa52a23ea446026f0d8f1 [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reportedly identifying as neurotypical the first time - but clarified that they had not properly read the answers to the question; the participant reports having a diagnosis of autism.                       |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 614759cfe568048a1689b38a [**(Keep in analysis)**]{.underline} | (Potentially) not autistic - participant completed twice - reportedly identifying as neurotypical the first time - but clarified that they had not properly understood the question; the participant reports having a diagnosis of autism.                                |
+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Participants with [**LM**]{.underline} after their ID are to be kept in the main analysis. Given that these participant belong to the autistic group, it might be that any difficulty in learning the underlying task structure (in this case, choosing one side consistently) is related to autism. Thus, we reason to include these participants in all analyses, however, for posterity, we note them above. It is also possible that their performance might be overly influential, thus, we may also run the analysis of learning rate data with and without their inclusion to see if their data disproportionately impacts results. The individuals marked with [**keep in analysis**]{.underline} represent those who initially reported being neurotypical/non-autistic but on clarification reported having an autism diagnosis - none of these participants ever explicitly reported not having an autism diagnosis, but rather reported, for example, not identifying as autistic. Because our data collection relied on individuals to self-report a diagnosis, we believe that these individuals should be included as autistic.

Create a variable containing excluded participant IDs:

```{r}
#Create a vector of subject IDs to exclude:
excluded_subjects <- c("5f3e8437fcbaca0b24f7e4e9", "66e57b9239b3cb31e8ca878b", "6745bbbed5198599fec4be5d", "6736167ddd3a31ee2b5fa8dd", "6672d9dfb3878b6f6c5fa763", "6715f821d59317137a6a123b", "6738d09a032bf7224008ce7e", "669f5d4b56cd9e5a3615a8ed", "6770d14c73f6dd06e978bfcc", "610299d092eb1df29a1c88b6", "66c9e33199e0581a3f40ec29", "666c4699beb014a7b85ccea1", "673333f751db5adf81499f5d", "61501f8fdda44b1783c3556b", "66a8ed63c7a150e76f2f6c9e", "675ef67a1ac1854b97ad3898", "66fc46712e0cd6ae205afc22", "673dcafa74e1f8b7786087f4", "67713b697d8d5646684521e9", "66a4f41f0a2e0870ee52c6bf", "609e083e65eac4b7143bfc59", "6718c98aba879c2ff2d74aa0", "676059932932a2b96881e5fb", "6731c8479c456a18d3ce684d", "61079db3e10fdd6232985023", "666457e5527ea238e24c018f", "663945510818036ee5453ff7", "616934a454cf1f0e21721134", "66bcf992235bc8cbb733eaf8","665f469f755e84fb40af7e8f", "66ba0c9bf26db3f44b274954", "605269ed69c7aac84a09dc72", "67597d346b33858dd36b1987", "671fb7f4e74935c3ed692844", "65f29af4187cf293ea407c16", "6720fa867a2d912c050af8e8") 
length(excluded_subjects)
```

Check neurotype of participants excluded for consecutive NR \>10:

```{r}
#Identify if exclusions were autistic or NT:
excluded_subjects %>%
  filter(subjectId %in% c("66e57b9239b3cb31e8ca878b", 
                          "6745bbbed5198599fec4be5d", 
                          "6736167ddd3a31ee2b5fa8dd", 
                          "6672d9dfb3878b6f6c5fa763", 
                          "6751b35e2ea9ab39c5c6aa77"))
df_sum %>%
  filter(subjectId %in% "6715f821d59317137a6a123b")
```

Remove excluded participant IDs from both raw and summary data frames:

```{r}
#Then subset to exclude desired subject IDs: 
df_sum <- subset(df_sum, !(subjectId %in% excluded_subjects)) 
df_raw <- subset(df_raw, !(subject %in% excluded_subjects))
```

Check that both data frames match by comparing participant IDs:

```{r}
#First check the number of unique IDs in each data frame: 
length(unique(df_raw$subject)) 
length(unique(df_sum$subjectId))  
#Check that both data frames contain the same unique IDs: 
setdiff(df_sum$subjectId, df_raw$subject)
```

Remove anyone with trialcount \<160:

```{r}
#Filter trialcount = 160:
df_sum <- df_sum %>%
  filter(trialcount == 160)
```

# Questionnaire data

Inspect age and gender information:

```{r}
#Return age and gender stats:
mean(df_sum$Age)
sd(df_sum$Age)
range(df_sum$Age)
table(df_sum$Gender)
table(df_sum$ASCdiag)
```

## Questionnaire scoring

Beginning with MASQ scores, dummy code all MASQ responses; items score 1 point for Not at all, 2 for A little bit, 3 for Moderately, 4 for Quite a bit or 5 for Extremely:

```{r}
#Dummy code all AQ responses (inc. items that require reverse scoring);
#items score 1 point for definitely or slightly agree:
MASQ_forward <- c("MASQ_2",	"MASQ_3",	"MASQ_9",	"MASQ_12",	"MASQ_15",	"MASQ_19",	"MASQ_20",	"MASQ_25",	"MASQ_45",	"MASQ_48",	"MASQ_52",	"MASQ_55",	"MASQ_57",	"MASQ_59",	"MASQ_61",	"MASQ_63",	"MASQ_65",	"MASQ_67",	"MASQ_69",	"MASQ_73",	"MASQ_75",	"MASQ_77",	"MASQ_79",	"MASQ_81",	"MASQ_82",	"MASQ_85",	"MASQ_87",	"MASQ_88")

df_sum[, MASQ_forward] <- ifelse(df_sum[, MASQ_forward] == "Not at all", 1 , 
                               ifelse(df_sum[, MASQ_forward] == "A little bit", 2,
                                      ifelse(df_sum[, MASQ_forward] == "Moderately", 3 , ifelse(df_sum[, MASQ_forward] == "Quite a bit", 4, ifelse(df_sum[, MASQ_forward] == "Extremely", 5, NA)))))
```

And for EDA-QA scores, dummy code all EDA-QA responses (inc. items that require reverse scoring; EDAQ_14, EDAQ_20); items score 3 points for 'Very true', 2 points for 'Mostly true', 1 point for 'Somewhat true', and 0 points for 'Not true':

```{r}
#Dummy code all EDA-QA responses (inc. items that require reverse scoring; EDAQ_14, EDAQ_20);
#items score 3 points for 'Very true', 2 points for 'Mostly true', 1 point for 'Somewhat true', and 0 points for 
#'Not true':
EDAQ_forward <- c("EDAQ1", "EDAQ2", "EDAQ3", "EDAQ4","EDAQ5", "EDAQ6", "EDAQ7", "EDAQ8", "EDAQ9", "EDAQ10", "EDAQ11", 
                  "EDAQ12", "EDAQ13", "EDAQ15", "EDAQ16", "EDAQ17", "EDAQ18", "EDAQ19", "EDAQ21", "EDAQ22", "EDAQ23", 
                  "EDAQ24", "EDAQ25", "EDAQ26")

df_sum[, EDAQ_forward] <- ifelse((df_sum[,EDAQ_forward]) == "Very True", 3 , 
                                 ifelse((df_sum[, EDAQ_forward]) == "Mostly True", 2,
                                        ifelse((df_sum[, EDAQ_forward]) == "Somewhat True", 1, ifelse((df_sum[, EDAQ_forward]) == "Not True", 0, NA))))

#Reverse scores:
EDAQ_reverse <- c("EDAQ14", "EDAQ20")
df_sum[, EDAQ_reverse] <- ifelse((df_sum[, EDAQ_reverse]) == "Very True", 0 , 
                                 ifelse((df_sum[, EDAQ_reverse]) == "Mostly True", 1,
                                        ifelse((df_sum[, EDAQ_reverse]) == "Somewhat True", 2, ifelse((df_sum[, EDAQ_forward]) == "Not True", 3, NA))))
```

## Multiple imputation analysis (MIA)

Imputation of missing questionnaire data is performed under the assumption that missing values are missing at random (Schafer & Graham, 2002). Multiple imputations are generated using Multivariate Imputation by Chained Equations (MICE); 5 datasets are imputed using 50 iterations and randomly generated seeds. Analyses run on each dataset are pooled according to Rubin's (1987) rules.

mice() needs a clean data frame containing only the variables relevant to the MIA (i.e., no participant IDs, etc.). Begin by extracting the necessary columns:

```{r}
#Extract relevant data columns:
df_MIA <- df_sum[, grepl("^MASQ|^EDAQ", names(df_sum))]

#And remove total AQ and EDA-Q scores (we'll recalculate these after imputation):
df_MIA <- df_MIA[, !colnames(df_MIA) %in% c("MASQ", "EDAQ")]
```

Inspect the data frame, df_MIA, and its missing values:

```{r}
#Inspect cases with missing values:
df_MIA[!complete.cases(df_MIA), ]
#There are _ missing values (from _ participants), seemingly missing at random.

#Check which columns have missing values:
sapply(df_MIA, function(x)any(is.na(x)))
#_, all have missing values.

#We can also view the amount of missing data per column/variable, in this case, per question:
sapply(df_MIA, function(x) sum(is.na(x)))

#And calculate the percentage of missing data:
MASQ_data <- df_MIA[, grepl("^MASQ", names(df_MIA))]
EDAQ_data <- df_MIA[, grepl("^EDAQ", names(df_MIA))]

#Check for percentage of missing data for each variable:
(sum(is.na(EDAQ_data)) / prod(dim(EDAQ_data))) * 100
(sum(is.na(MASQ_data)) / prod(dim(MASQ_data))) * 100
```

Impute missing values for MASQ items:

```{r}
#Impute missing values for the AQ:
imp_MASQ <- mice(df_MIA[, startsWith(colnames(df_MIA), "MASQ")], m=5, maxit = 50, method = 'pmm', seed = 500)
# m: the number of imputations made per missing observation 
#    (5 is normal–generates 5 data sets with imputed/original values)
# maxit: the number of iterations?
# method: We use ’probable means ?? 
# seed: Values to randomly generate from??

summary(imp_MASQ)

#stripplot(impMASQ, pch = 20, cex = 1.2)

#Pool means and std according to Rubin's (1987) rules:
#Stack imputed datasets in long format, exclude the original data
imp_MASQpool <- complete(imp_MASQ,action="long",include = FALSE)

# Add imputed data back to original data:
df_MIA[, startsWith(colnames(df_MIA), "MASQ")] <- complete(imp_MASQ, mean(1,2,3,4,5,6))
head(imp_MASQ)
```

Impute missing values for EDAQ-QA items:

```{r}
# Now repeat the process for the EDAQ, start by imputing missing values:
imp_EDAQ <- mice(df_MIA[, startsWith(colnames(df_MIA), "EDAQ")], m=5, maxit = 50, method = 'pmm', seed = 500)
# m: the number of imputations made per missing observation 
#    (5 is normal–generates 5 data sets with imputed/original values)
# maxit: the number of iterations?
# method: We use ’probable means ?? 
# seed: Values to randomly generate from??

summary(imp_EDAQ)

#stripplot(imp_EDAQ, pch = 20, cex = 1.2)

#Pool means and std according to Rubin's (1987) rules:
#Stack imputed datasets in long format, exclude the original data
imp_EDAQpool <- complete(imp_EDAQ,action="long",include = FALSE)

#Add imputed data back to original data:
df_MIA[, startsWith(colnames(df_MIA), "EDAQ")] <- complete(imp_EDAQ, mean(1,2,3,4,5,6))
head(imp_EDAQ)
```

Finally, check that MIA worked effectively by returning any withstanding missing values:

```{r}
#Check that the imputation worked by viewing the amount of missing data per column/variable:
sapply(df_MIA, function(x) sum(is.na(x)))
```

If MIA worked, the above code should return 0 for each item. If so, save the output:

```{r}
#Save complete data set to .csv:
write.table(df_MIA, file="df_MIA.csv",sep=",",row.names=F)
```

## Calculate questionnaire scores

Begin by summing MASQ and EDA-QA scores to derive total scores for each metric:

```{r}
#Load post-MIA data frame:
df_MIA <- read.csv("df_MIA.csv") 

#Start with AQ scores:
df_MIA$MASQ <- rowSums(df_MIA[, startsWith(colnames(df_MIA), "MASQ")])

#Do the same for EDA-QA scores:
df_MIA$EDAQ <- rowSums(df_MIA[, startsWith(colnames(df_MIA), "EDA")])
```

We do not need individual questionnaire item values for our analysis, extract total scores and save to a new data frame:

```{r}
#Collect varaibles of interest:
df_Q <- df_MIA[c("MASQ", "EDAQ")]
```

## Demographic information

Add participant IDs and diagnostic information to the data frame, df_Q:

```{r}
#Add Prolific IDs and dates:
df_Q$subject <- df_sum$subjectId
df_Q$date <- df_sum$startDate
df_Q$time <- df_sum$startTime

#Create a new column called dummy_code based on ASCdiag
df_Q$neurotype <- ifelse(df_sum$ASCdiag == "Yes", "ASC", "NT")
df_Q$neurotype[is.na(df_Q$neurotype)] <- "NT"

#Add age and gender:
df_Q$age <- df_sum$Age
df_Q$gender <- df_sum$Gender

#Check demographics by neurotype:
df_Q %>%
  dplyr::group_by(neurotype) %>%
  dplyr::summarize(
    mean_EDAQ = mean(EDAQ, na.rm = TRUE),
    sd_EDAQ = sd(EDAQ, na.rm = TRUE),
    mean_MASQ = mean(MASQ, na.rm = TRUE),
    sd_MASQ = sd(MASQ, na.rm = TRUE),
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    male = sum(gender == "Male", na.rm = TRUE),
    female = sum(gender == "Female", na.rm = TRUE),
    other = sum(gender == "Other", na.rm = TRUE)
  )

#Calculate the cutoff for the top 33% of EDAQ scores:
edaq_cutoff <- quantile(df_Q$EDAQ, 0.67)

#Create a new variable based on the cutoff and neurotype where 1=ASC, 2=NT, and 3=PDA
df_Q <- df_Q %>%
  mutate(group = ifelse(EDAQ >= edaq_cutoff, "PDA", ifelse(neurotype == "ASC", "ASC", "NT")))

#Check demographics by group:
df_Q %>%
  dplyr::group_by(group) %>%
  dplyr::summarize(
    mean_EDAQ = mean(EDAQ, na.rm = TRUE),
    sd_EDAQ = sd(EDAQ, na.rm = TRUE),
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    male = sum(gender == "Male", na.rm = TRUE),
    female = sum(gender == "Female", na.rm = TRUE),
    other = sum(gender == "Other", na.rm = TRUE)
  )
```

Finally, write the processed questionnaire data to .csv:

```{r}
#Write data to .csv:
write.table(df_Q, file="Questionnaire_Data.csv",sep=",",row.names=F)
```

# PRL data

Load RT data from raw Inquisit .csv:

```{r}
#Load RT data:
df_R <- df_raw
```

Extract from the RT data only the participants who returned complete questionnaire data. To do this, load the processed questionnaire data:

```{r}
#Read in questionnaire data:
df_Q <- read.csv("Questionnaire_Data.csv")
df_Q
```

Use time, date, and subject ID to filter and match participant RT and questionnaire data:

```{r}
df_Q$subject <- as.character(df_Q$subject)

#Filter df_R using df_Q:
df_R <- semi_join(df_R, df_Q, by = c("time", "subject"))

#Then check the number of unique IDs in each data frame:
length(unique(df_R$subject))
length(unique(df_Q$subject))

#Check that both data frames contain the same unique IDs - this should return character(0) if all values are equal:
setdiff(df_R$subject, df_Q$subject)
```

Filter variables of interest. For a simple Rescorla-Wagner model, we need response, feedback, and trial number. To study lose-shift and perseverative errors, we also need reversal point (reversal):

```{r}
#Collect varaibles of interest:
df_R <- df_R[c("subject", "trialcode", "trialcount","blockCount", "blockorder","blockLabel","trialType","ruleStart", "correctChoice", "correctChoicePosition", "responseSide", "correctResponse", "feedback", "phaseCount", "RTs")]
```

## Further exclusion criteria

We also want to extract trials of interest (i.e., experimental trials) removing things like consent and debrief information:

```{r}
#Extract only values of trials that begin with "choice":
df_R <- df_R[grepl("^reward|^punish", df_R$trialcode), ]
```

## Demographic information

Add demographic information from df_Q to df_R:

```{r}
#Add diagnosis, age, gender, AQ, and EDA-QA scores from df_Q:
df_R <- left_join(df_R, df_Q[, c("subject", "age", "gender", "neurotype", "group", "MASQ", "EDAQ")], by = "subject")

#Check that worked:
sum(df_R$group == "PDA")
sum(df_R$group == "ASC")
sum(df_R$group == "NT")
```

## Calculate behavioural parameters

Create a new columns to hold lose-shift behaviours and perseverative errors - variables that denote how many of each participants made. We define lose-shift behavious as a response switch made after receiving negative feedback, reported as a proportion of total negative feedback trials (as some people will have performed worse than others and will therefore have experienced more negative feedback); lose-shift is used as a measure of feedback sensitivity, thus, we are interested in total lose-shift behaviours irrespective of whether the shift is congruent with the task rule (correct) or incongruent (incorrect).

Because our paradigm utilises multiple reversals, we define perseverative and regressive errors as using D'Cruz et al. (2013; 2016) definitions:

-   **Perseveration** as the number of continued responses to the old rule after each reversal before making a correct choice.
-   **Regressive errors** as responses to the old rule after each reversal after making a correct choice.

Note: "feedback" is coded as 1 = negative feedback and 2 = positive feedback, "correct" is coded as 0 = incorrect and 1 = correct, and keyboard codes are E=18 and I=23:

```{r}
#Calculate and add shapeChoice to data frame - this represents the participant's choice on each trial calculated as: if the participant's responseSide matches the correctChoicePosition, then their shapeChoice is the correctChoice:
df_R <- df_R %>%
  mutate(
    shapeChoice = case_when(
      responseSide == "Left" & correctChoicePosition == "Left" ~ correctChoice,
      responseSide == "Right" & correctChoicePosition == "Right" ~ correctChoice,
      responseSide == "Left" & correctChoicePosition == "Right" ~ ifelse(correctChoice == "Triangle", "Circle", "Triangle"),
      responseSide == "Right" & correctChoicePosition == "Left" ~ ifelse(correctChoice == "Triangle", "Circle", "Triangle"),
      responseSide == "NR" ~ "No Response",
      TRUE ~ NA_character_
    )
  )

#Calculate lose_shift behaviour for each participant by lagging feedback and response (when previous feedback is negative, is response different from previous response) and add them to df_R:
df_R <- df_R %>%
  # Step 2: Lag responses and feedback
  group_by(subject) %>%
  mutate(
    previous_choice = lag(shapeChoice),
    previous_feedback = lag(feedback)
  ) %>%
  ungroup() %>%
  # Step 3: Calculate lose-shift behavior using shapeChoice
  mutate(
    lose_shift_count = case_when(
      previous_feedback == -1 & previous_choice %in% c("Triangle", "Circle") & 
        (shapeChoice != previous_choice | shapeChoice == "No Response") ~ "LS",  # Count lose-shift including no response
      TRUE ~ NA_character_
    )
  ) %>%
  # Step 4: Summarize lose-shift behavior
  group_by(subject) %>%
  dplyr::summarise(
    lose_shift = sum(lose_shift_count == "LS", na.rm = TRUE),  # Sum lose-shift occurrences
    punished_trials = sum(previous_feedback == -1 & previous_choice %in% c("Triangle", "Circle"), na.rm = TRUE),  # Count punished trials with valid choices
    lose_shift_proportion = lose_shift / punished_trials,  # Proportion of lose-shift responses
    .groups = 'drop'
  ) %>%
  # Step 5: Join summarized results back to the main dataset
  left_join(df_R, by = "subject")

#lose-shift is intended to measure how sensitive participants are to negative feedback. Since no response (response = 0) implies the participant didn’t engage with the task, the feedback is irrelevant — it's redundant to measure their shift after receiving negative feedback for no response. However, instances where a subsequent response to an active response (either left or right) is 0 (no response), are treated as informative about feedback sensitivity — the participant might have been influenced by the negative feedback to not make a choice at all, which could signal indecision or avoidance.

###################################################################################

#For post hoc analysis of lose-shift behaviour in the stable condition only:
df_R <- df_R %>%
  filter(blockLabel == "Stable") %>%
  # Step 2: Lag responses and feedback
  group_by(subject) %>%
  mutate(
    previous_choice = lag(shapeChoice),
    previous_feedback = lag(feedback)
  ) %>%
  ungroup() %>%
  # Step 3: Calculate lose-shift behavior using shapeChoice
  mutate(
    lose_shift_count_S = case_when(
      previous_feedback == -1 & previous_choice %in% c("Triangle", "Circle") & 
        (shapeChoice != previous_choice | shapeChoice == "No Response") ~ "LS",  # Count lose-shift including no response
      TRUE ~ NA_character_
    )
  ) %>%
  # Step 4: Summarize lose-shift behavior
  group_by(subject) %>%
  dplyr::summarise(
    lose_shift_S = sum(lose_shift_count_S == "LS", na.rm = TRUE),  # Sum lose-shift occurrences
    punished_trials_S = sum(previous_feedback == -1 & previous_choice %in% c("Triangle", "Circle"), na.rm = TRUE),  # Count punished trials with valid choices
    lose_shift_proportion_S = lose_shift_S / punished_trials_S,  # Proportion of lose-shift responses
    .groups = 'drop'
  ) %>%
  # Step 5: Join summarized results back to the main dataset
  left_join(df_R, by = "subject")
  
###################################################################################

#Calculate perseverative and regressive errors for each participant (filter for trials in the volatile condition and count the number of errors made before and after correct responses) and add to df_R:

# First, ensure that for each subject, blockorder is unique.
# (If not, you may need to take the first value or otherwise collapse it.)
df_R_unique <- df_R %>% 
  group_by(subject) %>% 
  summarise(blockorder = first(blockorder),
            .groups = 'drop')

# Now compute the summary statistics using summarise().
df_R_summary <- df_R %>%
  group_by(subject) %>%
  summarise(
    # Calculate perseveration count
    perseveration = {
      pers_count <- c()
      # Assume blockorder is constant within subject; use the first value.
      start_trial <- ifelse(first(blockorder) == "S/V", 81, 21)
      for (i in 1:n()) {
        if (lag(phaseCount, default = 99)[i] == 20 | 
            (lag(phaseCount, default = 99)[i] == 0 & blockCount[i] == 1)) {  
          consecutive_incorrect <- 0  # Reset counter
          for (j in i:n()) {  # Loop from the current trial onward
            if ((lag(phaseCount, default = 99)[j] == 20 | 
                 (lag(phaseCount, default = 99)[j] == 0 & blockCount[j] == 1 & trialcount[i] == 81)) && j != i) {  
              break  # Stop at the next reversal
            }
            if (correctResponse[j] != "Correct") {
              consecutive_incorrect <- consecutive_incorrect + 1
            } else {
              break
            }
          }
          pers_count <- c(pers_count, consecutive_incorrect)
        }
      }
      sum(pers_count)
    },
    # Calculate perseveration proportion
    perseveration_proportion = {
      start_trial <- ifelse(first(blockorder) == "S/V", 81, 21)
      pers_count <- c()
      for (i in 1:n()) {
        if (lag(phaseCount, default = 99)[i] == 20 | 
            (lag(phaseCount, default = 99)[i] == 0 & blockCount[i] == 1)) {  
          consecutive_incorrect <- 0
          for (j in i:n()) {
            if ((lag(phaseCount, default = 99)[j] == 20 | 
                 (lag(phaseCount, default = 99)[j] == 0 & blockCount[j] == 1 & trialcount[i] == 81)) && j != i) {  
              break
            }
            if (correctResponse[j] != "Correct") {
              consecutive_incorrect <- consecutive_incorrect + 1
            } else {
              break
            }
          }
          pers_count <- c(pers_count, consecutive_incorrect)
        }
      }
      total_pers_count <- sum(pers_count)
      total_trials <- sum(trialcount >= start_trial)
      total_pers_count / total_trials
    },
    # Calculate regression count
    regression = {
      reg_count <- 0
      found_correct <- FALSE
      start_trial <- ifelse(first(blockorder) == "S/V", 81, 21)
      for (i in 1:n()) {
        if (lag(phaseCount, default = 99)[i] == 20 | 
            (lag(phaseCount, default = 99)[i] == 0 & blockCount[i] == 1 & trialcount[i] == 81)) {  
          found_correct <- FALSE
          if (correctResponse[i] == "Correct") {
            found_correct <- TRUE
          }
          for (j in (i + 1):n()) {
            if ((lag(phaseCount, default = 99)[j] == 20 | 
                 (lag(phaseCount, default = 99)[j] == 0 & blockCount[j] == 1))) {  
              break
            }
            if (correctResponse[j] == "Correct") {
              found_correct <- TRUE
            } else if (correctResponse[j] == "Incorrect" && found_correct) {
              reg_count <- reg_count + 1
            }
          }
        }
      }
      reg_count
    },
    # Calculate regression proportion
    regression_proportion = {
      start_trial <- ifelse(first(blockorder) == "S/V", 81, 21)
      reg_count <- 0
      found_correct <- FALSE
      for (i in 1:n()) {
        if (lag(phaseCount, default = 99)[i] == 20 | 
            (lag(phaseCount, default = 99)[i] == 0 & blockCount[i] == 1 & trialcount[i] == 81)) {  
          found_correct <- FALSE
          if (correctResponse[i] == "Correct") {
            found_correct <- TRUE
          }
          for (j in (i + 1):n()) {
            if ((lag(phaseCount, default = 99)[j] == 20 | 
                 (lag(phaseCount, default = 99)[j] == 0 & blockCount[j] == 1))) {  
              break
            }
            if (correctResponse[j] == "Correct") {
              found_correct <- TRUE
            } else if (correctResponse[j] == "Incorrect" && found_correct) {
              reg_count <- reg_count + 1
            }
          }
        }
      }
      total_trials <- sum(trialcount >= start_trial)
      reg_count / total_trials
    },
    .groups = 'drop'
  )

# Merge the computed summary back with df_R by subject
df_R <- left_join(df_R, df_R_summary, by = "subject")

###################################################################################

#Also calculate lose-shift behaviours for each condition separately:
df_R <- df_R %>%
  # Step 2: Lag responses and feedback
  group_by(subject, blockLabel) %>%
  mutate(
    previous_choice_cond = lag(shapeChoice),
    previous_feedback_cond = lag(feedback)
  ) %>%
  ungroup() %>%
  # Step 3: Calculate lose-shift behavior using shapeChoice
  mutate(
    lose_shift_count_cond = case_when(
      previous_feedback_cond == -1 & previous_choice_cond %in% c("Triangle", "Circle") & 
        (shapeChoice != previous_choice_cond | shapeChoice == "No Response") ~ "LS",  # Count lose-shift including no response
      TRUE ~ NA_character_
    )
  ) %>%
  # Step 4: Summarize lose-shift behavior for each condition (blockLabel) and subject
  group_by(subject, blockLabel) %>%
  dplyr::summarise(
    lose_shift_cond = sum(lose_shift_count_cond == "LS", na.rm = TRUE),  # Sum lose-shift occurrences
    punished_trials_cond = sum(previous_feedback_cond == -1 & previous_choice_cond %in% c("Triangle", "Circle"), na.rm = TRUE),  # Count punished trials with valid choices
    lose_shift_proportion_cond = lose_shift_cond / punished_trials_cond,  # Proportion of lose-shift responses
    .groups = 'drop'
  ) %>%
  # Step 5: Join summarized results back to the main dataset
  left_join(df_R, by = c("subject", "blockLabel"))

#Check that worked:
df_R
```

Save data:

```{r}
write.table(df_R, file="df_R.csv",sep=",",row.names=F)

df_R <- read.csv("df_R.csv")
```

## Choice congruence

Calculate choice congruence for each participant as a percentage of congruent responses for every ten trials (sequentially):

```{r}
#Load data:
df_R <- read.csv("df_R.csv") %>%
  mutate(b_correct = ifelse(correctResponse == "Correct", 1, ifelse(correctResponse == "Incorrect", 0, 0)))

#Calculate congruence per ten trials for each participant in stable condition:
congruence_STABLE <- df_R %>%
  filter(blockLabel == "Stable") %>%
  group_by(subject) %>%
  mutate(trial_group = (blockCount - 1) %/% 10 + 1) %>%
  group_by(subject, trial_group) %>%
  summarise(
    congruence = mean(b_correct) * 100,
    total_correct = sum(b_correct),
    total_trials = n()
  ) %>%
  ungroup()

#Calculate congruence per ten trials for each participant in stable condition:
congruence_VOLATILE <- df_R %>%
  filter(blockLabel == "Volatile") %>%
  group_by(subject) %>%
  mutate(trial_group = (blockCount - 1) %/% 10 + 1) %>%
  group_by(subject, trial_group) %>%
  summarise(
    congruence = mean(b_correct) * 100,
    total_correct = sum(b_correct),
    total_trials = n()
  ) %>%
  ungroup()

#View the result:
congruence_STABLE
congruence_VOLATILE

#Also calculate the average congruence for each condition (i.e., the average percentage of congruent responses for each ten trial block):
avg_congruence_STABLE <- congruence_STABLE %>%
  group_by(trial_group) %>%
  summarise(avg_congruence = mean(congruence)) %>%
  ungroup()
avg_congruence_VOLATILE <- congruence_VOLATILE %>%
  group_by(trial_group) %>%
  summarise(avg_congruence = mean(congruence)) %>%
  ungroup()

#View the result:
avg_congruence_STABLE
avg_congruence_VOLATILE

# Calculate average congruence by neruotype and blockOrder for the Stable condition
avg_congruence_by_neurotype_blockOrder_STABLE <- df_R %>%
  filter(blockLabel == "Stable") %>%
  group_by(neurotype, blockorder, trial_group = (blockCount - 1) %/% 10 + 1) %>%
  summarise(avg_congruence = mean(b_correct) * 100, .groups = 'drop')

# Calculate average congruence by neruotype and blockOrder for the Volatile condition
avg_congruence_by_neurotype_blockOrder_VOLATILE <- df_R %>%
  filter(blockLabel == "Volatile") %>%
  group_by(neurotype, blockorder, trial_group = (blockCount - 1) %/% 10 + 1) %>%
  summarise(avg_congruence = mean(b_correct) * 100, .groups = 'drop')

# View the results
avg_congruence_by_neurotype_blockOrder_STABLE
avg_congruence_by_neurotype_blockOrder_VOLATILE

###################################################################################
###################################################################################

#plot the average congruence for the stable condition:
ggplot(data = avg_congruence_by_neurotype_blockOrder_STABLE, 
       aes(x = factor(trial_group), y = as.numeric(avg_congruence), group = neurotype, color = as.factor(neurotype))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Trial group", y = "Average Congruence", color = "Neurotype") +
  #scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Average Congruence For The Stable Condition by Block Order 
and Neurotype") +
  #ylim(0,1) +
  theme_minimal() +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "right",
    legend.spacing.y = unit(0.1, "cm"),  
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  
    legend.text = element_text(size = 10),  
    strip.text = element_text(size = 12)  # Keep facet labels for blockorder
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(. ~ factor(blockorder))
ggsave( 'Acc_Sta.png', plot = last_plot(), dpi = 300)

#Plot the average congruence for the volatile condition:
ggplot(data = avg_congruence_by_neurotype_blockOrder_VOLATILE, 
       aes(x = factor(trial_group), y = as.numeric(avg_congruence), group = neurotype, color = as.factor(neurotype))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Trial group", y = "Average Congruence", color = "Neurotype") +
  #scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Average Congruence For The Volatile Condition by Block Order 
and Neurotype") +
  #ylim(0,1) +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "right",
    legend.spacing.y = unit(0.1, "cm"),  
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  
    legend.text = element_text(size = 10),  
    strip.text = element_text(size = 12)  # Keep facet labels for blockorder
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(. ~ factor(blockorder))
ggsave( 'Acc_Vol.png', plot = last_plot(), dpi = 300)
```

The incremental choice congruence demonstrated by participant average in the stable condition suggest that probabilistic contingencies were learnt - responses were above chance level. Moreover, an accuracy of approx. 80% (in this case 74%) is in keeping with probability matching behaviour, a decision strategy in which predictions of a class membership are proportional to the class base rates. Such a strategy is arguably a useful tool for pattern search (Gaissmaier & Schooler, 2008) - in this case, it might suggest that participants were attempting to learn the underlying task structure.

Plotting congruence for the stable condition by neurotype and block order demonstrates that neurotypical and autistic groups have higher average congruence for those who completed the stable condition first. This makes sense; those who completed the volatile condition first went into the stable condition assuming volatility, and thus took longer to learn and sustain the rule. To that end, the dip in congruence displayed by the neurotypical group at trial group 4 in the V/S group might reflect an anticipated rule switch. In addiiton, the neurotypical group achieved more-or-less incremental improvements in congruence as trials in the stable condition progressed (here, plotted as group average in blocks of 10 trials), while the autistic group initially demonstrate similar incremental improvements, average congruence plateaus around trial 50, regardless of block order.

Plotting congruence for the volatile condition by neurotype and block order demonstrates that those who completed the stable condition before the volatile condition have lower average congruence in trial group 1 - this is true for both neurotypical and autistic groups. This is likely because, for participants in this block schedule, this is the first time they experience a reversal, and thus perseverate to the old rule. This pattern is not seen for the participants who complete the volatile condition first - they have no context to bias their choice decision. Overall, the neurotypical group demonstrate higher average congruence in both block presentation of the volatile condition compare to the autistic group - this difference is marginal.

It would be interesting to see how those with higher EDA-QA scores perform. Extract those with EDA-QA scores above the 67th percentile and plot their accuracy alongside NT and ASC groups.

```{r}
#Load data:
df_R <- read.csv("df_R.csv") %>%
  mutate(b_correct = ifelse(correctResponse == "Correct", 1, ifelse(correctResponse == "Incorrect", 0, 0)))

# Calculate average congruence by group and blockOrder for the Stable condition
avg_congruence_by_group_blockOrder_STABLE <- df_R %>%
  filter(blockLabel == "Stable") %>%
  group_by(group, blockorder, trial_group = (blockCount - 1) %/% 10 + 1) %>%
  summarise(avg_congruence = mean(b_correct) * 100, .groups = 'drop')

# Calculate average congruence by group and blockOrder for the Volatile condition
avg_congruence_by_group_blockOrder_VOLATILE <- df_R %>%
  filter(blockLabel == "Volatile") %>%
  group_by(group, blockorder, trial_group = (blockCount - 1) %/% 10 + 1) %>%
  summarise(avg_congruence = mean(b_correct) * 100, .groups = 'drop')

# View the results
avg_congruence_by_group_blockOrder_STABLE
avg_congruence_by_group_blockOrder_VOLATILE

#plot the average accuracy for the stable condition:
ggplot(data = avg_congruence_by_group_blockOrder_STABLE, 
       aes(x = factor(trial_group), y = as.numeric(avg_congruence), group = group, color = as.factor(group))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Trial group", y = "Average Congruence", color = "Group") +
  #scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Average Congruence For The Stable Condition by Block Order 
and Group") +
  #ylim(0,1) +
  theme_minimal() +
  scale_color_manual(values = c("#cc99ff", "#66cccc", "#336666"), name = "Group") +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "right",
    legend.spacing.y = unit(0.1, "cm"),  
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  
    legend.text = element_text(size = 10),  
    strip.text = element_text(size = 12)  # Keep facet labels for blockorder
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(. ~ factor(blockorder))
ggsave( 'Acc_Sta_PDA.png', plot = last_plot(), dpi = 300)

#Plot the average congruence for the volatile condition:
ggplot(data = avg_congruence_by_group_blockOrder_VOLATILE, 
       aes(x = factor(trial_group), y = as.numeric(avg_congruence), group = group, color = as.factor(group))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Trial group", y = "Average Congruence", color = "Group") +
  #scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Average Congruence For The Volatile Condition by Block Order 
and Group") +
  #ylim(0,1) +
  scale_color_manual(values = c("#cc99ff", "#66cccc","#336666"), name = "Group") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "right",
    legend.spacing.y = unit(0.1, "cm"),  
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  
    legend.text = element_text(size = 10),  
    strip.text = element_text(size = 12)  # Keep facet labels for blockorder
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(. ~ factor(blockorder))
ggsave( 'Acc_Vol_PDA.png', plot = last_plot(), dpi = 300)
```

Interestingly, plotting average congruence by group (inc. PDA), it seems that individuals with the highest EDA-QA scores show the lowest choice-congruence. This was not a prediction we made - we will run a post hoc analysis to investigate this unexpected trend.

## Feedback and congruence

And also check the number of consecutive runs of negative feedback for correct response. This should also be by block/phase for each participant:

```{r}
#Load data:
df_R <- read.csv("df_R.csv") %>%
  mutate(b_correct = ifelse(correctResponse == "Correct", 1, ifelse(correctResponse == "Incorrect", 0, 0)))

# Step 1: Calculate consecutive runs of negative feedback for correct responses
neg_run <- df_R %>%
  # Define trial phase: If blockname is "Volatile", break it into 20-trial phases
  mutate(
    trial_phase = ifelse(blockLabel == "Volatile", (blockCount - 1) %/% 20 + 1, 1)  # Stable block ("STO") as phase 1
  ) %>%
  
  # Identify correct responses with negative feedback (feedback == -1 and correct == 1)
  mutate(correct_and_neg = ifelse(correctResponse == "Correct" & feedback == -1, 1, 0)) %>%
  
  # Create a new column that groups consecutive sequences of 1's (correct and negative feedback)
  arrange(subject, blockLabel, trial_phase, blockCount) %>%  # Ensure ordered by trial
  group_by(subject, blockLabel, trial_phase) %>%
  
  # Base R approach to identify consecutive runs of 1's
  mutate(run_id = cumsum(c(1, diff(correct_and_neg) != 0)) * correct_and_neg) %>%
  
  filter(correct_and_neg == 1) %>%  # Only consider correct responses with negative feedback
  group_by(subject, blockLabel, trial_phase, run_id) %>%  # Group by run_id to capture each sequence
  summarise(run_length = n(), .groups = 'drop') %>%  # Calculate run length
  group_by(subject, blockLabel, trial_phase) %>%  # Group by subject, block, and trial phase
  summarise(max_neg_run = max(run_length), .groups = 'drop')  # Get the longest run for each group

#Print:
neg_run

#Print max:
max(neg_run$max_neg_run)

#Also, manually inspect the number of consecutive runs of neg. feedback for correct responses:
df_R %>%
  # Define trial phase: If blockname is "Volatile", break it into 20-trial phases
  mutate(
    trial_phase = ifelse(blockLabel == "Volatile", (blockCount - 1) %/% 20 + 1, 1)  # Stable block ("Stable") as phase 1
  ) %>%
  
  # Identify correct responses with negative feedback (feedback == -1 and correct == 1)
  mutate(correct_and_neg = ifelse(correctResponse == "Correct" & feedback == -1, 1, 0)) %>%
  
  # Create a new column that groups consecutive sequences of 1's (correct and negative feedback)
  arrange(subject, blockLabel, trial_phase, blockCount) %>%  # Ensure ordered by trial
  group_by(subject, blockLabel, trial_phase) %>%
  
  # Base R approach to identify consecutive runs of 1's
  mutate(run_id = cumsum(c(1, diff(correct_and_neg) != 0)) * correct_and_neg)
```

Above shows the maximum consecutive run of negative feedback given for correct responses - in no instance was this \>2, which is what we wanted to see.

Check the percentage of feedback and congruence (i.e., 80:20) for each participant in the stable, and each of the volatile phases:

```{r}
# Load data and preprocess
df_R <- read.csv("df_R.csv") %>%
  mutate(
    b_correct = ifelse(correctResponse == "Correct", 1, 0),  # Binary flag for correct responses
    trial_phase = ifelse(blockLabel == "Volatile",          # Define 20-trial phases in the volatile block
                         (blockCount - 1) %/% 30 + 1,       # Adjust to 20-trial phases
                         1)                                # Single phase for stable block
  )

# Calculate feedback percentages for both correct and incorrect responses
feedback_percentages <- df_R %>%
  group_by(subject, blockLabel, trial_phase, feedback) %>%  # Group by subject, block, phase, and feedback type
  summarise(count = n(), .groups = "drop") %>%             # Count occurrences of each feedback type
  group_by(subject, blockLabel, trial_phase) %>%           # Group by subject, block, and phase
  mutate(percentage = count / sum(count) * 100) %>%        # Calculate percentage for each feedback type
  ungroup()

# Summarise for checking
feedback_percentages %>%
  group_by(subject, blockLabel, trial_phase) %>%
  summarise(
    reward_percentage = sum(percentage[feedback == 1], na.rm = TRUE),
    punish_percentage = sum(percentage[feedback == -1], na.rm = TRUE),
    .groups = "drop"
  )

# Calculate feedback percentages for both correct and incorrect responses
feedback_percentages_10 <- df_R %>%
  filter(subject %in% 10) %>%
  group_by(subject, blockLabel, trial_phase, feedback) %>%  # Group by subject, block, phase, and feedback type
  summarise(count = n(), .groups = "drop") %>%             # Count occurrences of each feedback type
  group_by(subject, blockLabel, trial_phase) %>%           # Group by subject, block, and phase
  mutate(percentage = count / sum(count) * 100) %>%        # Calculate percentage for each feedback type
  ungroup()

# Summarise for checking
feedback_percentages_10 %>%
  group_by(subject, blockLabel, trial_phase) %>%
  summarise(
    reward_percentage = sum(percentage[feedback == 1], na.rm = TRUE),
    punish_percentage = sum(percentage[feedback == -1], na.rm = TRUE),
    .groups = "drop"
  )

#And for congruence:

#First derive congruence from data:
# Calculate congruence percentages for both correct and incorrect responses
feedback_con <- df_R %>%
  group_by(subject, blockLabel, trial_phase, trialType) %>%  # Group by subject, block, phase, and feedback type
  summarise(count = n(), .groups = "drop") %>%             # Count occurrences of each feedback type
  group_by(subject, blockLabel, trial_phase) %>%           # Group by subject, block, and phase
  mutate(percentage = count / sum(count) * 100) %>%        # Calculate percentage for each feedback type
  ungroup()

# Summarise for checking
feedback_con %>%
  group_by(subject, blockLabel, trial_phase) %>%
  summarise(
    Congruent_percentage = sum(percentage[trialType == "Congruent"], na.rm = TRUE),
    Incongruent_percentage = sum(percentage[trialType == "Incongruent"], na.rm = TRUE),
    .groups = "drop"
  )

#To check the above, calulate congruence again using raw behavioural data:
df_R <- df_R %>%
  mutate(
    congruence_check = case_when(
      (feedback == 1 & responseSide == correctChoicePosition) | 
      (feedback == -1 & responseSide != correctChoicePosition) ~ "Congruent",  # Positive alignment or negative misalignment
      (feedback == 1 & responseSide != correctChoicePosition) | 
      (feedback == -1 & responseSide == correctChoicePosition) ~ "Incongruent"  # Positive misalignment or negative alignment
    )
  )

feedback_con_check <- df_R %>%
  group_by(subject, blockLabel, trial_phase, congruence_check) %>%  # Group by subject, block, phase, and feedback type
  summarise(count = n(), .groups = "drop") %>%             # Count occurrences of each feedback type
  group_by(subject, blockLabel, trial_phase) %>%           # Group by subject, block, and phase
  mutate(percentage = count / sum(count) * 100) %>%        # Calculate percentage for each feedback type
  ungroup()

# Summarise for checking
feedback_con_check %>%
  group_by(subject, blockLabel, trial_phase) %>%
  summarise(
    Congruent_percentage = sum(percentage[congruence_check == "Congruent"], na.rm = TRUE),
    Incongruent_percentage = sum(percentage[congruence_check == "Incongruent"], na.rm = TRUE),
    .groups = "drop"
  )
```

The above table confirms that all participants were presented with 80% congruent feedback (i.e., positive feedback for a correct response and negative feedback for an incorrect response) and 20 % incongruent feedback (i.e., negative feedback for a correct response and positive feedback for an incorrect response) in both stable and volatile phases of the task - again, this is what we want to see.

Save data:

```{r}
write.table(df_R, file="df_R.csv",sep=",",row.names=F)

df_R <- read.csv("df_R.csv")
```

## Data cleaning

Filter and remove trials with latency \<200ms - we assume RTs \<200ms are too quick to represent a meaningful response:

```{r}
#Check how many trials are <200MS and remove: 
nrow(df_R %>%  filter(RTs < 200)) #There are 1463 trials in the data set that are <200MS.  

#Remove latency > 200: 
df_R_clean <- df_R %>%  filter(RTs > 200)
```

As per Browning et al. (2015), remove any absent responses - this should happen after perseverative, regressive and lose-shift behaviours have been calculated, as "no response" is still useful for discerning feedback sensitivity:

```{r}
#Filter all trials that do not have a response value of 0:
df_R_clean <- df_R_clean %>%  filter(responseSide != "NR")
```

Isolate those participants who chose only one sided responses (left or right):

```{r}
#Create a vector of subject IDs to exclude:
LM_excluded_subjects <- c("6731d652c0f5542c912b8bf7", "673f7c392d58dfa2121c85da", "66b246951b9aefec80b7158a", "6751b35e2ea9ab39c5c6aa77", "66a250c5cfb4c9802675c786", "6710c4cf3a160f87690ff9e8") 
length(LM_excluded_subjects)
```

For now, keep these individuals in the analysis - given that these participants all belong to the autistic sample, it is likely that choosing a single-sided response is indicative of genuine difficulty to learn the task rule. However, we can also perform the analysis without these participant to see if/how they influence results:

```{r}
#Subset to exclude desired subject IDs: 
#df_R_clean <- subset(df_R_clean, !(subject %in% LM_excluded_subjects)) 

#Check the number of unique IDs: 
length(unique(df_R_clean$subject)) 
```

Save data:

```{r}
#Write data to .csv:
write.table(df_R_clean, file="LM_Data.csv",sep=",",row.names=F)
 
df_R_clean <- read.csv("LM_Data.csv")
```

Check that condition order was equally balanced - i.e., that an equal number of participants completed stable/volatile and volatile/stable:

```{r}
#Create a table that shows how many participants completed each condition presentation:
df_R_clean %>%
  distinct(subject, blockorder) %>%
  count(blockorder)

#Also check that these numbers account for all participants in the sample:
df_R_clean %>%
  summarise(num_subjects = n_distinct(subject))
```

# Check learning rates differ for each condition

Load in learning rate data:

```{r}
#Load data:
LR_21_80 <- read.csv("results_RL_summary_21_80.csv")  #This is learning rates calculated using trials 21-80 in each condition
LR <- read.csv("results_RL_summary.csv")              #This is learning rates calculated using trials 01-80 in each condition

LR_01_20 <- read.csv("results_RL_summary_01_20.csv")  #This is learning rates calculated using trials 01-20 in each condition
```

The first 20 trials are the same in both stable and volatile conditions - we are interested in differences in learning in these two context, so we will analyse learning rates calculated using trials 21-80 in each condition. Visualise learning rates computed trial 21-80 by condition:

```{r}
#Extract unique participant IDs with their corresponding neurotype:
df_R_unique <- read.csv("LM_Data.csv") %>%
  select(subject, neurotype) %>%
  distinct(subject, .keep_all = TRUE)  # Keep only one diagnosis per subject

#Merge neurotype with learning rate data:
LR_21_80_unique <- LR_21_80 %>%
  left_join(df_R_unique, by = "subject")

#Create a long form data frame:
long_LR_21_80 <- pivot_longer(LR_21_80_unique, 
                             cols = c(stableAlpha, volatileAlpha), 
                             names_to = "condition", 
                             values_to = "LR")  
#Boxplot:
ggplot(data = long_LR_21_80, aes(x = condition, y = LR, fill = condition)) +
  geom_boxplot(alpha = 0.3, width = 0.4, position = position_dodge(width = 0.8)) +  # Side-by-side boxplots for each condition
  labs(x = "Condition", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 21-80 Plotted By 
Condition and Group") +
  scale_fill_manual(values = c("stableAlpha" = "#cc99ff", "volatileAlpha" = "#66cccc")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  facet_wrap(~neurotype)  # Separate by group (NT and ASC)

#Calculate means for each condition:
mean_long_LR_21_80 <- long_LR_21_80 %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))

#Plot means:
ggplot(data = long_LR_21_80, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.1) +
  labs(x = "Condition", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 21-80 Plotted By 
Condition") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  ) +
  # Add mean points for each neurotype group
  geom_point(data = mean_long_LR_21_80, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype group
  geom_line(data = mean_long_LR_21_80, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1) +
  facet_grid(. ~ neurotype)  # Swapped rows and columns to make groups on top

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = long_LR_21_80, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Condition", y = "Learning Rate", color = "Participant") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Learning Rates Computed From Trials 21-80 Plotted 
By Condition") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  facet_grid(. ~ neurotype)
```

For both NT and ASC groups, participants demonstrate higher average learning rates in the volatile condition compared to the stable condition - this trend is in keeping with what we expect from a learning task with stable and volatile learning conditions.

Also visualise learning rates computed for each block order for the 21-80 trials:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Merge data frames:
order_data_21_80 <- merge(df_R, LR_21_80, by = "subject")
order_data_21_80$neurotype <- as.factor(order_data_21_80$neurotype)

#Filter by block order:
Stable1st_21_80 <- order_data_21_80 %>%
  filter(blockorder == "S/V") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )
Volatile1st_21_80 <- order_data_21_80 %>%
  filter(blockorder == "V/S") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )

#Calculate means for each condition:
mean_Stable1st_21_80 <- Stable1st_21_80 %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))
mean_Volatile1st_21_80 <- Volatile1st_21_80 %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))

#Plot means:
ggplot(data = Stable1st_21_80, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 21-80 Plotted 
By Condition") +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Stable1st_21_80, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Stable1st_21_80, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1) +
ggplot(data = Volatile1st_21_80, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `neurotype` facet labels
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Volatile1st_21_80, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Volatile1st_21_80, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1)
ggsave( 'LR_21_80.png', plot = last_plot(), dpi = 300)

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Stable1st_21_80, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "Learning Rate", color = "Participant") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Learning Rates Computed From Trials 21-80 Plotted 
By Condition") +
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)  +

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Volatile1st_21_80, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "", color = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `diagnosis` facet labels
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)
```

Both NT and ASC participants demonstrate higher learning rates in the volatile compared to the stable condition, regardless of which condition was completed first - the paradigm seemingly captures a difference in learning rates for stable and volatile environments. In addition, there doesn't seem to be much difference between learning rates by group - research suggests that autism is associated with overall higher learning rates (Crawley et al., 2020), such that we expected learning rates to be higher in both stable and volatile conditions for the ASC group compared to the NT group.

Also review learning rates calculated using all trials in each condition; visualise learning rates by condition:

```{r}
#Extract unique participant IDs with their corresponding neurotype information:
df_R_unique <- read.csv("LM_Data.csv") %>%
  select(subject, neurotype) %>%
  distinct(subject, .keep_all = TRUE)  # Keep only one neurotype per subject

#Merge diagnositic information with learning rate data:
LR_unique <- LR %>%
  left_join(df_R_unique, by = "subject")

#Create a long form data frame:
long_LR <- pivot_longer(LR_unique, 
                                        cols = c(stableAlpha, volatileAlpha), 
                                        names_to = "condition", 
                                        values_to = "LR")

#Boxplot:
ggplot(data = long_LR, aes(x = condition, y = LR, fill = condition)) +
  geom_boxplot(alpha = 0.3, width = 0.4, position = position_dodge(width = 0.8)) +  # Side-by-side boxplots for each condition
  labs(x = "Condition", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 1-80 Plotted By 
Condition and Group") +
  scale_fill_manual(values = c("stableAlpha" = "#cc99ff", "volatileAlpha" = "#66cccc")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  facet_wrap(~neurotype)  # Separate by group (NT and ASC)

#Calculate means for each condition:
mean_data <- long_LR %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))

#Plot means:
ggplot(data = long_LR, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.1) +
  labs(x = "Condition", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 1-80 Plotted By 
Condition") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_data, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_data, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1) +
  facet_grid(. ~ neurotype)  # Swapped rows and columns to make groups on top

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = long_LR, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "Condition", y = "Learning Rate", color = "Participant") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Learning Rates Computed From Trials 1-80 Plotted 
By Condition") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  facet_grid(. ~ neurotype)
```

Again, for both NT and ASC groups, participants demonstrate on average higher in the volatile condition compared to the stable condition - this trend is still in keeping with what we expect from a learning task with stable and volatile learning conditions.

Also visualise learning rates computed for each block order:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Merge data frames:
order_data <- merge(df_R, LR, by = "subject")

#Filter by block order:
Stable1st <- order_data %>%
  filter(blockorder == "S/V") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )
Volatile1st <- order_data %>%
  filter(blockorder == "V/S") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )

#Calculate means for each condition:
mean_Stable1st <- Stable1st %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))
mean_Volatile1st <- Volatile1st %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))

#Plot means:
ggplot(data = Stable1st, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 1-80 Plotted 
By Condition") +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Stable1st, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Stable1st, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1) +
ggplot(data = Volatile1st, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `neurotype` facet labels
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each diagnostic group
  geom_point(data = mean_Volatile1st, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each diagnostic group
  geom_line(data = mean_Volatile1st, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1)

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Stable1st, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "Learning Rate", color = "Participant") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Learning Rates Computed From Trials 1-80 Plotted 
By Condition") +
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)  +

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Volatile1st, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "", color = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `diagnosis` facet labels
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)
```

Again, both NT and ASC participants demonstrate higher learning rates in the volatile compared to the stable condition, regardless of which condition was completed first - the paradigm effectively captures a difference in learning rates for stable and volatile environments. Interestingly, including all trials in the learning model influences the trend for autistic individuals who completed the stable condition first. For these participants, there is a less defined difference between learning rates generated in the stable compared to volatile condition. It is possible that this reduce difference was caused by higher learning rates in the first 20 trials of the learning rate because participants were still attempting to understand the task (i.e., learn the initial rule). Again, the first 20 trials of the stable condition (when completed before the volatile condition) are similar to a volatile phase (i.e., 20 trials of a new rule to be learnt).

Finally, review the first 20 trials of each condition:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Merge data frames:
order_data_01_20 <- merge(df_R, LR_01_20, by = "subject")
order_data_01_20$neurotype <- as.factor(order_data_01_20$neurotype)

#Filter by block order:
Stable1st_01_20 <- order_data_01_20 %>%
  filter(blockorder == "S/V") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )
Volatile1st_01_20 <- order_data_01_20 %>%
  filter(blockorder == "V/S") %>%
  group_by(subject) %>%
  pivot_longer(
    cols = c(stableAlpha, volatileAlpha), 
    names_to = "condition", 
    values_to = "LR"
  )

#Calculate means for each condition:
mean_Stable1st_01_20 <- Stable1st_01_20 %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))
mean_Volatile1st_01_20 <- Volatile1st_01_20 %>%
  group_by(neurotype, condition) %>%
  dplyr::summarize(mean_LR = mean(LR, na.rm = TRUE))

#Plot means:
ggplot(data = Stable1st_01_20, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "Learning Rate") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Learning Rates Computed From Trials 1-20 Plotted 
By Condition") +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Stable1st_01_20, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Stable1st_01_20, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1) +
ggplot(data = Volatile1st_01_20, aes(x = condition, y = LR)) +
  geom_jitter(color = "#9ED4D1", width = 0.0) +
  labs(x = "", y = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `neurotype` facet labels
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Volatile1st_01_20, aes(x = condition, y = mean_LR), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Volatile1st_01_20, aes(x = condition, y = mean_LR, group = neurotype),
            color = "#336666", size = 1)

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Stable1st_01_20, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "Learning Rate", color = "Participant") +
  scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ggtitle("Learning Rates Computed From Trials 1-20 Plotted 
By Condition") +
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)  +

#Plot line connecting indevidual stable and volatile learning rates:
ggplot(data = Volatile1st_01_20, aes(x = condition, y = LR, group = subject, color = as.factor(subject))) +
  geom_line() +  
  geom_point() +  
  labs(x = "", y = "", color = "") +
  scale_x_discrete(limits = c("volatileAlpha", "stableAlpha"), labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  scale_color_viridis_d() +  # Automatically assigns distinct colors
  ylim(0,1) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none",
    legend.spacing.y = unit(0.1, "cm"),  # Vertical spacing control
    legend.margin = margin(0, 0, 0, 0),
    legend.key.height = unit(0.1, "cm"),  # Controls the height of each key in the legend
    legend.text = element_text(size = 10),  # Adjusts the text size in the legend for readability
    #strip.text = element_text(size = 12),  # Keep `neurotype` facet labels
    strip.text.y = element_blank()  # Remove `blockorder` facet label
  ) +
  guides(color = guide_legend(ncol = 1)) +
  facet_grid(blockorder ~ neurotype)
```

Indeed, autistic individuals seemingly show higher learning rates in stable (S/V) condition (block order) compared to neurotypical individuals.

# Check behavioural data for each condition

Plot the proportion of lose-shift behaviours for each condition:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Filter by block order:
Stable1st <- df_R %>%
  filter(blockorder == "S/V") %>% 
  select(subject, neurotype, blockorder, blockLabel, lose_shift_proportion_cond) 
Volatile1st <- df_R %>%
  filter(blockorder == "V/S") %>% 
  select(subject, neurotype, blockorder, blockLabel, lose_shift_proportion_cond) 

#Calculate means for each condition:
mean_Stable1st <- Stable1st %>%
  group_by(neurotype, blockLabel) %>%
  dplyr::summarize(mean_LS = mean(lose_shift_proportion_cond, na.rm = TRUE))
mean_Volatile1st <- Volatile1st %>%
  group_by(neurotype, blockLabel) %>%
  dplyr::summarize(mean_LS = mean(lose_shift_proportion_cond, na.rm = TRUE))

#Plot means:
ggplot(data = Stable1st, aes(x = blockLabel, y = lose_shift_proportion_cond)) +
  geom_point(color = "#9ED4D1") +
  labs(x = "", y = "Proportion Of Lose-Shift Behaviours") +
  #scale_x_discrete(labels = c("stableAlpha" = "Stable", "volatileAlpha" = "Volatile")) +
  ggtitle("Proportion Of Lose-Shift Behaviours Plotted 
By Group and Condition") +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),  # Keep `neurotype` facet labels
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Stable1st, aes(x = blockLabel, y = mean_LS), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Stable1st, aes(x = blockLabel, y = mean_LS, group = neurotype),
            color = "#336666", size = 1) +
ggplot(data = Volatile1st, aes(x = blockLabel, y = lose_shift_proportion_cond)) +
  geom_point(color = "#9ED4D1") +
  labs(x = "", y = "") +
  scale_x_discrete(limits = c("Volatile", "Stable")) +
  ylim(0,1) +
  facet_grid(blockorder ~ neurotype) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    #strip.text = element_text(size = 12),  # Keep `diagnosis` facet labels
    strip.text.y = element_blank() 
  ) +
  # Add mean points for each neurotype
  geom_point(data = mean_Volatile1st, aes(x = blockLabel, y = mean_LS), 
             color = "#336666", size = 4, shape = 18) +  # Shape 18 for diamond
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_Volatile1st, aes(x = blockLabel, y = mean_LS, group = neurotype),
            color = "#336666", size = 1)
ggsave( 'LS.png', plot = last_plot(), dpi = 300)
```

Lose-shift behaviours are related to learning rates; “higher learning rates weight the most recent outcome more heavily, with full lose-shift behaviour – dependence only on the most recent outcome – equivalent to a learning rate of 1” (Piray & Daw, 2021, pp.8). As such, lose-shift behaviours should map on to learning rates - lose-shift behaviours should be more frequent in the volatile verses the stable condition, reflecting greater feedback sensitivity in volatile environments. For both groups, we see this trend, though less pronounced than the difference between learning rates in stable versus volatile conditions. In addition, there doesn't seem to be much difference between lose-shift behaviours by neurotype - research suggests lose-shift behaviours are associated with autism (Crawley et al., 2020; D'Crus et al., 2013), such that we expected lose-shift behaviours to be greater for the ASC group compared to the NT group.

Plot the number of perseverative errors by group:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Calculate means for each condition:
mean_P <- df_R %>%
  group_by(neurotype) %>%
  dplyr::summarize(mean_P = mean(perseveration, na.rm = TRUE))
 
# Ensure neurotype is a factor
df_R$neurotype <- as.factor(df_R$neurotype)
mean_P$neurotype <- factor(mean_P$neurotype, levels = levels(df_R$neurotype))

#Plot
ggplot(data = df_R, aes(x = neurotype, y = perseveration)) +
  geom_violin(color = "#336666" , fill = "#9ED4D1", alpha = 0.5) +
  geom_point(color = "#336666" , size = 2) +
  labs(
    x = "Group",
    y = "Perseverative Responses",
    title = "Distribution of Perseverative Behaviours by Group"
  ) +
  facet_grid(. ~ blockorder, labeller = labeller(blockorder = c("S/V" = "Stable - Volatile", "V/S" = "Volatile - Stable"))) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  # Add lines at the mean values for each group
  geom_segment(data = mean_P, aes(x = as.numeric(neurotype) - 0.1, 
                                   xend = as.numeric(neurotype) + 0.1, 
                                   y = mean_P, yend = mean_P), 
               color = "#336999", size = 1.2)
ggsave( 'P.png', plot = last_plot(), dpi = 300)
```

The number of perseverative behaviours are similarly distributed for both group - we expected the number of perseverative behaviours (i.e., the number of errors made after the first reversal before a correct response) to be greater for the ASC group compared to the NT gorup - this was based on Crawley et al. (2020) and Coldren & Halloran (2010) who both found autism to be associated with an elevated number of perseverative behaviours compared to neurotypical controls.

Plot the number of regressive errors by group:

```{r}
#Load data:
df_R <- read.csv("LM_Data.csv")

#Calculate means for each condition:
mean_R <- df_R %>%
  group_by(neurotype) %>%
  dplyr::summarize(mean_R = mean(regression, na.rm = TRUE))
 
# Ensure neurotype is a factor
df_R$neurotype <- as.factor(df_R$neurotype)
mean_R$neurotype <- factor(mean_R$neurotype, levels = levels(df_R$neurotype))

#Plot
ggplot(data = df_R, aes(x = neurotype, y = regression)) +
  geom_violin(color = "#336666" , fill = "#9ED4D1", alpha = 0.5) +
  geom_point(color = "#336666" , size = 2) +
  labs(
    x = "Group",
    y = "Regressive Responses",
    title = "Distribution of Regressive Behaviours by Group"
  ) +
   facet_grid(. ~ blockorder, labeller = labeller(blockorder = c("S/V" = "Stable - Volatile", "V/S" = "Volatile - Stable"))) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.position = "none"
  ) +
  # Add lines at the mean values for each group
  geom_segment(data = mean_R, aes(x = as.numeric(neurotype) - 0.1, 
                                   xend = as.numeric(neurotype) + 0.1, 
                                   y = mean_R, yend = mean_R), 
               color = "#336999", size = 1.2)
ggsave( 'R.png', plot = last_plot(), dpi = 300)
```

The number of regressive behaviours are also similarly distributed for both neurotype - much like perseverative behaviours, we expected the number of regressive errors to be greater for the ASC group compared to the NT group - this was based on Crawley et al. (2020), Coldren & Halloran (2010) and Goris et al., (2021; who found no difference in leanring rates between autistic traits and stable/volatile conditions, but did find a primacy bias - a tendency for higher autistic traits to be associated with a return to previously rewarded stimuli after reversal).

Note: there are more regressive behaviours in the volatile-stable block presentation - this is because regressions are counted as a return to the previous rule after a reversal, and a change from volatile to stable is classed as a reversal. Thus, those who completed the volatile condition first had an additional 80 opportunities to regress to the previously rewarded stimulus choice.

# Correlations between learning rates and behaviours

Load in data and merge files:

```{r}
#Load data and rename subject column:
LR_21_80 <- read.csv("results_RL_summary_21_80.csv") 
colnames(LR_21_80)[colnames(LR_21_80) == "subject"] <- "subject"
df_R <- read.csv("Pilot_LM_Data.csv")

#Merge data frames:
data <- merge(df_R, 
                    LR_21_80, 
                    by = "subject", all = TRUE)
```

Calculate reverse log learning rates:

```{r}
#As Browning et al. (2015), calculate reverse log learning rate as log(LR in volatile) - log(LR in stable):
data$RLLR <- log(data$volatileAlpha) - log(data$stableAlpha)
```

Now generate a correlation matrix including all behavioural variables and RLLR:

```{r}
#Select behavioural measures and RLLR:
selected_vars <- data[, c("RLLR", "lose_shift", "perseveration", "regression", "MASQ", "EDAQ")] 

#Compute the correlation matrix:
cor(selected_vars, use = "complete.obs")  #use = "complete.obs" excludes NA values
```

Plot these correlations:

```{r}
#Round and store correlations:
cormat <- round(cor(selected_vars, use = "complete.obs"),2)

#Reshape data frame to long format using melt:
melted_cormat <- melt(cormat)

#Get lower triangle of the correlation matrix:
  get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  #Get upper triangle of the correlation matrix:
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
reorder_cormat <- function(cormat){
#Use correlation between variables as distance:
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

#Reorder the correlation matrix:
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)

#Melt the correlation matrix:
melted_cormat <- melt(upper_tri, na.rm = TRUE)

#Plot correlation matrix:
ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "#66cccc", high = "#cc99ff", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed() + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank())
ggsave( 'Corr.png', plot = last_plot(), dpi = 300)
```
