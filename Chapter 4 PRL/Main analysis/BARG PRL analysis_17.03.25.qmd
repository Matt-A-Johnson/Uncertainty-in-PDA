---
title: "BARG simulation_PRL_18.10.24"
format: html
editor: visual
---

# Package installation

```{r}
#Install packages and libraries:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot", "correlation", "patchwork")

#Install packages if not already installed:
packages_to_install <- packages[!packages %in% installed.packages()]
if(length(packages_to_install)) install.packages(packages_to_install, dependencies = TRUE)

#If not already installed, install rethinking() separately:
#install.packages("rethinking", 
#                 repos=c(cran="https://cloud.r-project.org",
#                         rethinking="http://xcelab.net/R"))

#Remove and reinstall loo if experiencing issues with add_criterion():
remove.packages("loo")
remotes::install_github("stan-dev/loo")

#Rstan might need a bit of extra attention. If it doesn't install with the above code, remove any existing RStan via:
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")

#Set up compiler flags:
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXX17FLAGS=-O3 -march=native -mtune=native -fPIC",
    "CXX17=g++", # or clang++ but you may need a version postfix
    file = M, sep = "\n", append = TRUE)

#This code is for the development version of rstan- I've been told that this might function better than the up-to-date version:
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

#To verify your installation, you can run the RStan example/test model:
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

```{r}
#Once installed, load libraries using groundhog():
library(groundhog)

#Specify packages:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot", "correlation", "patchwork")

#Load packages:
groundhog.library(packages, "2024-06-28", tolerate.R.version='4.4.0')
```

```{r}
#If that doesn't work, load packages manually:
library(rstan)
library(cmdstanr)
library(devtools)
library(rethinking) #Add download above
library(V8)
library(brms)
library(tidyverse) 
library(plotrix)
library(gridExtra)
library(tidybayes)
library(modelsummary)
library(bayesplot)
library(correlation)
library(patchwork)
```

# Preamble

## Why Bayesian?

Global autism prevalence is estimated at approx. 1 in 100 (Zeidan et al., 2022). Little is known about Pathological Demand Avoidance (PDA) and its prevalence. However, it seems reasonable to assume that if PDA represents a proportion of the autistic population, prevalence of PDA is likely \<1 in 100. Additionally, the percentage of PDA and/or autistic individuals willing and able to participate in research is likely smaller still. Compared to frequentist methods that require large samples to produce precise, well-powered results, Bayesian approaches rely on estimates that have a clear and valid interpretation, no matter the sample size- though it is important to note that Bayesian estimates are dependent upon the initial plausibilities assigned to them (i.e., priors), which is especially true for estimates derived from small sample sizes. In this regard, Bayesian modelling offers us a powerful tool with which to better understand individuals from underrepresented groups, providing the priors we fit to our model are well justified- this will be explained and exemplified in our prior predictive checks and sensitivity analysis.

Furthermore, the autistic population is renowned for its heterogeneity (for an overview of heterogeneity in autism, see Masi et al., 2017), which often produces noisy data that is difficult to interpret. Though data generated by PDA individuals (not third-party report) is limited, its potential relationship to autism means that PDA data may be noisy, too. Again, Bayesian analyses allow us to incorporate prior knowledge about the parameters in the form of priors. This can be particularly useful when dealing with noisy data, as prior information can help regularize the estimates and improve model stability. In addition, Bayesian methods provide a way to quantify uncertainty in parameter estimates through posterior distributions. This is especially important when dealing with noisy data, as it allows us to assess the reliability of estimates and make more informed decisions.

## Goals of analysis

We were interested in how PDA, autism, and anxierty are associated with probabilistic learning. We leveraged a probabilistic reversal learning task in which shapes were probabilistically associated with reward and loss: reward contingencies were set at 80:20 and randomised for shape stimuli across participants (e.g., either a triangle or a circle would earn points 80% of the time and lose points 20% of the time, and vice versa). Participants completed two blocks: a stable block in which reward contingencies remained constant throughout (e.g., a circle would reward points 80% and lose points 20%, and vice versa for a triangle), and a volatilie block in which reward contingencies would reverse every 20 trials (e.g. Â a circle would reward points 80% and lose points 20%, and vice versa for a triangle for 20 trials, then a triangle would reward points 80% and lose points 20%, and vice versa for a circle).

We have two primary goals:

1\) To better understand probabilistic learning in the context of PDA. We wanted to know if PDA is associated with a tendency to learn similarly in both stable and volatile conditions (i.e., reduced RLLR values). We predicted that autism, anxiety, and PDA would be negatively associated with RLLR and positively associated with lose-shift behaviours.

[***\[2) To examine the relationship between PDA, autism, and anxiety. We aimed to determine whether PDA is associated with behavioural patterns similar to those observed in autism and anxiety. We predicted that, for neurotypical participants, PDA and anxiety would be associated with fewer perseverative and regressive behaviours. In contrast, we predicted that autism would be linked to elevated perseverative and regressive behaviours, regardless of PDA or anxiety.\]***]{.underline}

## Causal model

Below is a directed acyclical graphic (DAG) representation of our causal model. "SensoryProcessing" refers to sensory processing differences thought to underpin perception, cognition, and behaviour in autism (Palmer et al., 2017; Van de Cruys et al., 2014); probabilistic learning is thought to represent one facet of these sensory processing differences, and the facet we attempt to probe in this study. "Autism" here refers to a binary category- possessing a formal, clinical diagnosis of autism, or not. Finally, "PDA" refers to a spectrum of behavioural characteristics thought to be associated with PDA.

```{r}
dag_m <- dagitty( "dag {
    Autism -> PDA
    SensoryProcessing -> PDA
    SensoryProcessing -> Autism
}")
coordinates( dag_m ) <- list( x=c(Autism=0, PDA=1, SensoryProcessing=-1) , y=c(Autism=-0.5, PDA=1, SensoryProcessing=1) )
drawdag(dag_m)
```

Previous research suggests that aberrant probability learning is associated with autism (Lawson et al., 2017; Reisli et al., 2023). This compliments predictive processing theories that suggest aberrant handling of sensory information relative to prior knowledge might underpin autistic development (for review, see Cannon et al., 2021 and Chrysaitis & Series, 2023). Thus, a latent measure of probabilistic learning should causally influence an autism diagnosis. The relationship between PDA and autism is contentious- some view PDA as an autism subgroup (Christie, 2007), others posit that PDA is not different from autism (Milton, 2013; Moore, 2020), while others argue that PDA is a common mental health condition prevalent in the general population (Woods, 2018). Given that autism might underpin the development of PDA behaviours, we include a causal link between autism and PDA. The relationship between probabilistic learning (under the umbrella of sensory processing differences, labelled in the DAG above as SensoryProcessing) and PDA has yet to be empirically questioned- this relationship represents our primary interest.

# Data preparation

## Load data

Load and inspect structure of summary and raw data frames:

```{r}
#Load behavioural data, selecting relevant columns and summarize to retain a single row per participant:
B_df <- read.csv("LM_Data.csv") %>%
  group_by(subject) %>%
  summarise(
    perseveration = mean(perseveration, na.rm = TRUE),
    regression = mean(regression, na.rm = TRUE),
    perseveration_proportion = mean(perseveration_proportion, na.rm = TRUE),
    regression_proportion = mean(regression_proportion, na.rm = TRUE),
    loseShift = mean(lose_shift_proportion, na.rm = TRUE),
    neurotype = factor(first(neurotype)), 
    MASQ = mean(MASQ, na.rm = TRUE),
    EDAQ = mean(EDAQ, na.rm = TRUE),
    blockorder = factor(first(blockorder)),
    .groups = 'drop'
  )

#Load in learning rate data, selecting relevant columns and calculating relative log learning rates (RLLRs):
LR_df <- read.csv("results_RL_summary_21_80.csv") %>%   #If interested, run the analysis using the complete choice data (i.e., inc. trials 1-80), read.csv("results_RL_summary.csv"), or the first 20 trials, read.csv("results_RL_summary_01_20.csv")
  select(subject, stableAlpha, stableBeta, volatileAlpha, volatileBeta) %>%
  mutate(RLLR = log(volatileAlpha) - log(stableAlpha))

#Merge the data frames by subject:
data <- B_df %>%
  inner_join(LR_df, by = "subject")

#View the merged data frame:
head(data)
str(data)
```

Write the data to a .csv file:

```{r}
#Write data to csv for modelling (change name of file as appropriate):
write.table(data, file="Data.csv",sep=",",row.names=F)
```

## Visualisation

### Behaviour plotted by neurotype, PDA and anxiety

#### Lose-shift behaviour

We can also visualise the relationship between lose-shift behaviours and MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Plotting lose-shift as a funciton or EDA-QA and MASQ scores:

#Plot total scores (without neurotype):
ggplot(data = data, aes(x = EDAQ, y = loseShift, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = loseShift), fill = "#336666") +
  labs(x = "EDA-QA scores", y = "Proportion of Lose-Shift") +
  ggtitle("Relationship between Lose-Shift and EDA-QA scores") +
  theme_minimal() +
  scale_color_identity()

ggplot(data = data, aes(x = MASQ, y = loseShift, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = loseShift), fill = "#336666") +
  labs(x = "MASQ scores", y = "Proportion of Lose-Shift") +
  ggtitle("Relationship between Lose-Shift and MASQ scores") +
  theme_minimal() +
  scale_color_identity()

#Plot by neurotype:
ggplot(data = data, aes(x = EDAQ, y = loseShift, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = loseShift, fill = neurotype),  show.legend = FALSE) +
  labs(x = "EDA-QA scores", y = "Proportion of Lose-Shift", color = "Neurotype") +
  ggtitle("Relationship between Lose-Shift and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )

ggplot(data = data, aes(x = MASQ, y = loseShift, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = loseShift, fill = neurotype), show.legend = FALSE) +
  labs(x = "MASQ scores", y = "Proportion of Lose-Shift", color = "Neurotype") +
  ggtitle("Relationship between Lose-Shift and MASQ scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
```

We predicted 1) that the number of lose-shift behaviours (i.e., a response switch made after receiving negative feedback, reported as a proportion of total negative feedback trials) to be greater for the ASC group compared to the NT group - this was based on Crawley et al. (2020) and D'Crus et al. (2013). Similarly, 2) we expected that the number of lose-shift behaviours would be positively associated with anxiety - this was based on Piray and Daw (2021) theoretical model, as well as findings from Huang et al. (2017) and [Hein et al. (2023)](http://nature.com/articles/s42003-023-04628-1.pdf). 3) We predicted that the number of lose-shift behaviours produced by the ASC group would be similar irrespective of anxiety - this reflects previous findings that suggest autism is associated with lose-shift behaviours (Crawley et al., 2020; D'Crus et al., 2013). For the NT group, we predicted that lose-shift behaviours would be positively associated with anxiety - this is supported by literature that suggests anxiety is associated with more lose-shift behaviours (Huang et al., 2017; [Hein et al. (2023](http://nature.com/articles/s42003-023-04628-1.pdf))*.* We predicted a similar pattern of lose-shift behaviours as a function of PDA behaviours, such that the number of lose-shift behaviours produced by the ASC group would be similar irrespective of PDA behaviours, while for the NT gorup, lose-shift behaviours would be positively associated with PDA behaviours.

The above plots suggest that EDA-QA and MASQ scores for both group are slightly positively related to lose-shift behaviours.

#### Perseverative behaviour

We can also visualise perseverative behaviours:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Plotting perseveration as a funciton or EDA-QA and MASQ scores:

#Plot total scores (without neurotype):
ggplot(data = data, aes(x = EDAQ, y = perseveration, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = perseveration), fill = "#336666") +
  labs(x = "EDA-QA scores", y = "Number of Perseveration") +
  ggtitle("Relationship between perseveration and EDA-QA scores") +
  theme_minimal() +
  scale_color_identity()

ggplot(data = data, aes(x = MASQ, y = perseveration, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = perseveration), fill = "#336666") +
  labs(x = "MASQ scores", y = "Number of Perseveration") +
  ggtitle("Relationship between perseveration and MASQ scores") +
  theme_minimal() +
  scale_color_identity()

#Plot by neurotype:
ggplot(data = data, aes(x = EDAQ, y = perseveration, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = perseveration, fill = neurotype),  show.legend = FALSE) +
  labs(x = "EDA-QA scores", y = "Number of Perseveration", color = "Neurotype") +
  ggtitle("Relationship between perseveration and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )

ggplot(data = data, aes(x = MASQ, y = perseveration, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = perseveration, fill = neurotype), show.legend = FALSE) +
  labs(x = "MASQ scores", y = "Number of Perseveration", color = "Neurotype") +
  ggtitle("Relationship between perseveration and MASQ scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
```

We predicted 1) that the number of perseverative behaviours (i.e., the number of errors made after the first reversal before a correct response) to be greater for the ASC group compared to the NT gorup - this was based on Crawley et al. (2020) and Coldren & Halloran (2010). Similarly, 2) we expected that anxiety would be negatively associated with perseverative behaviours - this was based on Fang et al. (2024) and Zhukovsky et al. (2017) who both found perseveration to be negatively correlated with anxiety, with the former noting that the severity of anxiety symptoms was negatively associated wiht a tendency for perseverative strategies. Because PDA is thought to be underpinned by anxiety (Johnson & Saunderson, 2023; Stuart et al., 2020), we assumeed that PDA behaviours would also be negatively associated with perseverative behaviours.

The above plots suggest that for the NT group, EDA-QA scores are slightly negatively related to perseverative behaviours, while MASQ scores are slightly postively related. For the ASC group, it seems that EDA-QA and MASQ scores are slightly negatively related to perseveration.

#### Regressive behaviour

We can also visualise regressive behaviours:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Plotting regression as a funciton or EDA-QA and MASQ scores:

#Plot total scores (without neurotype):
ggplot(data = data, aes(x = EDAQ, y = regression , color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = regression), fill = "#336666") +
  labs(x = "EDA-QA scores", y = "Number of Regressions") +
  ggtitle("Relationship between regression and EDA-QA scores") +
  theme_minimal() +
  scale_color_identity()

ggplot(data = data, aes(x = MASQ, y = regression, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = regression), fill = "#336666") +
  labs(x = "MASQ scores", y = "Number of Regressions") +
  ggtitle("Relationship between regression and MASQ scores") +
  theme_minimal() +
  scale_color_identity()

#Plot by neurotype:
ggplot(data = data, aes(x = EDAQ, y = regression   , color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = regression, fill = neurotype),  show.legend = FALSE) +
  labs(x = "EDA-QA scores", y = "Number of Regressions", color = "Neurotype") +
  ggtitle("Relationship between regression and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )

ggplot(data = data, aes(x = MASQ, y = regression   , color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = regression, fill = neurotype), show.legend = FALSE) +
  labs(x = "MASQ scores", y = "Number of Regressions", color = "Neurotype") +
  ggtitle("Relationship between regression and MASQ scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
```

Much like perseverative behaviours, 1) we predicted that the number of regressive behaviours (i.e., the number of errors made after the first reversal after at least one correct response) to be greater for the ASC group compared to the NT group - this was based on Crawley et al. (2020), Coldren & Halloran (2010) and Goris et al., (2021; who found no difference in learning rates between autistic traits and stable/volatile conditions, but did find a primacy bias - a tendency for higher autistic traits to be associated with a return to previously rewarded stimuli after reversal). Similarly, 2) we expected that anxiety would be negatively associated with regressive behaviours - this was based on Fang et al. (2024) and Zhukovsky et al. (2017). Because PDA is thought to be underpinned by anxiety (Johnson & Saunderson, 2023; Stuart et al., 2020), we assumed that PDA behaviours would also be negatively associated with regressive behaviours.

The above plots suggest that both EDA-QA and MASQ scores are positively related to regressive behaviours, and this seems to be the case for both NT and ASC groups.

### Learning rate predicted by neurotype, PDA and anxiety

We can also visualise the relationship between relative log learning rates (RLLRs) and MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Plot total scores (without neurotype):
ggplot(data = data, aes(x = EDAQ, y = RLLR, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR), fill = "#336666") +
  labs(x = "EDA-QA scores", y = "Relative Log Learning Rate") +
  ggtitle("Relationship between probability slopes and 
EDA-QA scores") +
  theme_minimal() +
  scale_color_identity()

ggplot(data = data, aes(x = MASQ, y = RLLR, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR), fill = "#336666") +
  labs(x = "MASQ scores", y = "Relative Log Learning Rate") +
  ggtitle("Relationship between probability slopes and MASQ scores") +
  theme_minimal() +
  scale_color_identity()

#Plot by neurotype:
ggplot(data = data, aes(x = EDAQ, y = RLLR, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR, fill = neurotype),  show.legend = FALSE) +
  labs(x = "EDA-QA scores", y = "Relative Log Learning Rate", color = "Neurotype") +
  ggtitle("Relationship between probability slopes and 
EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )

ggplot(data = data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR, fill = neurotype), show.legend = FALSE) +
  labs(x = "MASQ scores", y = "Relative Log Learning Rate", color = "Neurotype") +
  ggtitle("Relationship between probability slopes and 
MASQ scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
```

Based on Browning et al. (2015) (and the assumption that PDA is underpinned by anxiety (Johnson & Saunderson 2023; Stuart et al., 2020), we predicted that both EDA-QA and MASQ scores would negatively correlate with relative log learning rates (i.e., log(LR in volatile block) - log(LR in stable block)). This assumes that PDA and anxiety are associated with reduced change in learning rates (i.e., learning rates for stable and volatile conditions are similar), but that RLLR for ASC individuals are consistently low, reflecting less variability in learning rates for stable and volatile conditions. Plots 3 and 4 above show the same relationships plotted by neurotype; ASC, anxiety, and PDA behaviours are all associated with lower RLLRs.

The above plots suggest that for the NT group, EDA-QA and MASQ scores are negatively related to RLLRs. For the ASC group, it seems that EDA-QA are negatively related to RLLRs, while MASQ scores are slightly postiviely related to RLLRs.

We can also plot these relationships by blockorder - as noted by Browning et al. (2015), if there is truly an effect of block (i.e., learning rates should be higher in the volatile compared to the stable condition) there should not be and effect of blockorder, so the plots should be similar for both S/V and V/S:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Plot total scores (without neurotype):
ggplot(data = data, aes(x = EDAQ, y = RLLR, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR), fill = "#336666") +
  labs(x = "EDA-QA scores", y = "Relative Log Learning Rate") +
  ggtitle("Relationship between RLLR and 
EDA-QA scores") +
  theme_minimal() +
  scale_color_identity() +
  facet_grid(. ~ blockorder)
  

ggplot(data = data, aes(x = MASQ, y = RLLR, color = "#336666")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR), fill = "#336666") +
  labs(x = "MASQ scores", y = "Relative Log Learning Rate") +
  ggtitle("Relationship between RLLR and MASQ scores") +
  theme_minimal() +
  scale_color_identity() +
  facet_grid(. ~ blockorder)

#Plot by neurotype:
ggplot(data = data, aes(x = EDAQ, y = RLLR, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR, fill = neurotype),  show.legend = FALSE) +
  labs(x = "EDA-QA scores", y = "Relative Log Learning Rate", color = "Neurotype") +
  ggtitle("Relationship between RLLR and 
EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  ) +
  facet_grid(. ~ blockorder)

ggplot(data = data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = RLLR, fill = neurotype), show.legend = FALSE) +
  labs(x = "MASQ scores", y = "Relative Log Learning Rate", color = "Neurotype") +
  ggtitle("Relationship between RLLR and 
MASQ scores by neurotype") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff"), breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  ) +
  facet_grid(. ~ blockorder)
```

Indeed, the plots above show that the relationship between EDA-QA and MASQ scores and RLLR is similar, irrespective of blockorder. In other words, regardless of which block participants complete first, the relationship between RLLRs, PDA, and anxiety remain the same.

# Model checking

For those unfamiliar with brms() (specifically with brms() syntax), chapter 12 of the following webpage provides some useful guidance on how to specify a brms() model: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

## Correlating EDA-QA and MASQ scores

Begin by checking that EDA-QA and MASQ scores are correlated - this will be important for interpreting the results of the regression models below:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Calculate Bayesian correlation between EDAQ and MASQ:
EDAQ_MASQ_cor <- brm(
  EDAQ ~ 0 + MASQ,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "EDAQ_MASQ_cor.rds"
  )
```

Load in and inspect model output:

```{r}
#Load the model output from a file:
EDAQ_MASQ_cor <- readRDS("EDAQ_MASQ_cor.rds")

#Return model output:
summary(EDAQ_MASQ_cor)

#Extract the posterior samples for the correlation:
EDAQ_MASQ_cor_posterior <- posterior_samples(EDAQ_MASQ_cor)

#Visualize the posterior distribution:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(EDAQ_MASQ_cor_posterior, pars = "b_MASQ", "sigma", point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Johnson and Saunderson (2023) report a correlation of *r*=0.57 between the EDA-QA and the MASQ-D30 (a short form version of the MASQ) - we find a similar correlation between the full MASQ anxiety sub-scale and the EDA-QA scores (*r*=0.74). Thus, we conclude that EDA-QA and MASQ scores are highly correlated with fairly tight intervals (95% HDI\[0.64, 0.84\]) - we can be fairly certain about this relationship.

We can also plot this correlation:

```{r}
#Load the model output from a file:
EDAQ_MASQ_cor <- readRDS("EDAQ_MASQ_cor.rds")

#First, extract predicted values:
EDAQ_MASQ_cor_preds <- data.frame(fitted(EDAQ_MASQ_cor))

#Add EDA-QA and MASQ scores to the predicted values:
EDAQ_MASQ_cor_preds <- EDAQ_MASQ_cor_preds %>%
  mutate( 
    EDAQ = std_data$EDAQ,
    MASQ = std_data$MASQ
)

#Plot correlation between EDA-QA and MASQ scores with a line representing predicted values and HDI:
ggplot(data = EDAQ_MASQ_cor_preds, aes(x = MASQ, y = EDAQ, color = "#66cccc")) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, fill = "#66cccc", color = NA) +
  labs(x = "EDA-QA scores (std)", y = "MASQ scores (std)") +
  ggtitle("Estimated correlation between simulated EDA-QA and MASQ scores") +
  theme_minimal() +
  scale_color_identity()
```

MASQ and EDA-QA scores are highly correlated. Ultimately, we want to know how the relationship between probabilistic learning in the context of PDA compares to that of probabilistic learning in the context of anxiety. Establishing a correlation between PDA (EDA-QA) and anxiety (MASQ) helps us to interpret any comparisons drawn between how probabilistic learning relates to each construct. For example, if probabilistic learning relates to PDA and anxiety in similar ways, it is possible that model purposed to explain aberrant probabilistic learning in anxiety (e.g., Piray & Daw, 2021) might apply to PDA also. Indeed, it might be that PDA is best understood as an anxiety condition (e.g., Johnson & Saunderson 2023; Stuart et al., 2020), though obviously further exploration would be required to draw such a conclusion. Conversely, if the relationship between probabilistic learning and PDA and anxiety are disparate, this too would be informative. It might be that PDA, though related to anxiety, is better conceptualised as an autism spectrum condition. In this case, we might expect differences in the way probabilistic learning relates to PDA and anxiety, namely, PDA might be associate with more perseverative behaviours - a behavioural departure from Piray and Daw's (2021) explanatory model of probabilistic learning in anxiety. Scenarios like these allow us to identify areas of difficulty, which may help us better understand the underlying facets driving PDA behaviours.

## Behaviour predicted by neurotype, PDA and anxiety

### Lose-shift behaviour

To better understand the relationship between PDA and probabilistic learning, we will run a set of Bayesian regression models, 1) will model lose-shift behaviours predicted by neurotype x MASQ scores, 2) will model lose-shift behaviours predicted by neurotype x MASQ and neurotype x EDA-QA scores. We will perform model selection analysis (LOOCV) to discern which model better accounts for our data. This will help us infer if the EDA-QA is explaining any additional variance in lose-shift behaviours over and above MASQ scores:

$$ loseShift_i \sim Normal( \mu_1, \sigma_1 )  \\ \mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j)\\ \alpha \sim {Normal}(0, 1) \quad \\ \beta_1, \beta_2, \beta_3  \sim {Normal}(0, 1) \quad  \\ \sigma \sim \text{Exponential}(1) $$ And for comparison:

$$
loseShift_i \sim \text{Normal}( \mu_i, \sigma ) \\
\mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j) + \beta_4 \text{EDAQ}_i + \beta_5 (\text{EDAQ}_i \times \text{neurotype}_j) \\
\alpha \sim {Normal}(0, 1) \quad \\
\beta_1, \beta_2, \beta_3, \beta_4, \beta_5  \sim {Normal}(0, 1) \quad  \\
\sigma \sim \text{Exponential}(1)
$$

Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the models to the data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Fit the model where lose-shift behaviours are predicted by MASQ scores by neurotype:
modLSM <- brm(
  loseShift ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modLSM_output.rds"
  )

#Fit the model where lose-shift behaviours are predicted by a MASQ*neurotype and EDA-QA*neurotype:
modLSME <- brm(
  loseShift ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSME_output.rds"
  )
```

Add LOO criterion for model comparison:

```{r}
#Add model LOO criterion (set reloo = TRUE if problematic observations are returned; IMPORTANT - make sure sig_figs = at least 10, otherwise add_criterion returns error: https://discourse.mc-stan.org/t/simplex-variable-error-during-bridgesampling/30811):  
modLSM <- add_criterion(modLSM, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)  
modLSME <- add_criterion(modLSME, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
```

Inspect output:

```{r}
#Load the model output from a file:
modLSM <- readRDS("modLSM_output.rds")
modLSME <- readRDS("modLSME_output.rds")

#Return model output:
summary(modLSM)
summary(modLSME)

#We can also manually check the range of Pareto-k values; anything near or above 0.7 is likely problematic:
#range(loo(modRM)$diagnostics$pareto_k)
#range(loo(modRME)$diagnostics$pareto_k)

#Also, run model comparison:
loo_compare(modLSM, modLSME, criterion = "loo")
```

Interpreting the LOO output above - this will vary slightly each time the analysis is run:

-   **`elpd_diff`**: This is the difference in the expected log predictive density (elpd) between the two models. The higher the elpd, the better the model is at predicting new data. Here, `modLSM` has an elpd difference of 0.0, which means it's used as the reference model. `modLSME` has an elpd difference of -1.0, meaning it is 1.0 unit worse in terms of predictive performance compared to `modLSM`.

-   **`se_diff`**: This is the standard error of the elpd difference. It quantifies the uncertainty in the elpd difference. A standard error of 1.6 for `modLSME` suggests that the estimate of the elpd difference has some variability, coupled with the small difference (-1.0) it seems that `modLSM` is not much better at predicting new data than `modLSME`.

Plot parameter distributions with credible intervals:

```{r}
#Extract posterior samples:
modLSM_posterior <- posterior_samples(modLSM)
modLSME_posterior <- posterior_samples(modLSME)


#Filter out 'lprior' and 'lp__':
modLSM_posterior <- modLSM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modLSME_posterior <- modLSME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modLSM_posterior_long <- pivot_longer(modLSM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modLSME_posterior_long <- pivot_longer(modLSME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modLSM_posterior_long$Parameter <- recode(modLSM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modLSME_posterior_long$Parameter <- recode(modLSME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modLSM_posterior_long$Group <- sub(":.*", "", modLSM_posterior_long$Parameter)
modLSME_posterior_long$Group <- sub(":.*", "", modLSME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modLSM_posterior_long$Parameter <- factor(modLSM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modLSME_posterior_long$Parameter <- factor(modLSME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modLSM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'LS_dist.png', plot = last_plot(), dpi = 300)

ggplot(modLSME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'LS_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

Visualise model-predicted lose-shift behaviours as a function of MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modLSM <- readRDS("modLSM_output.rds")
modLSME <- readRDS("modLSME_output.rds")

#Extract conditional effects:
ce_LSM <- conditional_effects(modLSM)
ce_LSME <- conditional_effects(modLSME)

#Convert the specific effect to a data frames:
ce_LSM_MASQ <- as.data.frame(ce_LSM$`MASQ:neurotype`)
ce_LSME_MASQ <- as.data.frame(ce_LSME$`MASQ:neurotype`)
ce_LSME_EDAQ <- as.data.frame(ce_LSME$`EDAQ:neurotype`)

#Select relevant columns:
ce_LSM_MASQ <- ce_LSM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_LSME_MASQ <- ce_LSME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_LSME_EDAQ <- ce_LSME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_LSM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Lose-Shift Behaviour By Neurotype",
       x = "MASQ Scores (std)",
       y = "Lose-Shift Behaviour (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'LS_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
LSMEplot_masq <- ggplot(ce_LSME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Lose-Shift Behaviour (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
LSMEplot_edaq <- ggplot(ce_LSME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))+
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))

#Combine the plots:
(LSMEplot_masq + LSMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Lose-Shift behaviour By Neurotype")
ggsave( 'LS_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above depict the estimated effect of MASQ and EDA-QA scores on lose-shift behaviours. Again, we expected that 1) the number of lose-shift behaviours would be greater for the ASC group compared to the NT group, 2) the number of lose-shift behaviours would be positively associated with anxiety and PDA behaviours, and 3) the number of lose-shift behaviours produced by the ASC group would be similar irrespective of anxiety and/or PDA behaviours. Modelling of the data tells a different story: models predict that for both NT and ASC groups, lose-shift behaviours are slightly negatively associated with anxiety scores and slightly positively associated with PDA behaviours. However, in both instances, both models predict a large amount of variability in parameter estimates, rendering these results inconclusive.

### Perseverative behaviour

To better understand the relationship between PDA and probabilistic learning, we will run a set of Bayesian regression models, 1) will model perseverative behaviours predicted by neurotype x MASQ scores, 2) will model perseverative behaviours predicted by neurotype x MASQ and neurotype x EDA-QA scores. We will perform model selection analysis (LOOCV) to discern which model better accounts for our data. This will help us infer if the EDA-QA is explaining any additional variance in perseverative behaviours over and above MASQ scores:

$$ perseveration_i \sim Normal( \mu_1, \sigma_1 )  \\ \mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j)\\ \alpha \sim {Normal}(0, 1) \quad \\ \beta_1, \beta_2, \beta_3  \sim {Normal}(0, 1) \quad  \\ \sigma \sim \text{Exponential}(1) $$ And for comparison:

$$
perseveration_i â¼ Normal( Î¼_i, Ï ) \\
\mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j) + \beta_4 \text{EDAQ}_i + \beta_5 (\text{EDAQ}_i \times \text{neurotype}_j) \\
\alpha \sim {Normal}(0, 1) \quad \\
\beta_1, \beta_2, \beta_3, \beta_4, \beta_5  \sim {Normal}(0, 1) \quad  \\
\sigma \sim \text{Exponential}(1)
$$

Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working - because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the models to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Fit the model where perseverative errors are predicted by MASQ scores by neurotype:
modPM <- brm(
  perseveration ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modPM_output.rds"
  )

#Fit the model where perseverative errors are predicted by a MASQ*neurotype and EDA-QA*neurotype:
modPME <- brm(
  perseveration ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modPME_output.rds"
  )
```

Add LOO criterion for model comparison:

```{r}
#Add model LOO criterion (set reloo = TRUE if problematic observations are returned; IMPORTANT - make sure sig_figs = at least 10, otherwise add_criterion returns error: https://discourse.mc-stan.org/t/simplex-variable-error-during-bridgesampling/30811): 
modPM <- add_criterion(modPM, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE) 
modPME <- add_criterion(modPME, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
```

Inspect output for both models:

```{r}
#Load the model output from a file:
modPM <- readRDS("modPM_output.rds")
modPME <- readRDS("modPME_output.rds")

#Return model output:
summary(modPM)
summary(modPME)

#We can also manually check the range of Pareto-k values; anything near or above 0.7 is likely problematic:
#range(loo(modRM)$diagnostics$pareto_k)
#range(loo(modRME)$diagnostics$pareto_k)

#Also, run model comparison:
loo_compare(modPM, modPME, criterion = "loo")
```

Interpreting the LOO output above - this will vary slightly each time the analysis is run:

-   **`elpd_diff`**: This is the difference in the expected log predictive density (elpd) between the two models. The higher the elpd, the better the model is at predicting new data. Here, `sim_modPM` has an elpd difference of 0.0, which means it's used as the reference model. `sim_modPME` has an elpd difference of -0.8, meaning it is 0.8 units worse in terms of predictive performance compared to `sim_modPM`.

-   **`se_diff`**: This is the standard error of the elpd difference. It quantifies the uncertainty in the elpd difference. A standard error of 1.5 for `sim_modPME` suggests that the estimate of the elpd difference has some variability - for the small difference (-0.8) , it suggests that `sim_modPME` is not much better at predicting new data than `sim_modPM`.

Plot parameter distributions with credible intervals:

```{r}
#Extract posterior samples:
modPM_posterior <- posterior_samples(modPM)
modPME_posterior <- posterior_samples(modPME)


#Filter out 'lprior' and 'lp__':
modPM_posterior <- modPM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modPME_posterior <- modPME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modPM_posterior_long <- pivot_longer(modPM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modPME_posterior_long <- pivot_longer(modPME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modPM_posterior_long$Parameter <- recode(modPM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modPME_posterior_long$Parameter <- recode(modPME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modPM_posterior_long$Group <- sub(":.*", "", modPM_posterior_long$Parameter)
modPME_posterior_long$Group <- sub(":.*", "", modPME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modPM_posterior_long$Parameter <- factor(modPM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modPME_posterior_long$Parameter <- factor(modPME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modPM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'P_dist.png', plot = last_plot(), dpi = 300)

ggplot(modPME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'P_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

Visualise model-predicted perseverative behaviours as a function of MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modPM <- readRDS("modPM_output.rds")
modPME <- readRDS("modPME_output.rds")

#Extract conditional effects:
ce_PM <- conditional_effects(modPM)
ce_PME <- conditional_effects(modPME)

#Convert the specific effect to a data frames:
ce_PM_MASQ <- as.data.frame(ce_PM$`MASQ:neurotype`)
ce_PME_MASQ <- as.data.frame(ce_PME$`MASQ:neurotype`)
ce_PME_EDAQ <- as.data.frame(ce_PME$`EDAQ:neurotype`)

#Select relevant columns:
ce_PM_MASQ <- ce_PM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_PME_MASQ <- ce_PME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_PME_EDAQ <- ce_PME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_PM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Perseverative  Errors By Neurotype",
       x = "MASQ Scores (std)",
       y = "Perseveration (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'Per_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
PMEplot_masq <- ggplot(ce_PME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Perseveration (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
PMEplot_edaq <- ggplot(ce_PME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))

#Combine the plots:
(PMEplot_masq + PMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Perseverative Errors By Neurotype")
ggsave( 'Per_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above depict the estimated effect of MASQ and EDA-QA scores on perseverative behaviours. We expected that 1) the number of perseverative behaviours would be greater for the ASC group compared to the NT group, 2) anxiety and PDA would be negatively associated with perseverative behaviours, and 3) the number of perseverative behaviours produced by the ASC group would be similar irrespective of anxiety or PDA. Model estimates suggest that for the NT group, perseveration is positively associated with anxiety, but negatively associated with PDA behaviours. For the ASC group, model estimates predict a slight negative association between perseverative behaviours and anxiety, and a slightly positive association between perseverative behaviours and PDA. Parameter estimates contain a lot of variability though, making these associations negligible.

### Regressive behaviour

To better understand the relationship between PDA and probabilistic learning, we will run a set of Bayesian regression models, 1) will model regressive behaviours predicted by neurotype x MASQ scores, 2) will model regressive behaviours predicted by neurotype x MASQ and neurotype x EDA-QA scores. We will perform model selection analysis (LOOCV) to discern which model better accounts for our data. This will help us infer if the EDA-QA is explaining any additional variance in regressive behaviours over and above MASQ scores:

$$
regression_i \sim Normal( \mu_1, \sigma_1 )  \\
\mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j)\\
\alpha \sim {Normal}(0, 1) \quad \\
\beta_1, \beta_2, \beta_3  \sim {Normal}(0, 1) \quad  \\
\sigma \sim \text{Exponential}(1)
$$ And for comparison:

$$ regression_i â¼ Normal( Î¼_i, Ï ) \\ \mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j) + \beta_4 \text{EDAQ}_i + \beta_5 (\text{EDAQ}_i \times \text{neurotype}_j) \\ \alpha \sim {Normal}(0, 1) \quad \\ \beta_1, \beta_2, \beta_3, \beta_4, \beta_5  \sim {Normal}(0, 1) \quad  \\ \sigma \sim \text{Exponential}(1) $$

Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working - because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the models to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Fit the model where regressive errors are predicted by MASQ scores by neurotype:
modReM <- brm(
  regression ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modReM_output.rds"
  )

#Fit the model where regressive errors are predicted by a MASQ*neurotype and EDA-QA*neurotype:
modReME <- brm(
  regression ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modReME_output.rds"
  )
```

Add LOO criterion for model comparison:

```{r}
#Add model LOO criterion (set reloo = TRUE if problematic observations are returned; IMPORTANT - make sure sig_figs = at least 10, otherwise add_criterion returns error: https://discourse.mc-stan.org/t/simplex-variable-error-during-bridgesampling/30811):
modReM <- add_criterion(modReM, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
modReME <- add_criterion(modReME, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
```

Inspect output for both models:

```{r}
#Load the model output from a file:
modReM <- readRDS("modReM_output.rds")
modReME <- readRDS("modReME_output.rds")

#Return model output:
summary(modReM)
summary(modReME)

#We can also manually check the range of Pareto-k values; anything near or above 0.7 is likely problematic:
#range(loo(modRM)$diagnostics$pareto_k)
#range(loo(modRME)$diagnostics$pareto_k)

#Also, run model comparison:
loo_compare(modReM, modReME, criterion = "loo")
```

Interpreting the LOO output above - this will vary slightly each time the analysis is run:

-   **`elpd_diff`**: This is the difference in the expected log predictive density (elpd) between the two models. The higher the elpd, the better the model is at predicting new data. Here, `sim_modReM` has an elpd difference of 0.0, which means it's used as the reference model. `sim_modReME` has an elpd difference of 0.0, meaning it is 0.0 units worse in terms of predictive performance compared to `sim_modReM`.

-   **`se_diff`**: This is the standard error of the elpd difference. It quantifies the uncertainty in the elpd difference. A standard error of 2.0 for `sim_modReME` suggests that the estimate of the elpd difference has some variability. Combined with the small difference (0.0), this SE indicates that `sim_modReM` is not much better at predicting new data than `sim_modReME`.

Plot parameter distributions with credible intervals:

```{r}
#Extract posterior samples:
modReM_posterior <- posterior_samples(modReM)
modReME_posterior <- posterior_samples(modReME)


#Filter out 'lprior' and 'lp__':
modReM_posterior <- modReM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modReME_posterior <- modReME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modReM_posterior_long <- pivot_longer(modReM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modReME_posterior_long <- pivot_longer(modReME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modReM_posterior_long$Parameter <- recode(modReM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modReME_posterior_long$Parameter <- recode(modReME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modReM_posterior_long$Group <- sub(":.*", "", modReM_posterior_long$Parameter)
modReME_posterior_long$Group <- sub(":.*", "", modReME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modReM_posterior_long$Parameter <- factor(modReM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modReME_posterior_long$Parameter <- factor(modReME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modReM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'R_dist.png', plot = last_plot(), dpi = 300)

ggplot(modReME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'R_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

Visualise model-predicted regressive behaviours as a function of MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modReM <- readRDS("modReM_output.rds")
modReME <- readRDS("modReME_output.rds")

#Extract conditional effects:
ce_ReM <- conditional_effects(modReM)
ce_ReME <- conditional_effects(modReME)

#Convert the specific effect to a data frames:
ce_ReM_MASQ <- as.data.frame(ce_ReM$`MASQ:neurotype`)
ce_ReME_MASQ <- as.data.frame(ce_ReME$`MASQ:neurotype`)
ce_ReME_EDAQ <- as.data.frame(ce_ReME$`EDAQ:neurotype`)

#Select relevant columns:
ce_ReM_MASQ <- ce_ReM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_ReME_MASQ <- ce_ReME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_ReME_EDAQ <- ce_ReME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_ReM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Regressive Errors By Neurotype",
       x = "MASQ Scores (std)",
       y = "Regression (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'Reg_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
ReMEplot_masq <- ggplot(ce_ReME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Regression (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
ReMEplot_edaq <- ggplot(ce_ReME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 

#Combine the plots:
(ReMEplot_masq + ReMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Regressive Errors By Neurotype")
ggsave( 'Reg_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above depict the estimated effect of MASQ and EDA-QA scores on regressive behaviours. Much like perseverative behaviours, we expected that 1) the number of regressions would be greater for the ASC group compared to the NT group, 2) anxiety and PDA would be negatively associated with regressive behaviours, and 3) the number of regressive behaviours produced by the ASC group would be similar irrespective of anxiety or PDA. Model parameter estimates suggest that, for the NT group, regressive behaviours are slightly negatively associated with anxiety, but positively associated with PDA behaviours. For the ASC, model estimates predict a positive association between regressions and anxiety, and a very weak negative association between regression and PDA behaviours. All parameter estimates carry a substantial amount of variability, rendering these predicted relationships negligible.

## Learning rate predicted by neurotype, PDA and anxiety

### Effect of block order on RLLR

First, learning rates should be higher in the volatile compared to the stable condition irrespective of blockorder, so check that there is no effect of blockorder on RLLR:

$$ RLLR_i \sim Normal( \mu_1, \sigma_1 )  \\ \mu_{ij} = \alpha \text{blocckOrder}_i:{neurotype}_j \\ \alpha \text{blocckOrder}_i:{neurotype}_j \sim {Normal}(0, 1) \quad \\ \sigma \sim \text{Exponential}(1) $$

Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the models to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype, blockorder) %>%
  distinct()

#Fit the model where relative log learning rate (RLLR) are predicted by blockorder by neurotype:
modRB <- brm(
  RLLR ~ blockorder:neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modRB_output.rds"
  )
```

Inspect output:

```{r}
#Load the model output from a file:
modRB <- readRDS("modRB_output.rds")

#Return model output:
summary(modRB)
```

To plot parameter distributions, first extract posterior samples:

```{r}
#Extract posterior samples:
modRB_posterior <- posterior_samples(modRB)

#To visualise later, save to .csv:
write.table(modRB_posterior, file="modRB_posterior.csv",sep=",",row.names=F)
```

Plot parameter distributions with credible intervals:

```{r}
#Read in posterior samples .csv to data frame:
modRB_posterior <- read.csv("modRB_posterior.csv")


#Filter out 'lprior' and 'lp__':
modRB_posterior <- modRB_posterior[, c("b_blockorderSDV.neurotypeASC", "b_blockorderVDS.neurotypeASC", "b_blockorderSDV.neurotypeNT", "b_blockorderVDS.neurotypeNT", "sigma")]

#Convert the data frame to long format:
modRB_posterior_long <- pivot_longer(modRB_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modRB_posterior_long$Parameter <- recode(modRB_posterior_long$Parameter,
                                            "b_blockorderSDV.neurotypeASC" = "ASC:S/V", 
                                            "b_blockorderVDS.neurotypeASC" = "ASC:V/S",
                                            "b_blockorderSDV.neurotypeNT" = "NT:S/V", 
                                            "b_blockorderVDS.neurotypeNT" = "NT:V/S", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modRB_posterior_long$Group <- sub(":.*", "", modRB_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modRB_posterior_long$Parameter <- factor(modRB_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","ASC:S/V","ASC:V/S","NT:S/V","NT:V/S", "Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modRB_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dif_dist.png', plot = last_plot(), dpi = 300)
```

Visualise model-predicted RLLR values by blockorder:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype, blockorder) %>%
  distinct()

#Load the model output from a file:
modRB <- readRDS("modRB_output.rds")

#Extract conditional effects:
ce_RB <- conditional_effects(modRB)

#Convert the specific effect to a data frames:
ce_RB <- as.data.frame(ce_RB$`blockorder:neurotype`)

#Select relevant columns:
ce_RB <- ce_RB %>% 
  select(blockorder, neurotype, estimate__, lower__, upper__)

#Calculate means for each condition:
mean_estimates <- ce_RB %>%
  group_by(blockorder, neurotype) %>%
  dplyr::summarize(mean_est = mean(estimate__, na.rm = TRUE))

#Plot for MASQ for sim_modRM:
ggplot(ce_RB, aes(x = factor(blockorder), y = estimate__, color = neurotype)) +
  geom_jitter(data = data, aes(x = factor(blockorder), y = RLLR, color = neurotype)) +
  geom_line(aes(group = interaction(neurotype, blockorder)),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of Block Order On Relative Log Learning Rate By Neurotype",
       x = "Block Order",
       y = "Estimated Effect") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  # Add error bars
  geom_errorbar(data = ce_RB, aes(x = blockorder, ymin = lower__, ymax = upper__), 
                width = 0.2, size = 1) +
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_estimates, aes(x = blockorder, y = mean_est, group = neurotype),
             size = 1) +
  # Add mean points for each neurotype
  geom_point(data = mean_estimates, aes(x = blockorder, y = mean_est), 
             color = "#336666", size = 4, shape = 18)  # Shape 18 for diamond
```

The above plot shows model estimates for the effect of block order on RLLRs by neurotype - diamonds and lines show mean estimates for each block presentation by neurotype. Together with posterior distributions, it is evident that the order participants complete each condition (stable and volatile) does not impact learning rates. This is true for both NT and ASC groups.

### Effect of EDA-QA and MASQ on RLLR

To better understand the relationship between PDA and probabilistic learning, we will run a set of Bayesian regression models, 1) will model RLLR predicted by neurotype x MASQ scores, 2) will model RLLR predicted by neurotype x MASQ and neurotype x EDA-QA scores. We will perform model selection analysis (LOOCV) to discern which model better accounts for our data. This will help us infer if the EDA-QA is explaining any additional variance in RLLR over and above MASQ scores:

$$
RLLR_i \sim Normal( \mu_1, \sigma_1 )  \\
\mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j)\\
\alpha \sim {Normal}(0, 1) \quad \\
\beta_1, \beta_2, \beta_3  \sim {Normal}(0, 1) \quad  \\
\sigma \sim \text{Exponential}(1)
$$

And for comparison:

$$
RLLR_i \sim Normal( \mu_1, \sigma_1 )  \\
\mu_{ij} = \alpha + \beta_1 \text{MASQ}_i + \beta_2 \text{neurotype}_j + \beta_3 (\text{MASQ}_i \times \text{neurotype}_j) + \beta_4 \text{EDAQ}_i + \beta_5 (\text{EDAQ}_i \times \text{neurotype}_j) \\
\alpha \sim {Normal}(0, 1) \quad \\
\beta_1, \beta_2, \beta_3, \beta_4, \beta_5  \sim {Normal}(0, 1) \quad  \\
\sigma \sim \text{Exponential}(1)
$$

Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the models to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv") 

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Fit the model where relative log learning rate (RLLR) are predicted by MASQ scores by neurotype:
modRM <- brm(
  RLLR ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE),
  sig_figs = 10,
  file = "modRM_output.rds"
  )

#Fit the model where relative log learning rate (RLLR) are predicted by MASQ and EDA-QA scores by neurotype:
modRME <- brm(
  RLLR ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  sig_figs = 10,
  file = "modRME_output.rds"
  )
```

Add LOO criterion for model comparison:

```{r}
#Run model comparison (set reloo = TRUE if problematic observations are returned; IMPORTANT - make sure sig_figs = at least 10, otherwise add_criterion returns error: https://discourse.mc-stan.org/t/simplex-variable-error-during-bridgesampling/30811):
modRM <- add_criterion(modRM, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
modRME <- add_criterion(modRME, "loo", moment_match = TRUE, reloo = TRUE, recompile = TRUE)
```

Inspect output for both models:

```{r}
#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Return model output:
summary(modRM)
summary(modRME)

#We can also manually check the range of Pareto-k values; anything near or above 0.7 is likely problematic:
#range(loo(modRM)$diagnostics$pareto_k)
#range(loo(modRME)$diagnostics$pareto_k)

#Also, run model comparison:
loo_compare(modRM, modRME, criterion = "loo")
```

Interpreting the LOO output above - this will vary slightly each time the analysis is run:

-   **`elpd_diff`**: This is the difference in the expected log predictive density (elpd) between the two models. The higher the elpd, the better the model is at predicting new data. Here, `sim_modRM` has an elpd difference of 0.0, which means it's used as the reference model. `sim_modRME` has an elpd difference of -1.1, meaning it is 1.1 units worse in terms of predictive performance compared to `sim_modRM`.

-   **`se_diff`**: This is the standard error of the elpd difference. It quantifies the uncertainty in the elpd difference. A standard error of 1.2 for `sim_modRME` suggests that the estimate of the elpd difference has some variability - the small difference (-1.1) therefore suggests that `sim_modRM` is not much better at predicting new data than `sim_modRME`.

To plot parameter distributions, first extract posterior samples:

```{r}
#Extract posterior samples:
modRM_posterior <- posterior_samples(modRM)
modRME_posterior <- posterior_samples(modRME)

#To visualise later, save to .csv:
write.table(modRM_posterior, file="modRM_posterior.csv",sep=",",row.names=F)
write.table(modRME_posterior, file="modRME_posterior.csv",sep=",",row.names=F)
```

Plot parameter distributions with credible intervals:

```{r}
#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Return model output:
summary(modRM)
summary(modRME)

#Read in posterior samples .csv to data frame:
modRM_posterior <- read.csv("modRM_posterior.csv")
modRME_posterior <- read.csv("modRME_posterior.csv")


#Filter out 'lprior' and 'lp__':
modRM_posterior <- modRM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ.neurotypeNT", "sigma")]
modRME_posterior <- modRME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ.neurotypeNT", "b_neurotypeNT.EDAQ", "sigma")]

#Convert the data frame to long format:
modRM_posterior_long <- pivot_longer(modRM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modRME_posterior_long <- pivot_longer(modRME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modRM_posterior_long$Parameter <- recode(modRM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ.neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modRME_posterior_long$Parameter <- recode(modRME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ.neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modRM_posterior_long$Group <- sub(":.*", "", modRM_posterior_long$Parameter)
modRME_posterior_long$Group <- sub(":.*", "", modRME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modRM_posterior_long$Parameter <- factor(modRM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modRME_posterior_long$Parameter <- factor(modRME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modRM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dist.png', plot = last_plot(), dpi = 300)

ggplot(modRME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

Visualise model-predicted RLLR values for MASQ and EDA-QA scores and their interaction with neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Extract conditional effects:
ce_RM <- conditional_effects(modRM)
ce_RME <- conditional_effects(modRME)

#Convert the specific effect to a data frames:
ce_RM_MASQ <- as.data.frame(ce_RM$`MASQ:neurotype`)
ce_RME_MASQ <- as.data.frame(ce_RME$`MASQ:neurotype`)
ce_RME_EDAQ <- as.data.frame(ce_RME$`EDAQ:neurotype`)

#Select relevant columns:
ce_RM_MASQ <- ce_RM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_RME_MASQ <- ce_RME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_RME_EDAQ <- ce_RME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_RM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On RLLR By Neurotype",
       x = "MASQ Scores (std)",
       y = "RLLR (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'RLLR_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
RMEplot_masq <- ggplot(ce_RME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "RLLR (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
RMEplot_edaq <- ggplot(ce_RME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))  

#Combine the plots:
(RMEplot_masq + RMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On RLLR By Neurotype")
ggsave( 'RLLR_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plot above depicts the estimated effect of MASQ and EDA-QA scores on relative log learning rate (RLLR). We predicted that 1) RLLRs would be smaller in the ASC group compared to the NT group, 2) anxiety and PDA would be negatively associated with RLLRs., and 3) RLLRs for the ASC group would be similar irrespective of anxiety or PDA. Parameter estimates predict for the NT group, a very slight negative association between RLLRs and anxiety, and a positive association between RLLRs and PDA behaviours. For the ASC group, model estimates suggest a positive relationship between RLLRs and anxiety, and a negative relationship between RLLRs and PDA behaviours. Once again, a large amount of variability in parameter estimates renders these associations negligible.

# Prior predictive checks

"A prior predictive check displays simulated data that are generated from parameter values in the prior distribution. The simulated data from the mathematically specified prior should show trends that match the trends assumed by prior knowledge" (Kruschke, 2021).

We want to know that our choice of priors accurately represents our assumptions about our parameters. To do this, we can simulate priors from our model and relate them to observed values.

We have chosen weakly informative priors, $Normal(0,1)$ - this decision was informed by our simulated data (BARG PRL simulation)

## Behaviour predicted by neurotype, PDA and anxiety

### Lose-shift behaviour

Model lose-shift behaviours predicted by MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#A model with broad priors that acommodate uncertainty pertaining to the relationship between loseShift and MASQ scores by neurotype:
modLSM_prior <- brm(
  loseShift ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSM_prior_output.rds"
  )

#A model with broad priors that acommodate uncertainty pertaining to the relationship between loseShift and MASQ and EDA-QA scores by neurotype:
modLSME_prior <- brm(
  loseShift ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSME_prior_output.rds"
  )
```

We can inspect the output here:

```{r}
#Load the model output from a file:
modLSM_prior <- readRDS("modLSM_prior_output.rds")
modLSME_prior <- readRDS("modLSME_prior_output.rds")

#Return model output:
summary(modLSM_prior)
summary(modLSME_prior)
```

Draw samples from the prior predictive simulation above and plot against our data:

```{r}
#Run a prior predictive check that compares the distribution of prior predicted values to the distribution of observed values:
pp_check(modLSM_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for loseShift ~ MASQ*neurotype")
pp_check(modLSME_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for loseShift ~ MASQ*neurotype + EDAQ*neurotype")
```

The 'y' line represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions and centered around 0. Importantly, model predictions overlap with observed data, which suggests that model predictions are consistent with the observed data.

### Perseverative behaviour

Model perseveration predicted by MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#A model with broad priors that acommodate uncertainty pertaining to the relationship between perseveration and MASQ scores by neurotype:
modPM_prior <- brm(
  perseveration ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPM_prior_output.rds"
  )

#A model with broad priors that acommodate uncertainty pertaining to the relationship between perseveration and MASQ and EDA-QA scores by neurotype:
modPME_prior <- brm(
  perseveration ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPME_prior_output.rds"
  )
```

We can inspect the output here:

```{r}
#Load the model output from a file:
modPM_prior <- readRDS("modPM_prior_output.rds")
modPME_prior <- readRDS("modPME_prior_output.rds")

#Return model output:
summary(modPM_prior)
summary(modPME_prior)
```

Draw samples from the prior predictive simulation above and plot against our data:

```{r}
#Run a prior predictive check that compares the distribution of prior predicted values to the distribution of observed values:
pp_check(modPM_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for perseveration ~ MASQ*neurotype")
pp_check(modPME_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for perseveration ~ MASQ*neurotype + EDAQ*neurotype")
```

Again, the 'y' line represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions and centered around 0. Importantly, model predictions overlap with observed data, which suggests that model predictions are consistent with the observed data.

### Regressive behaviour

Model regression predicted by MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#A model with broad priors that acommodate uncertainty pertaining to the relationship between regression and MASQ scores by neurotype:
modReM_prior <- brm(
  regression ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReM_prior_output.rds"
  )

#A model with broad priors that acommodate uncertainty pertaining to the relationship between regression and MASQ and EDA-QA scores by neurotype:
modReME_prior <- brm(
  regression ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReME_prior_output.rds"
  )
```

We can inspect the output here:

```{r}
#Load the model output from a file:
modReM_prior <- readRDS("modReM_prior_output.rds")
modReME_prior <- readRDS("modReME_prior_output.rds")

#Return model output:
summary(modReM_prior)
summary(modReME_prior)
```

Draw samples from the prior predictive simulation above and plot against our data:

```{r}
#Run a prior predictive check that compares the distribution of prior predicted values to the distribution of observed values:
pp_check(modReM_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for regression ~ MASQ*neurotype")
pp_check(modReME_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for regression ~ MASQ*neurotype + EDAQ*neurotype")
```

Again, the 'y' line represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions and centered around 0. Importantly, model predictions overlap with observed data, which suggests that model predictions are consistent with the observed data.

## Learning rate predicted by neurotype, PDA and anxiety

### Effect of block order on RLLR

We use our data to model RLLR predicted by blockorder:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype, blockorder) %>%
  distinct()

#A model with broad priors that acommodate uncertainty pertaining to the relationship between RLLR and blockorder by neurotype:
modRB_prior <- brm(
  RLLR ~ blockorder:neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRB_prior_output.rds"
  )
```

We can inspect the output here:

```{r}
#Load the model output from a file:
modRB_prior <- readRDS("modRB_prior_output.rds")

#Return model output:
summary(modRB_prior)
```

Draw samples from the prior predictive simulation above and plot against our data:

```{r}
#Run a prior predictive check that compares the distribution of prior predicted values to the distribution of observed values:
pp_check(modRB_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for RLLR ~ blockorder:neurotype")
```

Again, the 'y' line represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions and centered around 0. Importantly, model predictions overlap with observed data, which suggests that model predictions are consistent with the observed data.

### Effect of EDA-QA and MASQ on RLLR

We use our simulated data to model RLLR predicted by MASQ and EDA-QA scores:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#A model with broad priors that acommodate uncertainty pertaining to the relationship between RLLR and MASQ scores by neurotype:
modRM_prior <- brm(
  RLLR ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRM_prior_output.rds"
  )

#A model with broad priors that acommodate uncertainty pertaining to the relationship between RLLR and MASQ and EDA-QA scores by neurotype:
modRME_prior <- brm(
  RLLR ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")),
  sample_prior = "only",
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRME_prior_output.rds"
  )
```

We can inspect the output here:

```{r}
#Load the model output from a file:
modRM_prior <- readRDS("modRM_prior_output.rds")
modRME_prior <- readRDS("modRME_prior_output.rds")

#Return model output:
summary(modRM_prior)
summary(modRME_prior)
```

Draw samples from the prior predictive simulation above and plot against our data:

```{r}
#Run a prior predictive check that compares the distribution of prior predicted values to the distribution of observed values:
pp_check(modRM_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for RLLR ~ MASQ*neurotype")
pp_check(modRME_prior, ndraws = 100) + ggtitle("Prior Predictive Check Plot for RLLR ~ MASQ*neurotype + EDAQ*neurotype")
```

Again, the 'y' line represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions and centered around 0. Importantly, model predictions overlap with observed data, which suggests that model predictions are consistent with the observed data.

# Computation and posterior distribution

## Details of the computation

All computations to derive posterior distributions for all three models were conducted using the brms() package (source details available in Package install_20.05.24.r- see packages and libraries); this includes ËR and effective sample size (ESS).

Again, chapter 12 of the following webpage provides some useful guidance on how brms() models are specified: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

We must check that the MCMC chains for every parameter have converged and are long enough to provide stable estimates.

Convergence here is indicated by ËR, which must be near 1.0 to indicate convergence.Â When ËR is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn't trust the samples.

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain not to the sample size of the data. When n_eff is much lower than the actual number of iterations (minus warm-up) of your chains, it means the chains are inefficient, but possibly still okay.

### Behaviour predicted by neurotype, PDA and anxiety

#### Lose-shift behaviour

And again for lose-shift behaviours predicted by MASQ and EDA-QA scores by neurotype:

```{r}
#Load the model output from file:
modLSM <- readRDS("modLSM_output.rds")
modLSME <- readRDS("modLSME_output.rds")

#Return model output:
summary(modLSM)
summary(modLSME)

#We can also get model parameters:
get_prior(modLSM)
get_prior(modLSME)
```

ËR and n_eff (ESS) look ok; ËR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
#Generate trace plots for each model:
mcmc_trace(modLSM)
mcmc_trace(modLSME)
```

Again, all chains look good.

#### Perseverative behaviour

And again for perseverative behaviours predicted by MASQ and EDA-QA scores by neurotype:

```{r}
#Load the model output from file:
modPM <- readRDS("modPM_output.rds")
modPME <- readRDS("modPME_output.rds")

#Return model output:
summary(modPM)
summary(modPME)

#We can also get model parameters:
get_prior(modPM)
get_prior(modPME)
```

ËR and n_eff (ESS) look ok; ËR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
#Generate trace plots for each model:
mcmc_trace(modPM)
mcmc_trace(modPME)
```

Again, all chains look good.

#### Regerssive behaviour

And again for regressive behaviours predicted by MASQ and EDA-QA scores by neurotype:

```{r}
#Load the model output from file:
modReM <- readRDS("modReM_output.rds")
modReME <- readRDS("modReME_output.rds")

#Return model output:
summary(modReM)
summary(modReME)

#We can also get model parameters:
get_prior(modReM)
get_prior(modReME)
```

ËR and n_eff (ESS) look ok; ËR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
#Generate trace plots for each model:
mcmc_trace(modReM)
mcmc_trace(modReME)
```

Again, all chains look good.

### Learning rate predicted by neurotype, PDA and anxiety

#### Effect of block order on RLLR

For RLLR predicted by blockorder by neurotype:

```{r}
#Load the model output from a file:
modRB <- readRDS("modRB_output.rds")

#Return model output:
summary(modRB)

#We can also get model parameters:
get_prior(modRB)
```

For both models, ËR and n_eff (ESS) look ok; ËR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
#Generate trace plots for each model:
mcmc_trace(modRB)
```

Again, all chains look good.

#### Effect of EDA-QA and MASQ on RLLR

And finally for RLLR predicted by MASQ and EDA-QA scores by neurotype:

```{r}
#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Return model output:
summary(modRM)
summary(modRME)

#We can also get model parameters:
get_prior(modRM)
get_prior(modRME)
```

For both models, ËR and n_eff (ESS) look ok; ËR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
#Generate trace plots for each model:
mcmc_trace(modRM)
mcmc_trace(modRME)
```

Again, for both models, all chains look good.

## Posterior predictive check

### Behaviour predicted by neurotype, PDA and anxiety

#### Lose-shift behaviour

Below we view the posterior predictions plotted against our data for lose-shift behaviours predicted by MASQ and EDA-QA scores.

```{r}
#Perform a graphical posterior predictive check for both models:
pp_check(modLSM, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for loseShift ~ MASQ*neurotype")
pp_check(modLSME, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for loseShift ~ MASQ*neurotype + EDAQ*neurotype")
```

As with out prior predictive checks, the 'y' line of the pp_check output represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the posterior distribution of the model parameters). Observed data appear to be a mixture of Gaussian distributions centered around 0. Model predictions for both models are normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

#### Perseverative behaviour

Below we view the posterior predictions plotted against our data for perseveration predicted by MASQ and EDA-QA scores.

```{r}
#Perform a graphical posterior predictive check for both models:
pp_check(modPM, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for perseveration ~ MASQ*neurotype")
pp_check(modPME, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for perseveration ~ MASQ*neurotype + EDAQ*neurotype")
```

Observed data appear to be a mixture of Gaussian distributions centered around 0. Model predictions for both models are normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

#### Regressive behaviour

Below we view the posterior predictions plotted against our data for regression predicted by MASQ and EDA-QA scores.

```{r}
#Perform a graphical posterior predictive check for both models:
pp_check(modReM, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for regression ~ MASQ*neurotype")
pp_check(modReME, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for regression ~ MASQ*neurotype + EDAQ*neurotype")
```

Observed data appear to be a mixture of Gaussian distributions centered around 0. Though model predictions for both models are normally distributed (centered around 0), they overlap with the observed data - model predictions are consistent with the observed data. Broader priors would like help improve model fit, but would run the risk of overfitting - because our variables are standardised, our priors of $Normal(0,1)$ are well justified.

### Learning rate predicted by neurotype, PDA and anxiety

#### Effect of block order on RLLR

Below we view the posterior predictions plotted against our data for RLLR predicted by blockorder.

```{r}
#Perform a graphical posterior predictive check for both models:
pp_check(modRB, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for RLLR ~ blockorder:neurotype")
```

Observed data appear to be a mixture of Gaussian distributions centered around 0. Model predictions for both models are normally distributed (centered around 0). Though predicted values do not fit the observed data distribution perfectly - the mean area of observed values is not well represented by model predicted values - they do overlap with a good portion of the observed data. Broader priors, or a student t distribution, would like help improve model fit, but would run the risk of overfitting. Because our variables are standardised, our priors of $Normal(0,1)$ are well justified - it is likely that poor model fit is the consequence of insufficient data (i.e., we need more data).

#### Effect of EDA-QA and MASQ on RLLR

Below we view the posterior predictions plotted against our data for RLLR predicted by MASQ and EDA-QA scores.

```{r}
#Perform a graphical posterior predictive check for both models:
pp_check(modRM, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for RLLR ~ MASQ*neurotype")
pp_check(modRME, ndraws = 100) + ggtitle("Posterior Predictive Check Plot for RLLR ~ MASQ*neurotype + EDAQ*neurotype")
```

For both models, both distributions appear a mixture of Gaussian distributions centered around 0. Model predictions for both models are normally distributed (centered around 0). As above, though predicted values do not fit the observed data distribution perfectly - the mean area of observed values is not well represented by model predicted values - they do overlap with a good portion of the observed data.

## Marginal posterior distribution

### Behaviour predicted by neurotype, PDA and anxiety

#### Lose-shift behaviour

Consider the marginal posterior distribution of each parameter (central tendency and credible intervals) for models loseShift\~ MASQneurotype and loseShift \~ MASQneurotype *+* EDAQneurotype:

```{r}
#Extract posterior samples:
modLSM_posterior <- posterior_samples(modLSM)
modLSME_posterior <- posterior_samples(modLSME)


#Filter out 'lprior' and 'lp__':
modLSM_posterior <- modLSM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modLSME_posterior <- modLSME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modLSM_posterior_long <- pivot_longer(modLSM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modLSME_posterior_long <- pivot_longer(modLSME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modLSM_posterior_long$Parameter <- recode(modLSM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modLSME_posterior_long$Parameter <- recode(modLSME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modLSM_posterior_long$Group <- sub(":.*", "", modLSM_posterior_long$Parameter)
modLSME_posterior_long$Group <- sub(":.*", "", modLSME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modLSM_posterior_long$Parameter <- factor(modLSM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modLSME_posterior_long$Parameter <- factor(modLSME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modLSM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'LS_dist.png', plot = last_plot(), dpi = 300)

ggplot(modLSME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'LS_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

For the first model, loseShift \~ MASQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `loseShift` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated lose-shift behavior for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `loseShift` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ and neurotypeNT**: These represent the effects of `MASQ` and `neurotypeNT` on `loseShift`, respectively. `MASQ` has a distribution centered just around 0, and a narrow HDI - this suggests the model is certain that `MASQ` has no effect on `loseShift`- again, because variables are standardised, this suggests that when `MASQ` is at it's average values, `loseShift` is also at it's average value. `neurotypeNT` has a distribution centered just below 0 and fairly broad HDI, which can be interpreted as the difference in lose-shift behavior between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A negative estimate means that NT individuals, on average, show less lose-shift behavior than ASC individuals when anxiety is at the mean, however, broad HDIs make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `loseShift`.

-   **MASQ:neurotypeNT**: This is the interaction term, indicating whether the effect of `MASQ` on `loseShift` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered around 0, with a wide HDI - this suggests that the effect of `MASQ` on `loseShift` does not vary by neurotype, again there is a relative degree of model uncertainty in this estimate.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `RLLR` that isnât accounted for by the predictors - here the distribution for sigma is centered around 1 with very narrow HDI - much of the variance in `loseShift` is being captured by the model.

For the second model, loseShift \~ MASQ\*neurotype+ EDAQ\*neurotype:

-   **Intercept**: Again, this is the estimated baseline value of `loseShift` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated lose-shift behavior for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `loseShift` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ, neurotypeNT, and EDAQ**: These represent the effects of `MASQ` and `neurotypeNT` on `loseShift`, respectively. `MASQ` has a distribution centered just below 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.09 standard deviation decrease in lose-shift behavior. However, HDIs overlapping 0 suggest `MASQ` has no effect on `loseShift`. `neurotypeNT` has a distribution centered just below 0 and fairly broad HDI, which can be interpreted as the difference in lose-shift behavior between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A negative estimate means that ASC individuals, on average, show less lose-shift behavior than NTs when anxiety is at the mean, however, broad HDIs make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `loseShift`. `EDAQ` has a distribution centered just above 0, and HDI that overlaps 0 - because variables are standardised, this means that a one-standard-deviation increase in PDA behaviours (EDAQ) is associated with a 0.19 standard deviation increase in lose-shift behavior. However, HDIs overlapping 0 make this an unreliable effect - `EDAQ` has no meaningful effect on `loseShift`.

-   **MASQ:neurotypeNT and neurotypeNT:EDAQ**: These are the interaction terms, indicating whether the effect of `MASQ` and `EDAQ` on `loseShift` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered just above 0, with a wide HDI - a positive estimate means that higher anxiety decreases lose-shift less strongly for NT individuals compared to ASC individuals. However, wide HDIs make this effect unreliable - the effect of `MASQ` on `loseShift` likely does not vary by neurotype. `EDAQ:neurotypeNT` has a distribution centered just below 0, with a wide HDI - a negative estimate means that more PDA behaviours leads to a smaller increase in lose-shift for NT individuals compared to ASC individuals. Once again, wide HDIs make this effect uncertain - the effect of `MASQ` on `loseShift` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `RLLR` that isnât accounted for by the predictors - here the distribution for sigma is centered around 1 with very narrow HDI - much of the variance in `loseShift` is being captured by the model.

Visualise model-predicted lose-shift values for MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modLSM <- readRDS("modLSM_output.rds")
modLSME <- readRDS("modLSME_output.rds")

#Extract conditional effects:
ce_LSM <- conditional_effects(modLSM)
ce_LSME <- conditional_effects(modLSME)

#Convert the specific effect to a data frames:
ce_LSM_MASQ <- as.data.frame(ce_LSM$`MASQ:neurotype`)
ce_LSME_MASQ <- as.data.frame(ce_LSME$`MASQ:neurotype`)
ce_LSME_EDAQ <- as.data.frame(ce_LSME$`EDAQ:neurotype`)

#Select relevant columns:
ce_LSM_MASQ <- ce_LSM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_LSME_MASQ <- ce_LSME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_LSME_EDAQ <- ce_LSME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_LSM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Lose-Shift Behaviour By Neurotype",
       x = "MASQ Scores (std)",
       y = "Lose-Shift Behaviour (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'LS_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
LSMEplot_masq <- ggplot(ce_LSME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Lose-Shift Behaviour (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
LSMEplot_edaq <- ggplot(ce_LSME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = loseShift, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))+
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))

#Combine the plots:
(LSMEplot_masq + LSMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Lose-Shift behaviour By Neurotype")
ggsave( 'LS_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above show regression lines for the model-predicted estimates (with HDI ribbons) over our data. Contrary to expectations, neither anxiety (MASQ) nor PDA (EDAQ) meaningfully influenced lose-shift behaviours - this was true for both ASC and NT groups.

1\) We predicted that we expect the number of lose-shift behaviours (i.e., a response switch made after receiving negative feedback, reported as a proportion of total negative feedback trials) would be greater for the ASC group compared to the NT group - this was based on Crawley et al. (2020) and D'Cruz et al. (2013). Both models plotted above demonstrate that predicted lose-shift values were higher for ASC compared to NT group. However, broad HDIs indicate a large degree of model uncertainty.

2\) We predicted that the number of lose-shift behaviours would be positively associated with anxiety- this was based on Piray and Daw (2021) theoretical model, as well as findings from Huang et al. (2017) and [Hein et al. (2023)](http://nature.com/articles/s42003-023-04628-1.pdf). And, that the number of lose-shift errors produced by the ASC group would be similar irrespective of anxiety - this reflected previous findings that suggest autism is associated with lose-shift behaviours (Crawley et al., 2020; D'Crus et al., 2013). This did appear to be the case in both models for the ASC group - lose-shift behaviours did not vary as a function of anxiety (MASQ) in the ASC group (coefficients). However, for the NT group, we predicted that lose-shift behaviours would be positively associated with anxiety - this is supported by literature that suggests anxiety is associated with more lose-shift behaviours (Huang et al., 2017; [Hein et al. (2023](http://nature.com/articles/s42003-023-04628-1.pdf)) - this prediction was not supported, as the NT group demonstrated a relationship between MASQ and lose-shift similar to the ASC group*.* Taken together, in both model with and without EDA-QA scores, MASQ scores did not meaningfully influence lose-shift behaviours - again broad HDIs indicate a large degree of model uncertainty.

3\) We predicted a similar pattern of lose-shift behaviours as a function of PDA behaviours; the number of lose-shift behaviours produced by the ASC group would be similar irrespective of PDA behaviours, while for the NT gorup, lose-shift behaviours would be positively associated with PDA behaviours - we assume that PDA is underpinned by anxiety and thus, findings that suggest anxiety is associated with more lose-shift behaviours are likely true for PDA also. The addition of EDA-QA scores did not meaningfully improve model fit (as assessed by LOO), suggesting that PDA did not account for substantial additional variance in lose-shift behaviors beyond what was explained by anxiety. However, the large `se_diff` values indicate considerable uncertainty in this conclusion. Additionally, EDA-QA was positively associated with lose-shift behaviours (as predicted), but this was true for both groups. However, broad HDIs again indicate a large degree of model uncertainty.

#### Perseverative behaviour

Consider the marginal posterior distribution of each parameter (central tendency and credible intervals) for models perseveration\~ MASQneurotype and perseveration\~ MASQneurotype *+* EDAQneurotype:

```{r}
#Extract posterior samples:
modPM_posterior <- posterior_samples(modPM)
modPME_posterior <- posterior_samples(modPME)


#Filter out 'lprior' and 'lp__':
modPM_posterior <- modPM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modPME_posterior <- modPME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modPM_posterior_long <- pivot_longer(modPM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modPME_posterior_long <- pivot_longer(modPME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modPM_posterior_long$Parameter <- recode(modPM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modPME_posterior_long$Parameter <- recode(modPME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modPM_posterior_long$Group <- sub(":.*", "", modPM_posterior_long$Parameter)
modPME_posterior_long$Group <- sub(":.*", "", modPME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modPM_posterior_long$Parameter <- factor(modPM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modPME_posterior_long$Parameter <- factor(modPME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modPM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'P_dist.png', plot = last_plot(), dpi = 300)

ggplot(modPME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'P_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

For the first model, perseveration \~ MASQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `perseveration` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated perseverative behaviour for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `perseveration` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ and neurotypeNT**: These represent the effects of `MASQ` and `neurotypeNT` on `perseveration`, respectively. `MASQ` has a distribution centered just below 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.07 standard deviation decrease in perseveration. However, HDIs overlapping 0 suggest `MASQ` has no effect on `perseveration`. `neurotypeNT` has a distribution centered just above 0 and fairly broad HDI, which can be interpreted as the difference in perseverative behaviour between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A positive estimate means that NT individuals, on average, show more perseverative behaviour than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `perseveration`.

-   **MASQ:neurotypeNT**: This is the interaction term, indicating whether the effect of `MASQ` on `perseveration` differs neurotypes. `MASQ:neurotypeNT` has a distribution centered just above 0, with a wide HDI - a positive estimate means that higher anxiety decreases perseveration less strongly for NT individuals compared to ASC individuals. However, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `perseveration` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `perseveration` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `perseveration` is being captured by the model.

For the second model, perseveration \~ MASQ\*neurotype+ EDAQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `perseveration` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated lose-shift behavior for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `perseveration` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ, neurotypeNT, and EDAQ**: These represent the effects of `MASQ`, `neurotypeNT`, and `EDAQ` on `perseveration`, respectively. `MASQ` has a distribution centered just below 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.1 standard deviation decrease in perseverative behaviour. However, HDIs overlapping 0 suggest `MASQ` has no effect on `perseveration`. `neurotypeNT` has a distribution centered just above 0 and fairly broad HDI, which can be interpreted as the difference in perseverative behaviour between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A positive estimate means that NT individuals, on average, show more perseverative behaviour than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `perseveration`. `EDAQ` has a distribution centered just above 0, and HDI that overlaps 0 - because variables are standardised, this means that a one-standard-deviation increase in PDA behaviours (EDAQ) is associated with a 0.04 standard deviation increase in perseverative behaviour. However, HDIs overlapping 0 make this an unreliable effect - `EDAQ` has no meaningful effect on `perseveration`.

-   **MASQ:neurotypeNT and neurotypeNT:EDAQ**: These are the interaction terms, indicating whether the effect of `MASQ` and `EDAQ` on `perseveration` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered just above 0, with a wide HDI - a positive estimate means that higher anxiety decreases perseveration less strongly for NT individuals compared to ASC individuals. However, wide HDIs overlapping 0 make this effect unreliable - the effect of `MASQ` on `perseveration` likely does not vary by neurotype. `EDAQ:neurotypeNT` has a distribution centered just below 0, with a wide HDI - a negative estimate means that more PDA behaviours leads to a smaller increase in perseveration for NT individuals compared to ASC individuals. Once again, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `perseveration` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `perseveration` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `perseveration` is being captured by the model.

Visualise model-predicted perseveration values for MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modPM <- readRDS("modPM_output.rds")
modPME <- readRDS("modPME_output.rds")

#Extract conditional effects:
ce_PM <- conditional_effects(modPM)
ce_PME <- conditional_effects(modPME)

#Convert the specific effect to a data frames:
ce_PM_MASQ <- as.data.frame(ce_PM$`MASQ:neurotype`)
ce_PME_MASQ <- as.data.frame(ce_PME$`MASQ:neurotype`)
ce_PME_EDAQ <- as.data.frame(ce_PME$`EDAQ:neurotype`)

#Select relevant columns:
ce_PM_MASQ <- ce_PM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_PME_MASQ <- ce_PME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_PME_EDAQ <- ce_PME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_PM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Perseverative  Errors By Neurotype",
       x = "MASQ Scores (std)",
       y = "Perseveration (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'Per_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
PMEplot_masq <- ggplot(ce_PME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Perseveration (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
PMEplot_edaq <- ggplot(ce_PME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = perseveration, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 

#Combine the plots:
(PMEplot_masq + PMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Perseverative Errors By Neurotype")
ggsave( 'Per_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above show regression lines for the model-predicted estimates (with HDI ribbons) over our data. Contrary to expectations, neither anxiety (MASQ) nor PDA (EDAQ) meaningfully influenced perseverative behaviours - this was true for both ASC and NT groups.

1\) We predicted that the number of perseverative behaviours (i.e., the number of errors made after each reversal before a correct response) would be greater for the ASC group compared to the NT gorup - this was based on Crawley et al. (2020) and Coldren & Halloran (2010). Contrary to expectations, both models predict slightly higher perseverative behaviours for NT compared to ASC individuals. However, broad HDIs indicate a large degree of model uncertainty.

2\) We predicted that anxiety would be negatively associated with perseverative errors for NT individuals - this was based on Fang et al. (2024) and Zhukovsky et al. (2017) who both found perseveration to be negatively correlated with anxiety, with the former noting that the severity of anxiety symptoms was negatively associated wiht a tendency for perseverative strategies. And, that perseveration in the ASC group would be similar irrespective of anxiety. Indeed, both models predict a flatter slope for ASC compared to NT, suggesting that, for the ASC group, anxiety influenced perseveration less than for the NT group. However, in the NT group, the first model (without EDAQ) suggests that anxiety does not influence perseveration, and in the second model (inc. EDAQ) that anxiety is associated with more perseverative behaviours. Importantly, broad HDIs indicate a large degree of model uncertainty.

3\) Because PDA is thought to be underpinned by anxiety (Johnson & Saunderson, 2023; Stuart et al., 2020), we predicted that PDA behaviours would also be negatively associated with perseverative behaviours for the NT group, but that the ASC group would demonstrate similar perseveration irrespective of PDA. The addition of EDA-QA scores did not meaningfully improve model fit (as assessed by LOO), suggesting that PDA did not account for substantial additional variance in perseveration beyond what was explained by anxiety. Again, the large `se_diff` values indicate considerable uncertainty in this conclusion. Again, for the ASC group, the model did predict a flatter slope for ASC compared to NT, suggesting that, for the ASC group, PDA influenced perseveration less than for the NT group. However, in the NT group, model estimates suggest that PDA is associated with fewer perseverative behaviours - this aligns with expectations. Importantly, broad HDIs again indicate a large degree of model uncertainty.

#### Regressive behaviour

Consider the marginal posterior distribution of each parameter (central tendency and credible intervals) for models regression \~ MASQneurotype and regression \~ MASQneurotype *+* EDAQneurotype:

```{r}
#Extract posterior samples:
modReM_posterior <- posterior_samples(modReM)
modReME_posterior <- posterior_samples(modReME)


#Filter out 'lprior' and 'lp__':
modReM_posterior <- modReM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ:neurotypeNT", "sigma")]
modReME_posterior <- modReME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ:neurotypeNT", "b_neurotypeNT:EDAQ", "sigma")]

#Convert the data frame to long format:
modReM_posterior_long <- pivot_longer(modReM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modReME_posterior_long <- pivot_longer(modReME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modReM_posterior_long$Parameter <- recode(modReM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modReME_posterior_long$Parameter <- recode(modReME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ:neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT:EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modReM_posterior_long$Group <- sub(":.*", "", modReM_posterior_long$Parameter)
modReME_posterior_long$Group <- sub(":.*", "", modReME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modReM_posterior_long$Parameter <- factor(modReM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modReME_posterior_long$Parameter <- factor(modReME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modReM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'R_dist.png', plot = last_plot(), dpi = 300)

ggplot(modReME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'R_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

For the first model, regression \~ MASQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `regression` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated regressive behaviour for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `regression` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ and neurotypeNT**: These represent the effects of `MASQ` and `neurotypeNT` on `regression`, respectively. `MASQ` has a distribution centered just above 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.17 standard deviation increase in regression. However, HDIs overlapping 0 suggest `MASQ` has no effect on `regression`. `neurotypeNT` has a distribution centered just below 0 and fairly broad HDI, which can be interpreted as the difference in regressive behaviour between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A negative estimate means that NT individuals, on average, show less regressive behaviour than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `regression`.

-   **MASQ:neurotypeNT**: This is the interaction term, indicating whether the effect of `MASQ` on `regression` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered just above 0, with a wide HDI - a positive estimate means that higher anxiety increases regression more strongly for NT individuals compared to ASC individuals. However, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `regression` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `regression` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `regression` is being captured by the model.

For the second model, regression \~ MASQ\*neurotype+ EDAQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `regression` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated regressive behaviour for the `NT` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `regression` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ, neurotypeNT, and EDAQ**: These represent the effects of `MASQ`, `neurotypeNT`, and `EDAQ` on `regression`, respectively. `MASQ` has a distribution centered just above 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.16 standard deviation increase in regressive behaviour. However, HDIs overlapping 0 suggest `MASQ` has no effect on `regression`. `neurotypeNT` has a distribution centered just below 0 and fairly broad HDI, which can be interpreted as the difference in regressive behaviour between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A negative estimate means that NT individuals, on average, show less regressive behaviour than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `regression`. `EDAQ` has a distribution centered at 0, with a HDI that sits both sides of 0 - because variables are standardised, this means that a one-standard-deviation increase in PDA behaviours (EDAQ) is associated with a 0.01 standard deviation increase in regressive behaviour. HDIs overlapping 0 suggest that `EDAQ` has no meaningful effect on `regression`.

-   **MASQ:neurotypeNT and neurotypeNT:EDAQ**: These are the interaction terms, indicating whether the effect of `MASQ` and `EDAQ` on `regression` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered just below 0, with a wide HDI - a negative estimate means that higher anxiety decreases regression less strongly for NT individuals compared to ASC individuals. However, wide HDIs overlapping 0 make this effect unreliable - the effect of `MASQ` on `regression` likely does not vary by neurotype. `EDAQ:neurotypeNT` has a distribution centered just above 0, with a wide HDI - a positive estimate means that more PDA behaviours increases regression more strongly for NT individuals compared to ASC individuals. Once again, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `regression` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `regression` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `regression` is being captured by the model.

Visualise model-predicted regression values for MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modReM <- readRDS("modReM_output.rds")
modReME <- readRDS("modReME_output.rds")

#Extract conditional effects:
ce_ReM <- conditional_effects(modReM)
ce_ReME <- conditional_effects(modReME)

#Convert the specific effect to a data frames:
ce_ReM_MASQ <- as.data.frame(ce_ReM$`MASQ:neurotype`)
ce_ReME_MASQ <- as.data.frame(ce_ReME$`MASQ:neurotype`)
ce_ReME_EDAQ <- as.data.frame(ce_ReME$`EDAQ:neurotype`)

#Select relevant columns:
ce_ReM_MASQ <- ce_ReM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_ReME_MASQ <- ce_ReME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_ReME_EDAQ <- ce_ReME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_ReM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On Regressive Errors By Neurotype",
       x = "MASQ Scores (std)",
       y = "Regression (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'Reg_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
ReMEplot_masq <- ggplot(ce_ReME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "Regression (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
ReMEplot_edaq <- ggplot(ce_ReME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = regression, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 

#Combine the plots:
(ReMEplot_masq + ReMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On Regressive Errors By Neurotype")
ggsave( 'Reg_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above show regression lines for the model-predicted estimates (with HDI ribbons) over our data. Contrary to expectations, neither anxiety (MASQ) nor PDA (EDAQ) meaningfully influenced regressive behaviours - this was true for both ASC and NT groups.

1\) Much like for perseverative behaviours, we predicted that the number of regressive behaviours (i.e., the number of errors made after the each reversal after at least one correct response) would be greater for the ASC group compared to the NT group - this was based on Crawley et al. (2020), Coldren & Halloran (2010) and Goris et al., (2021; who found no difference in learning rates between autistic traits and stable/volatile conditions, but did find a primacy bias - a tendency for higher autistic traits to be associated with a return to previously rewarded stimuli after reversal). Con sistent with this prediciton, both models predict slightly lower regressive behaviours for NT compared to ASC individuals. However, broad HDIs indicate a large degree of model uncertainty.

2\) We predicted that anxiety would be negatively associated with regressive errors for NT individuals - this was based on Fang et al. (2024) and Zhukovsky et al. (2017) - and, that regression in the ASC group would be similar irrespective of anxiety. In both models, anxiety was positively associated with regression for the ASC group. For the NT group, the first model (without EDAQ) estimates suggest a positive relationship between anxiety and regression, while the second model (inc. EDAQ) predicts a negative association. Importantly, broad HDIs indicate a large degree of model uncertainty.

3\) Because PDA is thought to be underpinned by anxiety (Johnson & Saunderson, 2023; Stuart et al., 2020), we assume PDA behaviours would also be negatively associated with regressive behaviours for the NT group and that regression in the ASC group would be similar irrespective of PDA. The addition of EDA-QA scores did not meaningfully improve model fit (as assessed by LOO), suggesting that PDA did not account for substantial additional variance in regression beyond what was explained by anxiety. Again, the large `se_diff` values indicate considerable uncertainty in this conclusion. Again, for the ASC group, the model did predict a flatter slope for ASC compared to NT, suggesting that, for the ASC group, PDA influenced regression less than for the NT group. However, contrary to expectations, for the NT group, model estimates suggest that PDA is associated with more regressive behaviours. Importantly, broad HDIs again indicate a large degree of model uncertainty.

### Learning rate predicted by neurotype, PDA and anxiety

#### Effect of block order on RLLR

Inspect the output for model-predicted RLLR values for blockorder and its interaction with neurotype, this time we will consider the marginal posterior distribution of each parameter (central tendency and credible intervals):

```{r}
#Read in posterior samples .csv to data frame:
modRB_posterior <- read.csv("modRB_posterior.csv")


#Filter out 'lprior' and 'lp__':
modRB_posterior <- modRB_posterior[, c("b_blockorderSDV.neurotypeASC", "b_blockorderVDS.neurotypeASC", "b_blockorderSDV.neurotypeNT", "b_blockorderVDS.neurotypeNT", "sigma")]

#Convert the data frame to long format:
modRB_posterior_long <- pivot_longer(modRB_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modRB_posterior_long$Parameter <- recode(modRB_posterior_long$Parameter,
                                            "b_blockorderSDV.neurotypeASC" = "ASC:S/V", 
                                            "b_blockorderVDS.neurotypeASC" = "ASC:V/S",
                                            "b_blockorderSDV.neurotypeNT" = "NT:S/V", 
                                            "b_blockorderVDS.neurotypeNT" = "NT:V/S", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modRB_posterior_long$Group <- sub(":.*", "", modRB_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modRB_posterior_long$Parameter <- factor(modRB_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","ASC:S/V","ASC:V/S","NT:S/V","NT:V/S", "Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modRB_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dif_dist.png', plot = last_plot(), dpi = 300)
```

For the first model, RLLR \~ blockOrder:neurotype:

-   **ASC:S/V**: `ASC:S/V` has a distribution centered around 0 with a wide HDI, suggesting that the most probable estimate is no effect, but with considerable uncertainty.

-   **ASC:V/S:** `ASC:V/S` has a distribution centerted around 0 with a wide HDI - there is no strong evidence that block order impacts `RLLR` in the `ASC/NT` groups, given that the posterior distributions for `S/V` and `V/S` conditions are similar.

-   **NT:S/V**: `NT:S/V` has a distribution centered around 0 with a wide HDI, suggesting that the most probable estimate is no effect, but with considerable uncertainty.

    **NT:V/S**: `NT:V/S` has a distribution centered around 0 with a wide HDI - again, there is no strong evidence that block order impacts `RLLR` in the `ASC/NT` groups, given that the posterior distributions for `S/V` and `V/S` conditions are similar.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `RLLR` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `RLLR` is being captured by the model.

Visualise model-predicted RLLR values for blockorder by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype, blockorder) %>%
  distinct()

#Load the model output from a file:
modRB <- readRDS("modRB_output.rds")

#Extract conditional effects:
ce_RB <- conditional_effects(modRB)

#Convert the specific effect to a data frames:
ce_RB <- as.data.frame(ce_RB$`blockorder:neurotype`)

#Select relevant columns:
ce_RB <- ce_RB %>% 
  select(blockorder, neurotype, estimate__, lower__, upper__)

#Calculate means for each condition:
mean_estimates <- ce_RB %>%
  group_by(blockorder, neurotype) %>%
  dplyr::summarize(mean_est = mean(estimate__, na.rm = TRUE))

#Plot for MASQ for sim_modRM:
ggplot(ce_RB, aes(x = factor(blockorder), y = estimate__, color = neurotype)) +
  geom_jitter(data = data, aes(x = factor(blockorder), y = RLLR, color = neurotype)) +
  geom_line(aes(group = interaction(neurotype, blockorder)),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of Block Order On Relative Log Learning Rate By Neurotype",
       x = "Block Order",
       y = "Estimated Effect") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  # Add error bars
  geom_errorbar(data = ce_RB, aes(x = blockorder, ymin = lower__, ymax = upper__), 
                width = 0.2, size = 1) +
  # Add lines connecting the mean points for each neurotype
  geom_line(data = mean_estimates, aes(x = blockorder, y = mean_est, group = neurotype),
             size = 1) +
  # Add mean points for each neurotype
  geom_point(data = mean_estimates, aes(x = blockorder, y = mean_est), 
             color = "#336666", size = 4, shape = 18)  # Shape 18 for diamond
```

Again, the above plot shows model estimates for the effect of block order on RLLRs by neurotype- diamonds and lines show mean estimates for each block presentation by neurotype. Together with posterior distributions, it is evident that the order participants complete each condition (stable and volatile) does not impact learning rates. This is true for both NT and ASC groups. There is also no difference between RLLRs predicted by each neurotype; mean RLLRs are slightly higher for the NT group compared to the ASC group - as we predicted - but not reliably (or meaningfully) so.

#### Effect of EDA-QA and MASQ on RLLR

Consider the marginal posterior distribution of each parameter (central tendency and credible intervals) for models RLLR \~ MASQneurotype and RLLR \~ MASQneurotype *+* EDAQneurotype:

```{r}
#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Return model output:
summary(modRM)
summary(modRME)

#Read in posterior samples .csv to data frame:
modRM_posterior <- read.csv("modRM_posterior.csv")
modRME_posterior <- read.csv("modRME_posterior.csv")


#Filter out 'lprior' and 'lp__':
modRM_posterior <- modRM_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT", "b_MASQ.neurotypeNT", "sigma")]
modRME_posterior <- modRME_posterior[, c("b_Intercept", "b_MASQ", "b_neurotypeNT","b_EDAQ", "b_MASQ.neurotypeNT", "b_neurotypeNT.EDAQ", "sigma")]

#Convert the data frame to long format:
modRM_posterior_long <- pivot_longer(modRM_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")
modRME_posterior_long <- pivot_longer(modRME_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'MASQ', 'EDAQ', and 'sigma':
modRM_posterior_long$Parameter <- recode(modRM_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ.neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")
modRME_posterior_long$Parameter <- recode(modRME_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_MASQ" = "MASQ", 
                                            "b_neurotypeNT" = "neurotypeNT",
                                            "b_EDAQ" = "EDAQ", 
                                            "b_MASQ.neurotypeNT" = "MASQ:neurotypeNT", 
                                            "b_neurotypeNT.EDAQ" = "neurotypeNT:EDAQ", 
                                            "sigma" = "Sigma")

#Extract group information from parameter names:
modRM_posterior_long$Group <- sub(":.*", "", modRM_posterior_long$Parameter)
modRME_posterior_long$Group <- sub(":.*", "", modRME_posterior_long$Parameter)

#Reorder the parameters and flip the order:
modRM_posterior_long$Parameter <- factor(modRM_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","MASQ:neurotypeNT","Sigma")))
modRME_posterior_long$Parameter <- factor(modRME_posterior_long$Parameter, 
                                            levels = rev(c("Intercept","MASQ","neurotypeNT","EDAQ","MASQ:neurotypeNT","neurotypeNT:EDAQ","Sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(modRM_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dist.png', plot = last_plot(), dpi = 300)

ggplot(modRME_posterior_long, aes(x = Estimate, y = Parameter, fill = neurotype)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3, fill = "#9ED4D1") +  # Adjusting the size of the points
  #scale_fill_manual(values = colors) +
  labs(x = "Parameter estimates", y = "Parameters", fill = "Neurotype") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave( 'RLLR_dist_EDAQ.png', plot = last_plot(), dpi = 300)
```

For the first model, RLLR \~ MASQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `RLLR` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated RLLR for the `ASC` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `RLLR` tendency (before adding effects of anxiety orneurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ and neurotypeNT**: These represent the effects of `MASQ` and `neurotypeNT` on `RLLR`, respectively. `MASQ` has a distribution centered just above 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.04 standard deviation increase in RLLR. However, HDIs overlapping 0 suggest `MASQ` has no effect on `RLLR`. `neurotypeNT` has a distribution centered just above 0 and fairly broad HDI, which can be interpreted as the difference in RLLR between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A positive estimate means that NT individuals, on average, show higher RLLRs than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `RLLR`.

-   **MASQ:neurotypeNT**: This is the interaction term, indicating whether the effect of `MASQ` on `RLLR` differs across groups. `MASQ:neurotypeNT` has a distribution centered just below 0, with a wide HDI - a negative estimate means that higher anxiety increases RLLRs less strongly for NT individuals compared to ASC individuals. However, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `RLLR` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `RLLR` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `RLLR` is being captured by the model.

For the second model, RLLR \~ MASQ\*neurotype+ EDAQ\*neurotype:

-   **Intercept**: This is the estimated baseline value of `RLLR` when all other predictors are zero. Because variables are standardised, the intercept gives the estimated RLLR for the `ASC` group when anxiety is at its mean.Since the estimate is around 0, this suggests that the baseline `RLLR` tendency (before adding effects of anxiety or neurotype) is close to the overall mean - the distribution is centered around 0 with narrow HDI.

-   **MASQ, neurotypeNT, and EDAQ**: These represent the effects of `MASQ`, `neurotypeNT`, and `EDAQ` on `RLLR`, respectively. `MASQ` has a distribution centered just above 0, and a narrow HDI - again, because variables are standardised, this means that a one-standard-deviation increase in anxiety (MASQ) is associated with a 0.14 standard deviation increase in RLLRs. However, HDIs overlapping 0 suggest `MASQ` has no effect on `RLLR`. `neurotypeNT` has a distribution centered just above 0 and fairly broad HDI, which can be interpreted as the difference in RLLRs between NT and ASC groups when `MASQ` = 0 (i.e., at mean anxiety). A positive estimate means that NT individuals, on average, show higher RLLRs than ASC individuals when anxiety is at the mean, however, broad HDIs that overlap 0 make this effect unreliable - `neurotypeNT` likely has no meaningful effect on `RLLR`. `EDAQ` has a distribution centered just below 0, with a HDI that sits both sides of 0 - because variables are standardised, this means that a one-standard-deviation increase in PDA behaviours (EDAQ) is associated with a 0.15 standard deviation decrease in RLLRs. HDIs overlapping 0 suggest that `EDAQ` has no meaningful effect on `RLLR`.

-   **MASQ:neurotypeNT and neurotypeNT:EDAQ**: These are the interaction terms, indicating whether the effect of `MASQ` and `EDAQ` on `RLLR` differs across neurotypes. `MASQ:neurotypeNT` has a distribution centered just below 0, with a wide HDI - a negative estimate means that higher anxiety increases RLLRs less strongly for NT individuals compared to ASC individuals. However, wide HDIs overlapping 0 make this effect unreliable - the effect of `MASQ` on `RLLR` likely does not vary by neurotype. `EDAQ:neurotypeNT` has a distribution centered at 0, with a wide HDI - a negative estimate means that more PDA behaviours decrease RLLRs more strongly for NT individuals compared to ASC individuals. Once again, wide HDIs that overlap 0 make this effect unreliable - the effect of `MASQ` on `RLLR` likely does not vary by neurotype.

-   **Sigma**: This represents the standard deviation of the residuals (the variability not explained by the model). A larger sigma indicates more variability in `RLLR` that isnât accounted for by the predictors - here the distribution for sigma sits above 0 with very narrow HDI - much of the variance in `RLLR` is being captured by the model.

Visualise model-predicted RLLR values for MASQ and EDA-QA scores by neurotype:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()

#Load the model output from a file:
modRM <- readRDS("modRM_output.rds")
modRME <- readRDS("modRME_output.rds")

#Extract conditional effects:
ce_RM <- conditional_effects(modRM)
ce_RME <- conditional_effects(modRME)

#Convert the specific effect to a data frames:
ce_RM_MASQ <- as.data.frame(ce_RM$`MASQ:neurotype`)
ce_RME_MASQ <- as.data.frame(ce_RME$`MASQ:neurotype`)
ce_RME_EDAQ <- as.data.frame(ce_RME$`EDAQ:neurotype`)

#Select relevant columns:
ce_RM_MASQ <- ce_RM_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_RME_MASQ <- ce_RME_MASQ %>% 
  select(MASQ, neurotype, estimate__, lower__, upper__)
ce_RME_EDAQ <- ce_RME_EDAQ %>% 
  select(EDAQ, neurotype, estimate__, lower__, upper__)

#Plot for MASQ for sim_modRM:
ggplot(ce_RM_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(title = "Effect Of MASQ Scores On RLLR By Neurotype",
       x = "MASQ Scores (std)",
       y = "RLLR (std)") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) 
ggsave( 'RLLR_plot_MASQ.png', plot = last_plot(), dpi = 300)

#Plot for MASQ for sim_modRME:
RMEplot_masq <- ggplot(ce_RME_MASQ, aes(x = MASQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = MASQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "MASQ Scores (std)",
       y = "RLLR (std)") +
  scale_color_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") +
  scale_fill_manual(values = c("#cc99ff", "#66cccc"),  guide = "none") 

#Plot for EDAQ for sim_modRME:
RMEplot_edaq <- ggplot(ce_RME_EDAQ, aes(x = EDAQ, y = estimate__, color = neurotype)) +
  geom_point(data = std_data, aes(x = EDAQ, y = RLLR, color = neurotype)) +
  geom_line(aes(y = estimate__),size = 1) +  # Line for estimated effects
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = neurotype), alpha = 0.3, color = NA) +  # Ribbons for uncertainty
  theme_minimal() +  # Use a minimal theme
  labs(x = "EDA-QA Scores (std)",
       y = "") +
  scale_color_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC")) +
  scale_fill_manual(values = c("ASC" = "#cc99ff", "NT" = "#66cccc"), name = "Neurotype", breaks = c("NT", "ASC"))  

#Combine the plots:
(RMEplot_masq + RMEplot_edaq) + plot_annotation(title = "Effect Of MASQ And EDAQ Scores On RLLR By Neurotype")
ggsave( 'RLLR_plot_EDAQ.png', plot = last_plot(), dpi = 300)
```

The plots above show regression lines for the model-predicted estimates (with HDI ribbons) over our data. Contrary to expectations, neither anxiety (MASQ) nor PDA (EDAQ) meaningfully influenced RLLRs - this was true for both ASC and NT groups.

1\) We predicted that RLLRs (i.e., log(LR in volatile block) - log(LR in stable block)) would be lower for the ASC group compared to the NT group - this was based on Crawley et al. (2020). Indeed, both models predict slightly lower RLLRs for ASC compared to NT individuals. However, broad HDIs indicate a large degree of model uncertainty.

2\) We predicted that anxiety would be negatively associated with RLLRs for NT individuals - this was based on Browning et al. (2015)- and, that RLLRs in the ASC group would be similar irrespective of anxiety. In both models, anxiety was positively associated with RLLRs for the ASC group. For the NT group, both models predicted a negative relationship between RLLRs and anxiety. Importantly, broad HDIs indicate a large degree of model uncertainty.

3\) Because PDA is thought to be underpinned by anxiety (Johnson & Saunderson, 2023; Stuart et al., 2020), we assume PDA behaviours would also be negatively associated with RLLRs for the NT group and that RLLRs in the ASC group would be similar irrespective of PDA. The addition of EDA-QA scores did not meaningfully improve model fit (as assessed by LOO), suggesting that PDA did not account for substantial additional variance in RLLR beyond what was explained by anxiety. Again, the large `se_diff` values indicate considerable uncertainty in this conclusion. For the ASC group, the model predicted similar slopes for NT and ASC groups, suggesting that there was little difference in how PDA influences RLLR by neurotype- for both groups, PDA was negatively associated with RLLR. Importantly, broad HDIs again indicate a large degree of model uncertainty.

# Sensitivity analysis

"A sensitivity analysis explores how changes in assumptions influence inference. If none of the alternative assumptions you consider have much impact on inference, that's worth reporting. Likewise, if the alternatives you consider do have an important impact on inference, that's also worth reporting. The same sort of advice follows for other modeling assumptions: likelihoods, linear models, priors, and even how the model is fit to data" - McElreath (2020).

## Behaviour predicted by neurotype, PDA and anxiety

### Lose-shift behaviour

First, read in the data:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()
```

Begin with three models that consider different priors for $Î±$ , $Î²$, and $Ï$ default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
modLSM_default <- brm(
  loseShift ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSM_default_output.rds"
  )
modLSME_default <- brm(
  loseShift ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSME_default_output.rds"
  )

#Fit the model with narrow priors:
modLSM_narrow <- brm(
  loseShift ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSM_narrow_output.rds"
  )
modLSME_narrow <- brm(
  loseShift ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSME_narrow_output.rds"
  )

#Fit the model with broad priors:
sim_modLSM_broad <- brm(
  loseShift ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSM_broad_output.rds"
  )
modLSME_broad <- brm(
  loseShift ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modLSME_broad_output.rds"
  )
```

Visualise the output of the above models to compare the impact of differing prior assumptions for each parameter:

```{r}
#Load models output from files:
modLSM_default <- readRDS("modLSM_default_output.rds")
modLSM_narrow <- readRDS("modLSM_narrow_output.rds")
modLSM_broad <- readRDS("modLSM_broad_output.rds")
modLSME_default <- readRDS("modLSME_default_output.rds")
modLSME_narrow <- readRDS("modLSME_narrow_output.rds")
modLSME_broad <- readRDS("modLSME_broad_output.rds")

#Return model output:
modelsLSM <- list(modLSM_default, modLSM_narrow, modLSM_broad)
modelsLSME <- list(modLSME_default, modLSME_narrow, modLSME_broad)

#Iterate summary over the models:
for(i in 1:length(modelsLSM)) {
  # Print model summary
  print(summary(modelsLSM[[i]]))
}
for(i in 1:length(modelsLSME)) {
  # Print model summary
  print(summary(modelsLSME[[i]]))
}

#Put models in a list:
modelsLSM <- list(Default = modLSM_default, Narrow = modLSM_narrow, Broad = modLSM_broad)
modelsLSME <- list(Default = modLSME_default, Narrow = modLSME_narrow, Broad = modLSME_broad)

#Create a data frame to hold coefficients:
dfLSM <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())
dfLSME <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(modelsLSM)) {
  coefsLSM <- fixef(modelsLSM[[model_name]])
  temp_dfLSM <- data.frame(Model = model_name, Parameters = rownames(coefsLSM), Estimate = coefsLSM[, 1], SE = coefsLSM[, 2])
  dfLSM <- rbind(dfLSM, temp_dfLSM)
}
for (model_name in names(modelsLSME)) {
  coefsLSME <- fixef(modelsLSME[[model_name]])
  temp_dfLSME <- data.frame(Model = model_name, Parameters = rownames(coefsLSME), Estimate = coefsLSME[, 1], SE = coefsLSME[, 2])
  dfLSME <- rbind(dfLSME, temp_dfLSME)
}

# Order coefficients by the order they appear in the first model, then reverse the order
dfLSM$Parameters <- factor(dfLSM$Parameters, levels = rev(rownames(fixef(modelsLSM[[1]]))))
dfLSME$Parameters <- factor(dfLSME$Parameters, levels = rev(rownames(fixef(modelsLSME[[1]]))))

#Plot using ggplot:
ggplot(dfLSM, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
ggplot(dfLSME, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All six models show comparable estimates for each parameter; we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

### Perseverative behaviour

Read in the data:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()
```

Begin with three models that consider different priors for $Î±$ , $Î²$, and $Ï$ default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
modPM_default <- brm(
  perseveration ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPM_default_output.rds"
  )
modPME_default <- brm(
  perseveration ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPME_default_output.rds"
  )

#Fit the model with narrow priors:
modPM_narrow <- brm(
  perseveration ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPM_narrow_output.rds"
  )
modPME_narrow <- brm(
  perseveration ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPME_narrow_output.rds"
  )

#Fit the model with broad priors:
modPM_broad <- brm(
  perseveration ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPM_broad_output.rds"
  )
modPME_broad <- brm(
  perseveration ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modPME_broad_output.rds"
  )
```

Visualise the output of the above models to compare the impact of differing prior assumptions for each parameter:

```{r}
#Load models output from files:
modPM_default <- readRDS("modPM_default_output.rds")
modPM_narrow <- readRDS("modPM_narrow_output.rds")
modPM_broad <- readRDS("modPM_broad_output.rds")
modPME_default <- readRDS("modPME_default_output.rds")
modPME_narrow <- readRDS("modPME_narrow_output.rds")
modPME_broad <- readRDS("modPME_broad_output.rds")

#Return model output:
modelsPM <- list(modPM_default, modPM_narrow, modPM_broad)
modelsPME <- list(modPME_default, modPME_narrow, modPME_broad)

#Iterate summary over the models:
for(i in 1:length(modelsPM)) {
  # Print model summary
  print(summary(modelsPM[[i]]))
}
for(i in 1:length(modelsPME)) {
  # Print model summary
  print(summary(modelsPME[[i]]))
}

#Put models in a list:
modelsPM <- list(Default = modPM_default, Narrow = modPM_narrow, Broad = modPM_broad)
modelsPME <- list(Default = modPME_default, Narrow = modPME_narrow, Broad = modPME_broad)

#Create a data frame to hold coefficients:
dfPM <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())
dfPME <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(modelsPM)) {
  coefsPM <- fixef(modelsPM[[model_name]])
  temp_dfPM <- data.frame(Model = model_name, Parameters = rownames(coefsPM), Estimate = coefsPM[, 1], SE = coefsPM[, 2])
  dfPM <- rbind(dfPM, temp_dfPM)
}
for (model_name in names(modelsPME)) {
  coefsPME <- fixef(modelsPME[[model_name]])
  temp_dfPME <- data.frame(Model = model_name, Parameters = rownames(coefsPME), Estimate = coefsPME[, 1], SE = coefsPME[, 2])
  dfPME <- rbind(dfPME, temp_dfPME)
}

# Order coefficients by the order they appear in the first model, then reverse the order
dfPM$Parameters <- factor(dfPM$Parameters, levels = rev(rownames(fixef(modelsPM[[1]]))))
dfPME$Parameters <- factor(dfPME$Parameters, levels = rev(rownames(fixef(modelsPME[[1]]))))

#Plot using ggplot:
ggplot(dfPM, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
ggplot(dfPME, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All six models show comparable estimates for each parameter; we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

Adopting narrow priors shrinks model estimates toward 0 in some cases (e.g., for interaction parameters). This can happen when modelling insufficient data - without sufficient data to adequately influence the posterior, narrow priors drag model estimates toward 0. Our models are likely under powered - we need more data to better understand the relationship between PDA, anxiety, and perseveration.

### Regressive behaviour

Read in the data:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()
```

Begin with three models that consider different priors for $Î±$ , $Î²$, and $Ï$ default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
modReM_default <- brm(
  regression ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReM_default_output.rds"
  )
sim_modReME_default <- brm(
  regression ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReME_default_output.rds"
  )

#Fit the model with narrow priors:
modReM_narrow <- brm(
  regression ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReM_narrow_output.rds"
  ) 

modReME_narrow <- brm(
  regression ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReME_narrow_output.rds"
  )

#Fit the model with broad priors:
modReM_broad <- brm(
  regression ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReM_broad_output.rds"
  )
modReME_broad <- brm(
  regression ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modReME_broad_output.rds"
  )
```

Visualise the output of the above models to compare the impact of differing prior assumptions for each parameter:

```{r}
#Load models output from files:
modReM_default <- readRDS("modReM_default_output.rds")
modReM_narrow <- readRDS("modReM_narrow_output.rds")
modReM_broad <- readRDS("modReM_broad_output.rds")
modReME_default <- readRDS("modReME_default_output.rds")
modReME_narrow <- readRDS("modReME_narrow_output.rds")
modReME_broad <- readRDS("modReME_broad_output.rds")

#Return model output:
modelsReM <- list(modReM_default, modReM_narrow, modReM_broad)
modelsReME <- list(modReME_default, modReME_narrow, modReME_broad)

#Iterate summary over the models:
for(i in 1:length(modelsReM)) {
  # Print model summary
  print(summary(modelsReM[[i]]))
}
for(i in 1:length(modelsReME)) {
  # Print model summary
  print(summary(modelsReME[[i]]))
}

#Put models in a list:
modelsReM <- list(Default = modReM_default, Narrow = modReM_narrow, Broad = modReM_broad)
modelsReME <- list(Default = modReME_default, Narrow = modReME_narrow, Broad = modReME_broad)

#Create a data frame to hold coefficients:
dfReM <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())
dfReME <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(modelsReM)) {
  coefsReM <- fixef(modelsReM[[model_name]])
  temp_dfReM <- data.frame(Model = model_name, Parameters = rownames(coefsReM), Estimate = coefsReM[, 1], SE = coefsReM[, 2])
  dfReM <- rbind(dfReM, temp_dfReM)
}
for (model_name in names(modelsReME)) {
  coefsReME <- fixef(modelsReME[[model_name]])
  temp_dfReME <- data.frame(Model = model_name, Parameters = rownames(coefsReME), Estimate = coefsReME[, 1], SE = coefsReME[, 2])
  dfReME <- rbind(dfReME, temp_dfReME)
}

# Order coefficients by the order they appear in the first model, then reverse the order
dfReM$Parameters <- factor(dfReM$Parameters, levels = rev(rownames(fixef(modelsReM[[1]]))))
dfReME$Parameters <- factor(dfReME$Parameters, levels = rev(rownames(fixef(modelsReME[[1]]))))

#Plot using ggplot:
ggplot(dfReM, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
ggplot(dfReME, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All six models show comparable estimates for each parameter; we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

Adopting narrow priors shrinks model estimates toward 0 in some cases (e.g., for interaction parameters). This can happen when modelling insufficient data - without sufficient data to adequately influence the posterior, narrow priors drag model estimates toward 0. Our models are likely under powered - we need more data to better understand the relationship between PDA, anxiety, and regression.

## Learning rate predicted by neurotype, PDA and anxiety

### Effect of block order on RLLR

Read in the data:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype, blockorder) %>%
  distinct()
```

We will begin with three models that consider different priors for $Î±$ , $Î²$, and $Ï$ default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
modRB_default <- brm(
  RLLR ~ blockorder:neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRB_default_output.rds"
  )

#Fit the model with narrow priors:
modRB_narrow <- brm(
  RLLR ~ blockorder:neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRB_narrow_output.rds"
  )

#Fit the model with broad priors:
modRB_broad <- brm(
  RLLR ~ blockorder:neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRB_broad_output.rds"
  )
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for each parameter:

```{r}
#Load models output from files:
modRB_default <- readRDS("modRB_default_output.rds")
modRB_narrow <- readRDS("modRB_narrow_output.rds")
modRB_broad <- readRDS("modRB_broad_output.rds")

#Return model output:
modelsRB <- list(modRB_default, modRB_narrow, modRB_broad)

#Iterate summary over the models:
for(i in 1:length(modelsRB)) {
  # Print model summary
  print(summary(modelsRB[[i]]))
}

#Put models in a list:
modelsRB <- list(Default = modRB_default, Narrow = modRB_narrow, Broad = modRB_broad)

#Create a data frame to hold coefficients:
dfRB <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(modelsRB)) {
  coefsRB <- fixef(modelsRB[[model_name]])
  temp_dfRB <- data.frame(Model = model_name, Parameters = rownames(coefsRB), Estimate = coefsRB[, 1], SE = coefsRB[, 2])
  dfRB <- rbind(dfRB, temp_dfRB)
}

# Order coefficients by the order they appear in the first model, then reverse the order
dfRB$Parameters <- factor(dfRB$Parameters, levels = rev(rownames(fixef(modelsRB[[1]]))))

#Plot using ggplot:
ggplot(dfRB, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All three models show comparable estimates for each parameter; we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

### Effect of EDA-QA and MASQ on RLLR

Read in the data:

```{r}
#Read in saved data file:
data <- read.csv("Data.csv")

#Prepare and standardise variables:
std_data <- data %>%
  mutate(
    EDAQ = standardize(EDAQ),
    MASQ = standardize(MASQ),
    RLLR = standardize(RLLR),
    perseveration = standardize(perseveration),
    regression = standardize(regression),
    loseShift = standardize(loseShift)
  ) %>%
  select(subject, EDAQ, MASQ, RLLR, loseShift, perseveration, regression, neurotype) %>%
  distinct()
```

Begin with three models that consider different priors for $Î±$ , $Î²$, and $Ï$ default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
modRM_default <- brm(
  RLLR ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRM_default_output.rds"
  )
modRME_default <- brm(
  RLLR ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRME_default_output.rds"
  )

#Fit the model with narrow priors:
modRM_narrow <- brm(
  RLLR ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRM_narrow_output.rds"
  )
modRME_narrow <- brm(
  RLLR ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.2), class = "Intercept"),
    prior(normal(0, 0.2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRME_narrow_output.rds"
  )

#Fit the model with broad priors:
modRM_broad <- brm(
  RLLR ~ MASQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRM_broad_output.rds"
  )
modRME_broad <- brm(
  RLLR ~ MASQ*neurotype + EDAQ*neurotype,
  data = std_data,
  backend = "cmdstan",
  family = gaussian(),
  prior = c(
    prior(normal(0, 2), class = "Intercept"),
    prior(normal(0, 2), class = "b"),
    prior(exponential(1), class = "sigma")
  ),
  control = list(adapt_delta = 0.99),
  cores = 4,
  file = "modRME_broad_output.rds"
  )
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for each parameter:

```{r}
#Load models output from files:
modRM_default <- readRDS("modRM_default_output.rds")
modRM_narrow <- readRDS("modRM_narrow_output.rds")
modRM_broad <- readRDS("modRM_broad_output.rds")
modRME_default <- readRDS("modRME_default_output.rds")
modRME_narrow <- readRDS("modRME_narrow_output.rds")
modRME_broad <- readRDS("modRME_broad_output.rds")

#Return model output:
modelsRM <- list(modRM_default, modRM_narrow, modRM_broad)
modelsRME <- list(modRME_default, modRME_narrow, modRME_broad)

#Iterate summary over the models:
for(i in 1:length(modelsRM)) {
  # Print model summary
  print(summary(modelsRM[[i]]))
}
for(i in 1:length(modelsRME)) {
  # Print model summary
  print(summary(modelsRME[[i]]))
}

#Put models in a list:
modelsRM <- list(Default = modRM_default, Narrow = modRM_narrow, Broad = modRM_broad)
modelsRME <- list(Default = modRME_default, Narrow = modRME_narrow, Broad = modRME_broad)

#Create a data frame to hold coefficients:
dfRM <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())
dfRME <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(modelsRM)) {
  coefsRM <- fixef(modelsRM[[model_name]])
  temp_dfRM <- data.frame(Model = model_name, Parameters = rownames(coefsRM), Estimate = coefsRM[, 1], SE = coefsRM[, 2])
  dfRM <- rbind(dfRM, temp_dfRM)
}
for (model_name in names(modelsRME)) {
  coefsRME <- fixef(modelsRME[[model_name]])
  temp_dfRME <- data.frame(Model = model_name, Parameters = rownames(coefsRME), Estimate = coefsRME[, 1], SE = coefsRME[, 2])
  dfRME <- rbind(dfRME, temp_dfRME)
}

# Order coefficients by the order they appear in the first model, then reverse the order
dfRM$Parameters <- factor(dfRM$Parameters, levels = rev(rownames(fixef(modelsRM[[1]]))))
dfRME$Parameters <- factor(dfRME$Parameters, levels = rev(rownames(fixef(modelsRME[[1]]))))

#Plot using ggplot:
ggplot(dfRM, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
ggplot(dfRME, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All six models show comparable estimates for each parameter; we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

Adopting narrow priors shrinks model estimates toward 0 in some cases (e.g., for interaction parameters). This can happen when modelling insufficient data - without sufficient data to adequately influence the posterior, narrow priors drag model estimates toward 0. Our models are likely under powered - we need more data to better understand the relationship between PDA, anxiety, and learning rates.
