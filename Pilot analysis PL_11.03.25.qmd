---
title: "Pilot analysis_probabilistic learning_11.03.25"
format: html
editor: visual
---

# Package installation

```{r}
#Install packages and libraries:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot")

#Install packages if not already installed:
packages_to_install <- packages[!packages %in% installed.packages()]
if(length(packages_to_install)) install.packages(packages_to_install, dependencies = TRUE)

#If not already installed, install rethinking() separately:
#install.packages("rethinking", 
#                 repos=c(cran="https://cloud.r-project.org",
#                         rethinking="http://xcelab.net/R"))

#Rstan might need a bit of extra attention. If it doesn't install with the above code, remove any existing RStan via:
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")

#Set up compiler flags:
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXX17FLAGS=-O3 -march=native -mtune=native -fPIC",
    "CXX17=g++", # or clang++ but you may need a version postfix
    file = M, sep = "\n", append = TRUE)

#This code is for the development version of rstan- I've been told that this might function better than the up-to-date version:
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

#To verify your installation, you can run the RStan example/test model:
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

```{r}
#Once installed, load libraries using groundhog():
library(groundhog)

#Specify packages:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot")

#Load packages:
groundhog.library(packages, "2024-06-28", tolerate.R.version='4.4.0')
```

```{r}
#If that doesn't work, load packages manually:
library(rstan)
library(cmdstanr)
library(devtools)
library(rethinking) #Add download above
library(V8)
library(brms)
library(tidyverse) 
library(plotrix)
library(gridExtra)
library(tidybayes)
library(modelsummary)
library(bayesplot)

#Add downloads above
library(lme4)
library(lmerTest)
library(emmeans)
```

# Pilot analysis

NOTE: pilot data were processed using the Data preparation_PL_11.03.25.qmd file.

Load in pilot data:

```{r}
#Load data:
data <- read.csv("Pilot data_07.03.25.csv")

#Inspect:
data
```

Check summary of d':

```{r}
#Mean:
mean_dprime <- mean(data$dprime)

#Standard Deviation:
sd_dprime <- sd(data$dprime)

#Standard Error of the Mean:
sem_dprime <- sd_dprime / sqrt(length(data$dprime))

#95% Confidence Interval (using t-distribution for small sample)
t_value <- qt(0.975, df = length(data$dprime) - 1)
ci_lower <- mean_dprime - t_value * sem_dprime
ci_upper <- mean_dprime + t_value * sem_dprime

#Print results
mean_dprime
sd_dprime
ci_lower
ci_upper
```

## Visualisation

Plot Insensitivity Indices (II) for each participant:

```{r}
#Read in questionnaire data:
ggplot(data) +
  labs(title= "II for individual participants",
       x="Probability (%)", y="RT (ms)") +
  theme(legend.position="none") +
  geom_abline(aes(slope=II,
                  intercept=intercept,
                  color=factor(subject))) +
  xlim(0, 100) +
  ylim(0, 700) +
  theme_minimal()
```

## Analysis

To confirm that our version of the paradigm produces similar effects to that of Reisli et al., (2023), run a LMEM to estimate the effects of different levels of probability on RTs, without an intercept, and included random intercepts to account for variation among individuals:

```{r}
#create data set with blocks and rts as column headers:
block_l <- pivot_longer(data, 
                        cols = starts_with("block"),  
                        names_to = "probability", 
                        values_to = "rts")

#and convert conditions to volatility percentages:
block_l <- block_l %>%
  mutate(probability = recode(probability, 
                              "block1_median" = 100, 
                              "block2_median" = 84, 
                              "block3_median" = 67, 
                              "block4_median" = 33)) %>%
  mutate(probability = as.numeric(probability))  # Ensuring it's numeric

#For the sake of interpretability, first reorder the levels of probability in descending order.
#Define the desired order of levels:
#desired_order <- c("33", "67", "84", "100")    #If you prefer this order - go for it!
desired_order <- c("100", "84", "67", "33")

#Convert probability to an ordered factor with the desired order:
block_l$probability <- factor(block_l$probability, levels = desired_order, ordered = TRUE)

#First run the model:
mod <- lmerTest::lmer(rts ~ 0 + probability + (1 | subject), data = block_l)

#View the results
#ranef(mod)
summary(mod)
confint(mod)
anova(mod)

#Then planned contrasts:
contrast_result <- emmeans(mod, ~ probability, 
                           contr = list(
                             "100 vs 84" = c(0, 0, 1, -1),
                             "84 vs 67" = c(0, 1, -1, 0),
                             "67 vs 33" = c(1, -1, 0, 0)
                           ))

#View the results:
summary(contrast_result)
confint(contrast_result)

#Create the boxplot with ordered levels:
ggplot(block_l, aes(x = probability, y = rts)) +   
  geom_boxplot(fill = "#9ED4D1") +
  #geom_smooth(method = 'lm', aes(group = 1)) +
  ylim(250,550) +
  labs(title = "Pilot data median RTs by probability",
       x = "Probability of cue-target pair", y = "RT (ms)") +
  scale_x_discrete(labels = c("100%", "84%", "67%", "33%")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
ggsave( 'Pilotdata.png', plot = last_plot(), dpi = 300)
```

Scaled residuals demonstrated a range from -2.40 to 2.09, with a median close to zero, suggesting a reasonable model fit.

Results suggest that probability was negatively associated with RTs: for the 33% probability condition, the model estimated mean RTs of 467 ms (*t* = 45.99, *p* \< 0.001, 95% CI\[447, 487\]), for the 67% probability condition, 435 ms (*t* = 42.84, *p* \< 0.001, 95% CI\[415,455\]), for the 84% probability condition, 418 ms (*t* = 41.20, *p* \< 0.001, 95% CI\[398, 438\]), and for the 100% probability condition, 381 ms (*t* = 37.52, *p* \< 0.001, 95% CI\[361, 401\]). Random intercepts for ID suggest on average the effect is strong, with some variation between subjects (*SD* = 30.34).

Planned contrasts were conducted to investigate specific differences in RTs across varying levels of probability. The estimated difference in RTs between the 100% and 84% probability conditions was 37 ms (*t* = 3.915, *p* \<0.001, 95% CI\[18.13, 56.6\]), indicating a significant increase in RTs as probability decreased from 100% to 84%. The estimated difference in RTs between the 84% and 67% probability conditions was 17 ms (*t* = 1.740, *p* = 0.089, 95% CI\[-2.62, 35.8\]), suggesting no clear RT change between the 84% and 67% probability levels. Finally, the estimated difference in RTs between the 67% and 33% probability conditions was 32 ms (*t* = 3.361, *p* = 0.002, 95% CI\[12.85, 51.3\]), reflecting a significant increase in RTs when probability decreased from 67% to 33%.
