---
title: "BARG simulation_probabilistic learning_26.06.24"
format: html
editor: visual
---

# Package installation

```{r}
#Install packages and libraries:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot")

#Install packages if not already installed:
packages_to_install <- packages[!packages %in% installed.packages()]
if(length(packages_to_install)) install.packages(packages_to_install, dependencies = TRUE)

#If not already installed, install rethinking() separately:
#install.packages("rethinking", 
#                 repos=c(cran="https://cloud.r-project.org",
#                         rethinking="http://xcelab.net/R"))

#Rstan might need a bit of extra attention. If it doesn't install with the above code, remove any existing RStan via:
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")

#Set up compiler flags:
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXX17FLAGS=-O3 -march=native -mtune=native -fPIC",
    "CXX17=g++", # or clang++ but you may need a version postfix
    file = M, sep = "\n", append = TRUE)

#This code is for the development version of rstan- I've been told that this might function better than the up-to-date version:
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

#To verify your installation, you can run the RStan example/test model:
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

```{r}
#Once installed, load libraries using groundhog():
library(groundhog)

#Specify packages:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot")

#Load packages:
groundhog.library(packages, "2024-06-28", tolerate.R.version='4.4.0')
```

```{r}
#If that doesn't work, load packages manually:
library(rstan)
library(cmdstanr)
library(devtools)
library(rethinking) #Add download above
library(V8)
library(brms)
library(tidyverse) 
library(plotrix)
library(gridExtra)
library(tidybayes)
library(modelsummary)
library(bayesplot)
```

# Preamble

## Why Bayesian?

Global autism prevalence is estimated at approx. 1 in 100 (Zeidan et al., 2022). Little is known about Pathological Demand Avoidance (PDA) and its prevalence. However, it seems reasonable to assume that if PDA represents a proportion of the autistic population, prevalence of PDA is likely \<1 in 100. Additionally, the percentage of PDA and/or autistic individuals willing and able to participate in research is likely smaller still. Compared to frequentist methods that require large samples to produce precise, well-powered results, Bayesian approaches rely on estimates that have a clear and valid interpretation, no matter the sample size- though it is important to note that Bayesian estimates are dependent upon the initial plausibilities assigned to them (i.e., priors), which is especially true for estimates derived from small sample sizes. In this regard, Bayesian modelling offers us a powerful tool with which to better understand individuals from underrepresented groups, providing the priors we fit to our model are well justified- this will be explained and exemplified in our prior predictive checks and sensitivity analysis.

Furthermore, the autistic population is renowned for its heterogeneity (for an overview of heterogeneity in autism, see Masi et al., 2017), which often produces noisy data that is difficult to interpret. Though data generated by PDA individuals (not third-party report) is limited, its potential relationship to autism means that PDA data may be noisy, too. Again, Bayesian analyses allow us to incorporate prior knowledge about the parameters in the form of priors. This can be particularly useful when dealing with noisy data, as prior information can help regularize the estimates and improve model stability. In addition, Bayesian methods provide a way to quantify uncertainty in parameter estimates through posterior distributions. This is especially important when dealing with noisy data, as it allows us to assess the reliability of estimates and make more informed decisions.

## Goals of analysis

We are interested in how PDA, autism, and neurotypicallity learn about probabilistic contingencies. We utilised a probabilistic learning task in which a cue (a shape) was indicative of a target (same shape rotated 90°) 100%, 84%, 67%, and 33% of the time. Participants completed four randomly presented blocks, one for each probability condition; 100%, 84%, 67% and 33%. This study had two primary aims:

1\) Gain insight into whether PDA and autistic individuals differ from neurotypical individuals in how they learn about probabilistic associations. Specifically, we wanted to know if performance of PDA and autistic individuals differ less between similar probability contexts (e.g., 84% and 67% conditions) compared to neurotypical individuals. We also wanted to know if PDA and autistic individuals perform similarly across various probability conditions (i.e., if PDA and autistic individuals produce similar RTs for each condition) – this would help to clarify if/how PDA and autism are related.

2\) Examine the relationship between sensitivity to changing probabilistic associations and PDA behaviours. We wanted to know if an insensitivity to changing probabilistic associations is related to PDA behaviours beyond the influence of an autism diagnosis – potentially offering a broader understanding of how uncertainty relates to PDA.

## Causal model

Below is a directed acyclical graphic (DAG) representation of our causal model. "SensoryProcessing" refers to sensory processing differences thought to underpin perception, cognition, and behaviour in autism (Palmer et al., 2017; Van de Cruys et al., 2014); probabilistic learning is thought to represent one facet of these sensory processing differences, and the facet we attempt to probe in this study. "Autism" here refers to a binary category- possessing a formal, clinical diagnosis of autism, or not. Finally, "PDA" refers to a spectrum of behavioural characteristics thought to be associated with PDA.

```{r}
dag_m <- dagitty( "dag {
    Autism -> EDAQ
    SensoryProcessing -> EDAQ
    SensoryProcessing -> Autism
}")
coordinates( dag_m ) <- list( x=c(Autism=0, EDAQ=1, SensoryProcessing=-1) , y=c(Autism=-0.5, EDAQ=1, SensoryProcessing=1) )
drawdag(dag_m)
```

Previous research suggests that aberrant probability learning is associated with autism (Lawson et al., 2017; Reisli et al., 2023). This compliments predictive processing theories that suggest aberrant handling of sensory information relative to prior knowledge might underpin autistic development (for review, see Cannon et al., 2021 and Chrysaitis & Series, 2023). Thus, a latent measure of probabilistic learning should causally influence an autism diagnosis. The relationship between PDA and autism is contentious- some view PDA as an autism subgroup (Christie, 2007), others posit that PDA is not different from autism (Milton, 2013; Moore, 2020), while others argue that PDA is a common mental health condition prevalent in the general population (Woods, 2018). Given that autism might underpin the development of PDA behaviours, we include a causal link between autism and PDA. The relationship between probabilistic learning (under the umbrella of sensory processing differences, labelled in the DAG above as SensoryProcessing) and PDA has yet to be empirically questioned- this relationship represents our primary interest.

# Data simulation

## Simulation

### RTs, group, probability condition, and trial type

The following code was based on code sourced here: <https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/>

We simulate seven variables:

blockcode- is a categorical variable that represents each condition in our probabilistic learning paradigm. Values range 1-4 to represent four experimental conditions.

trial_n- is an ordinal variable that represents the number of trails each participant completes in each condition of our experiment.

latency- is a measure of reaction time (RT); how quickly participants respond in each trial after stimulus onset.

label- a categorical variable that represents trial type; TT stands for target-target indicating a target trial that is preceded by a target trail, and it stands for discractor-target indicating a target trial that is preceded by an distractor trial.

id- a categorical variable that represents individual participant IDs.

group- a categorical variable consisting of PDA, ASC, and NT categories. Because PDA lack formal diagnosis, we created a PDA group utilising the top 33% of EDAQ scores.

EDAQ- represents a total score on the EDA-QA, a descriptive measure of PDA.

Create function to simulate data:

```{r}
gen_trials_from_block <- function(blockcode, n_trials, mu, sigma, group) {
  # Generate label for each trial
  label <- sample(c("DT", "TT"), n_trials, replace = TRUE)
  
  # Adjust mu based on group, blockcode, and label
  adjusted_mu <- case_when(
    blockcode == 1 & label == "TT" ~ mu * (1 - 0.3),  # same adjustment for "TT" in block 1 for all groups
    blockcode == 1 & label == "DT" ~ mu * (1 - 0.01),  # same adjustment for "DT" in block 1 for all groups
    group == "NT" & label == "TT" ~ mu * (1 - 0.07 * (5-blockcode)),
    group == "NT" & label == "DT" ~ mu * (1 - 0.02 * blockcode),
    group == "ASC" & label == "TT" ~ mu * (1 - 0.03 * blockcode),
    group == "ASC" & label == "DT" ~ mu * (1 - 0.01 * blockcode),
    group == "PDA" & label == "TT" ~ mu * (1 - 0.04 * blockcode),
    group == "PDA" & label == "DT" ~ mu * (1 - 0.02 * blockcode),
    TRUE ~ mu
  )
  
  tibble(
    blockcode = blockcode,
    trial_n = as.numeric(1:n_trials),
    latency = rnorm(n_trials, adjusted_mu, sigma),
    label = label
  )
}

#Create a funciton that generates individual particiapnt data:
gen_participant <- function(id) {
  
  # Define mu block values for each diagnosis level
  mu_values <- list(
    NT = c(325, 350, 375, 400),
    ASC = c(330, 355, 370, 405),
    PDA = c(340, 360, 375, 410)
  )
  
  # Define sigma block values
  sigma <- c(50, 50, 50, 50)
  
  # Generate group for the participant
  group <- sample(c("NT", "ASC", "PDA"), 1)
  
  # Assign mu and edaq_mu based on group
  mu <- mu_values[[group]]
  edaq_mu <- ifelse(group == "NT", 72, ifelse(group == "ASC", 90, 100))
  
  blocks <- tibble(
    blockcode = as.numeric(1:4),
    n_trials = rnorm(4, c(92.58824, 76.11765, 74.35294, 72.88235), c(4.975676, 4.060209, 5.555866, 5.048296)),
    mu = mu,
    sigma = sigma
  )
  
  # Generate trials with adjusted latencies
  trials <- pmap_df(blocks, gen_trials_from_block, group = group)
  
  # Add id and group variable to the trials data frame
  trials$id <- as.numeric(id)
  trials$group <- as.factor(group)
  
  # Simulate EDAQ with a positive relationship with latency, adjusted based on group
  trials$EDAQ <- as.numeric(sample(round(pmin(abs(rnorm(1, trials$latency)), edaq_mu)), 1))
  
  return(trials)
}

d <- map_df(1:1e2, gen_participant)
```

Inspect structure and write the simulated data to a .csv file:

```{r}
#Check simulated data structure:
str(d)

#Write data to csv for modelling (change name of file as appropriate):
#write.table(d, file="Sim_Data_Mod1.csv",sep=",",row.names=F)
```

### EDA-QA, neurotype, and sensitivity to probabilistic contingencies

We simulate three variables:

neurotype- a categorical variable that denotes whether each simulated participant is neurotypical (NT) or autistic (ASC).

II- simply refers to slopes calculated for median RTs across 84%, 67%, and 33% probability conditions for each participant, providing a measure of sensitivity to changing probabilistic contingencies. As such, larger II values indicate insensitivity to changing probabilistic contingencies, while smaller II values suggest sensitivity to changing probabilistic contingencies.

EDAQ- a self-report descriptive measure of PDA behaviours. Driven by theory, we hypothesise that slopes will predict EDAQ scores. As such, we need to build this into the simulation. It is also possible that PDA and autism are associated; we will build this possibility into the simulation as well.

Create function to simulate data:

```{r}
sim_d <- function(seed, n) {
  
  set.seed(seed) #Setting seeds will allow for reproducibility- we'll see the same output.
  
  tibble(neurotype = sample(as.factor(rep(c("NT", "ASC"), each = n)))) %>%  #sim neurotype; equal ASC/NT
    mutate(II = ifelse(neurotype == "NT",  
                           rnorm(n, -0.5, sd = 0.2),
                           rnorm(n, 0, sd = 0.3)), #sim diagnosis -> II. These values relate to the coefficient of the II
           
           EDAQ = ifelse(neurotype == "NT",  
                         rnorm(n, 0.7*II-0.5, sd = 0.2),
                         rnorm(n, 0.7*II+0.5, sd = 0.3))) #sim diagnosis ->  II. The first value here relates to the coefficient (i.e., the relationship between II and EDAQ), and the second value relates to EDA-QA scores (i.e., how high/low each neurotype tends to score on the EDA-QA)

}

d <- sim_d(123,100) #This simulates 200 participants; 100 of each diagnostic category.
```

Inspect structure and write the simulated data to a .csv file:

```{r}
#Check simulated data structure:
str(d)

#Write data to csv for modelling (change name of file as appropriate):
#write.table(d, file="Sim_Data_Mod3.csv",sep=",",row.names=F)
```

## Visualisation

### RTs predicted by group × probability condition

We can plot the simulation to visualise the expected relationship between condition and group:

```{r}
#Read in saved data file:
mod1_data <- read.csv("Sim_Data_Mod1.csv") %>%
  mutate(RTs_std = standardize(latency),
         group = factor(group),
         blockcode = factor(blockcode),
         label = factor(label))

#Plot raw data:
ggplot(mod1_data, aes(group, latency, fill = blockcode)) + 
  geom_boxplot() +
  labs(x = "Group", y = "Simulated RTs", fill = "Condition") +
  ggtitle("Simulated RTs plotted for each group by probability condition") +
  scale_fill_manual(values = c("1" = "#66cccc", "2" = "#336666", "3" = "#cc99ff", "4" = "#336999"),
                     labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  scale_x_discrete(labels = c("1" = "NT", "2" = "ASC", "3" = "PDA")) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
mod1_data_means <- mod1_data %>%
  group_by(group, blockcode) %>%
  summarise(
    mean_RT = mean(latency, na.rm = TRUE),
    sd_RT = sd(latency, na.rm = TRUE)
  )

ggplot(mod1_data_means, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Probability of cue-target pair", y = "Simulated RTs (ms)", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
   scale_color_manual(values = c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666"),  breaks = c("NT", "ASC", "PDA")) +
  scale_x_discrete(labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
ggsave( 'RT_cond_sim.png', plot = last_plot(), dpi = 300)
```

Reisli et al., (2023) found that CNV amplitudes for 84%, 67%, and 33% probability conditions were reduced in their autistic cohort compared to their neurotypical cohort. The autistic group also showed larger CNV amplitude differences between 100% and 80% probability conditions. RT data showed overall faster RTs in response to greater probability in both autistic and neurotypical groups, mirroring Lawson et al. (2017) who also found a similar trend in both groups' RTs. Additionally, RTs demonstrated a pattern similar to CNV amplitudes; for the neurotypical group, RTs scaled with probability condition (i.e., quicker for 100%, slower for 33%), for the autistic group, RTs did not differ between 84% and 67% conditions. Reisli et al. (2023) conclude that this pattern of diminished differentiation between probability conditions in autism is indicative of a binary classification of unceratinty. Here, instead of inferring the degree to which an environment is uncertain based upon probabilistic information, autistic individuals are thought to categorise an environment as either certain or not.

Based on Reisli et al., (2023), we expect that the neurotypical group (NT) will demonstrate incrementally slower RTs as a function of condition. That is, participants will perform faster when probability is high than when probability is low. Given that PDA is categorically associated with autism (either as a subgroup; Christie, 2007; or a form of autistic self advocacy; Milton, 2013; Moore, 2020) we anticipate a pattern of RTs that conform to a binary classification of volatility for both the autistic (ASC) and PDA groups; RTs that do not differ between 84% and 67% conditions.

### RTs predicted by group × probability condition × trial type

We can also plot the simulation to visualise the expected relationship between group, condition, and trial type:

```{r}
#We should also derive means for each tiral type:
mod2_data_means <- read.csv("Sim_Data_Mod1.csv") %>%
  group_by(group, blockcode, label) %>%
  summarise(
    mean_RT = mean(latency, na.rm = TRUE),
    sd_RT = sd(latency, na.rm = TRUE)
  )

#Plot:
ggplot(mod2_data_means, aes(x = label, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "RTs (ms)", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )

####################################################################################################################
#And without 100% (there are only TT trials in the 100% condition; conceptually, it doesn't make sense to include the 100% condition):

#Derive means for each tiral type excluding the 100% condition:
mod2_data_means <- read.csv("Sim_Data_Mod1.csv") %>%
  filter(blockcode != "1") %>%  # Exclude the 100% condition
  group_by(group, blockcode, label) %>%
  summarise(
    mean_RT = mean(latency, na.rm = TRUE),
    sd_RT = sd(latency, na.rm = TRUE)
  ) %>%
  mutate(mean_RT = ifelse(blockcode == "4" & group %in% c("ASC", "PDA"), mean_RT - 30, mean_RT))

# Plot:
ggplot(mod2_data_means, aes(x = label, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "RTs (ms)", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
   scale_color_manual(values = c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666"),  breaks = c("NT", "ASC", "PDA")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
ggsave( 'TTDT sim plot.png', plot = last_plot(), dpi = 300)
```

It is argued that a pattern of incrementally longer RTs as a function of reduced probability (i.e., short RTs for high probability conditions, and long RTs for low probability conditions) represents appropriate volatility estimation. However, this pattern could emerge via two mechanisms: 1) where participants learn probabilistic contingencies to form expectations about cue-outcome associations (i.e., estimate certainty that a cue will predict an outcome), RTs should be shorter when probability is high (i.e., more certain), and longer when probability is low (i.e., less certain), or 2) as an artifact of task design. In the latter case, the proportion of target and distractor trials within each condition may influence median RTs.

Assuming appropriate learning of probabilistic contingencies, participants should form expectations regarding cue-target probabilities. For example, in the 84% condition, a target should be expected on approximately 84% of trials. Deviations from expectations (e.g., the occurrence of a distractor trial in the 84% condition) should slow responses. As such, participants who are sensitive to changing probabilistic contingencies are expected to respond faster to target trials when they are frequent (e.g., high-probability conditions) compared to when they occur less frequently (e.g., low-probability conditions). Conversely, when distractor trials are rare (e.g., in a high probability conditions), their occurrence should slow participant responses to subsequent target trials more than when they occur frequently (e.g., low probability conditions).

In contrast, relying on the previous trial (i.e., *n* - 1) to inform expectations of the current trial outcome might represent difficulties learning probabilistic contingencies – which theoretically could be indicative of individuals who treat probabilistic outcomes as wholly uncertain. In this instance, targets preceded by targets (TT) would be responded to faster than targets preceded by distractors (DT). This would lead to a pattern of median RTs differ between TT and DT trials within each probability condition, but not between probability condition – indicating that probability contingencies had not been learnt.

However, target and distractor trials are unevenly distributed in each probability condition (i.e., conditions in order of most target and fewest distractor trials: 100%, 84%, 67%, then 33% – note that 100% contains no distractor trials). As such, median RTs might be unduly influenced by the proportion of target and distractor trials within each condition. For example, conditions with more target/TT trials might have shorter median RTs compared to conditions with fewer target trials. Conversely, conditions with more distractor/DT trials might have longer median RTs compared to conditions with fewer distractor trials. This could artificially create a pattern of incrementally slower RTs as a function of reduced probability.

### EDA-QA predicted by II

Plot simulated data:

```{r}
#Read in the simulated data:
mod3_d <- read.csv("Sim_Data_Mod3.csv") 

#Plot:
ggplot(data = mod3_d, aes(x = II, y = EDAQ, color = neurotype)) +
  geom_point() +
  labs(x = "Insensitivity Index (std)", y = "EDA-QA scores (std)", color = "Neurotype") +
  ggtitle("Simulated data demonstrating the predicted relationship between 
II and EDA-QA scores by group") +
   scale_color_manual(values = c( "NT" = "#66cccc", "ASC" = "#cc99ff"),  breaks = c("NT", "ASC")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )
ggsave( 'SI_PDA sim plot.png', plot = last_plot(), dpi = 300)
```

EDA-QA scores and Insensitivity Indices (II) were also simulated for each neurotype (i.e., neurotypical and autistic individuals). II simply refers to slopes calculated for median RTs across 84%, 67%, and 33% probability conditions for each participant, providing a measure of sensitivity to changing probabilistic contingencies. As such, larger II values indicate insensitivity to changing probabilistic contingencies, while smaller II values suggest sensitivity to changing probabilistic contingencies. Simulation assumed that for both neurotypical and autistic data, II would predict scores of a descriptive measure of PDA behaviour (EDA-QA). Though an overall positive trend for both neurotypical and autistic data was anticipated, neurotypical and autistic data were specified separately. This was to capture any potential difference between neurotypes. For example, because PDA is thought to have greater prevalence in the autistic population, it was expected that autistic data would include higher EDA-QA scores. It was expected neurotypical individuals would on average generate smaller II values compared to autistic individuals, indicating greater sensitivity to changing probabilistic contingencies. Autistic individuals were expected to have II values around 0, in line with Reisli et al. (2023), where 0 is indicative of insensitivity to changing probabilistic contingencies. Simulated data in this Chapter reflected these assumptions. Potential differences in how sensitivity to changing probabilistic contingencies relates to PDA in neurotypical and autistic individuals were of particular interest. For example, it might be that the relationship is more pronounced for the autistic group, due to insensitivity to changing probabilistic contingencies and more instances of PDA behaviours. By considering both neurotypical and autistic data separately, these potential variations could be captured.

## Model checking

For those unfamiliar with brms() (specifically with brms() syntax), chapter 12 of the following webpage provides some useful guidance on how to specify a brms() model: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

### RTs predicted by group × probability condition

To better understand if the reaction times (RTs) of PDA and autistic individuals differ less for adjacent conditions (84%, 67%, and 33% conditions) compared to neurotypical individuals, and if PDA and autistic individuals preform similarly (i.e., PDA and autistic individuals perform similarly across 100%, 84%, 67%, and 33% conditions), we will run a Bayesian regression model where participant responses (RTs) to probabilistic learning task are predicted by a group, condition interaction:

$$
RT_i ∼ Normal( μ_i, σ ) \\
μ_i = α_{group, condition​}  \\
α_{group, condition​} ∼ ​Normal(0, 1) \\
σ ∼ Exponential(1)
$$Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the model to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Fit the model:
sim_mod1_brm <- brm(RTs_std ~ 0 + group:blockcode,
            data = mod1_data,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(sim_mod1_brm, file = "Sim_mod1_brm_output.rds")

#Load the model output from a file:
sim_mod1_brm <- readRDS("Sim_mod1_brm_output.rds")

#Return model output:
summary(sim_mod1_brm)
```

Plot parameter distributions with means and credible intervals:

```{r}
#Extract posterior samples:
#sim_mod1_brm_posterior <- posterior_samples(sim_mod1_brm)

#To visualise later, save to .csv:
#write.table(sim_mod1_brm_posterior, file="Sim_mod1_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
sim_mod1_brm_posterior <- read.csv("Sim_mod1_brm_posterior.csv")

plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(sim_mod1_brm_posterior,
           pars =  c("b_groupASC.blockcode1", "b_groupNT.blockcode1", "b_groupPDA.blockcode1", 
                     "b_groupASC.blockcode2", "b_groupNT.blockcode2", "b_groupPDA.blockcode2", 
                     "b_groupASC.blockcode3", "b_groupNT.blockcode3", "b_groupPDA.blockcode3", 
                     "b_groupASC.blockcode4", "b_groupNT.blockcode4", "b_groupPDA.blockcode4", 
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")

```

Finally, we can visualise our model-predicted RT values for each group by condition:

```{r}
#First, extract predicted values:
#sim_mod1_brm_preds <- data.frame(fitted(sim_mod1_brm))

#As above, to visualise later, save to .csv:
#write.table(sim_mod1_brm_preds, file="Sim_mod1_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
sim_mod1_brm_preds <- read.csv("Sim_mod1_brm_preds.csv")

#Add group and blockcode variables to the data frame:
sim_mod1_brm_preds <- sim_mod1_brm_preds  %>%
  mutate(group = mod1_data$group,
         blockcode = mod1_data$blockcode)

#Create plot, the first showing predicted RTs for each group by probability condition, the second showing predicted RTs for each probability condition by group:
ggplot(sim_mod1_brm_preds, aes(x = group, y = Estimate, color = blockcode, group = blockcode)) +
  geom_point() +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  labs(x = "Group", y = "RTs", color = "Condition") +
  ggtitle("Predicted RTs for each group by probability condition") +
  scale_color_manual(values = c("1" = "#66cccc", "2" = "#336666", "3" = "#cc99ff", "4" = "#336999"),
                     labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  scale_x_discrete(labels = c("1" = "NT", "2" = "ASC", "3" = "PDA")) +
  theme_minimal()

ggplot(sim_mod1_brm_preds, aes(x = blockcode, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  labs(x = "Group", y = "RTs", color = "Group") +
  ggtitle("Predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  theme_minimal()
```

The plots above depict predicted RTs in two different ways; the first shows predicted RTs for each group by probability condition, the second shows predicted RTs for each probability condition by group. Both plots illustrate a group difference between ASC and PDA groups and the NT group. For the NT group, our model predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions). While ASC and PDA groups show a similar pattern of diminishing RTs as a function of probability, the difference between 84% and 67% probability conditions is small/non-existent. This is in line with Reisli et al. (2023) who found that autistic individuals generated similar RTs for 84% and 67% probability conditions in a similar task. Reisli et al. (2023) concluded that this pattern of performance was indicative of an insensitivity to changing probabilistic contingencies, and suggested that autistic individuals might perceive uncertainty as a binary operation (i.e., either an environment is certain or not). Here, we expect to replicate Reisli et al.'s original finding that autistic individuals show less difference between probability conditions (specifically, 84% and 67%). Assuming that PDA is categorically linked to autism, we expect that PDA individuals will perform similarly to autistic individuals.

### RTs predicted by group × probability condition × trial type

To further investigate how PDA and autistic individuals learn about probbailistic contingecies compared to neurotypical individuals, we will also run a Bayesian regression model where participant reponses (RTs) to probabilistic learning task are predicted by a group, condition, label interaction:

$$
RT_i ∼ Normal( μ_i, σ ) \\
μ_i = α_{group, condition​, trial type}  \\
α_{group, condition, trial type​} ∼ ​Normal(0, 1) \\
σ ∼ Exponential(1)
$$Soon we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Now fit the model to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Fit the model:
sim_mod2_brm <- brm(RTs_std ~ 0 + group:blockcode:label,
            data = mod1_data,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(sim_mod2_brm, file = "Sim_mod2_brm_output.rds")

#Load the model output from a file:
sim_mod2_brm <- readRDS("Sim_mod2_brm_output.rds")

#Return model output:
summary(sim_mod2_brm)
```

Plot parameter distributions with means and credible intervals:

```{r}
#Extract posterior samples:
#sim_mod2_brm_posterior <- posterior_samples(sim_mod2_brm)

#To visualise later, save to .csv:
#write.table(sim_mod2_brm_posterior, file="Sim_mod2_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
sim_mod2_brm_posterior <- read.csv("Sim_mod2_brm_posterior.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(sim_mod2_brm_posterior,
           pars =  c("b_groupASC.blockcode1.labelit", "b_groupNT.blockcode1.labelit", 
                     "b_groupPDA.blockcode1.labelit", "b_groupASC.blockcode2.labelit",
                     "b_groupNT.blockcode2.labelit", "b_groupPDA.blockcode2.labelit", 
                     "b_groupASC.blockcode3.labelit", "b_groupNT.blockcode3.labelit",
                     "b_groupPDA.blockcode3.labelit", "b_groupASC.blockcode4.labelit",
                     "b_groupNT.blockcode4.labelit", "b_groupPDA.blockcode4.labelit",
                     "b_groupASC.blockcode1.labeltt", "b_groupNT.blockcode1.labeltt",
                     "b_groupPDA.blockcode1.labeltt", "b_groupASC.blockcode2.labeltt", 
                     "b_groupNT.blockcode2.labeltt", "b_groupPDA.blockcode2.labeltt", 
                     "b_groupASC.blockcode3.labeltt", "b_groupNT.blockcode3.labeltt", 
                     "b_groupPDA.blockcode3.labeltt", "b_groupASC.blockcode4.labeltt", 
                     "b_groupNT.blockcode4.labeltt", "b_groupPDA.blockcode4.labeltt",
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")

```

Finally, we can visualise our model-predicted RT values for each group by condition:

```{r}
#First, extract predicted values:
sim_mod2_brm_preds <- data.frame(fitted(sim_mod2_brm))

#As above, to visualise later, save to .csv:
write.table(sim_mod2_brm_preds, file="Sim_mod2_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
sim_mod2_brm_preds <- read.csv("Sim_mod2_brm_preds.csv")

#Add group and blockcode variables to the data frame:
sim_mod2_brm_preds <- sim_mod2_brm_preds  %>%
  mutate(group = mod1_data$group,
         blockcode = mod1_data$blockcode,
         label = mod1_data$label)

#Create plot  showing predicted RTs for each trial type by probability condition and group:
ggplot(sim_mod2_brm_preds, aes(x = label, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal()
```

The plot above depicts predicted RTs for each trial type (DT and TT) in each probability condition by group. The plot illustrates a group difference between ASC and PDA groups and the NT group. For the NT group, our model predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions). In addition, the difference between RTs for it and tt trials is predicted as a function of probability condition; participants are faster in TT compared to DT trials while the probability of a cue indicating a target is high (e.g., 84% probability condition) and diminishing differences in RTs between tt and it trials when probability of a cue indicating a target is low (e.g., 33% probability condition). ASC and PDA groups show a similar pattern of diminishing RTs as a function of probability, however, the difference between 84% and 67% probability conditions is small. This is in line with Reisli et al. (2023)- see above for details (notes on visualisation for model 1). Additionally, the model predicts less variation in RTs between DT and TT trials across each probability condition, reflecting a tendency to only use the previous trial as informative of the current trial outcome (i.e., an distractor trial is treated as indicative of another distractor trial). Thus, in each probability condition, RTs for TT trials are faster than DT trials.

### EDA-QA predicted by II

We are also interested in the relationship between probabilistic learning and PDA behaviours; we will run a Bayesian regression where EDAQ scores (a self-report descriptive measure of PDA behaviours) are predicted by II (an experimentally derived measure of sensitivity to changing probabilistic contingencies). $$
EDAQ_i ∼ Normal( μ_i, σ ) \\
μ_i = \alpha + \beta_{II} \\
α ∼ ​Normal( 0 , 1 ) \\
β ∼ Normal(0, 1) \\
σ ∼ Exponential(1)
$$

It is possible that there are differences in how sensitivity to changing probabilistic contingencies relates to PDA in neurotypical and autistic individuals. For example, it might be that the relationship is more pronounced for the autistic group, due to insensitivity to changing probabilistic contingencies and more instances of PDA behaviours. A model was constructed to explore potential effects of autism diagnosis:\
$$
EDAQ_i ∼ Normal( μ_i, σ ) \\
μ_i = \alpha + \beta_{neurotype_{NT}} + \beta_{II} + \beta_{{neurotype}_{{NT}} \times \ II} \\
α ∼ ​Normal( 0 , 1 ) \\
β ∼ Normal(0, 1) \\
σ ∼ Exponential(1)
$$

Again, later we will perform prior predictive checks and sensitivity analysis to justify our choice of priors, but for now, we will use uninformative flat priors to check the simulation is working- because we simulate standardised variables we will assume $Normal(0,1)$.

Fit both models to the simulated data- again, in lieu of justified priors, we will assume $Normal(0,1)$:

```{r}
#Fit the model 3:
mod3_brm <- brm(EDAQ ~ II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod3_brm, file = "mod3_brm_output.rds")

#Fit the model 4:
mod4_brm <- brm(EDAQ ~ neurotype * II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod4_brm, file = "mod4_brm_output.rds")

#Load the model output from file:
mod3_brm <- readRDS("mod3_brm_output.rds")
mod4_brm <- readRDS("mod4_brm_output.rds")

#Return model output:
summary(mod3_brm)
summary(mod4_brm)

#Get priors (if interested):
#get_prior(mod3_brm)
#get_prior(mod4_brm)
```

Plot parameter distributions with means and credible intervals:

```{r}
#Extract posterior samples:
mod3_brm_posterior <- posterior_samples(mod3_brm)

#To visualise later, save to .csv:
#write.table(mod3_brm_posterior, file="mod3_brm_posterior_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_posterior <- read.csv("mod3_brm_posterior_24.06.24.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod3_brm_posterior,
           pars =  c("b_Intercept", "b_II", "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

And again, plot parameter distributions with means and credible intervals for model including neurotype:

```{r}
#Extract posterior samples:
mod4_brm_posterior <- posterior_samples(mod4_brm)

#To visualise later, save to .csv:
#write.table(mod4_brm_posterior, file="mod4_brm_posterior_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_posterior <- read.csv("mod4_brm_posterior_24.06.24.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod4_brm_posterior,
           pars =  c("b_Intercept", "b_II", "b_neurotypeNT", "b_neurotypeNT.II", "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Plot the observed data points and predicted lines:

```{r}
#First, extract predicted values for model 3:
#mod3_brm_preds <- data.frame(fitted(mod3_brm))

#As above, to visualise later, save to .csv:
#write.table(mod3_brm_preds, file="mod3_brm_preds_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_preds <- read.csv("mod3_brm_preds_24.06.24.csv")

#Add group and blockcode variables to the data frame:
mod3_brm_preds <- mod3_brm_preds %>%
  mutate( slopes = mod3_d$slope,
  EDAQ = mod3_d$EDAQ,
  group = mod3_d$group
)

#Create the plot:
ggplot(mod3_brm_preds, aes(x = II, y = EDAQ)) +
  geom_point(color = "#339999") +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate), color = "#339999") +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, fill = "#339999") +
  labs(x = "II (std)", y = "EDA-QA scores (std)") +
  ggtitle("Simulated data demonstrating the predicted relationship between II and EDA-QA scores") +
  theme_minimal()

#Repeat for model 4, extract predicted values:
#mod4_brm_preds <- data.frame(fitted(mod4_brm))

#As above, to visualise later, save to .csv:
#write.table(mod4_brm_preds, file="mod4_brm_preds_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_preds <- read.csv("mod4_brm_preds_24.06.24.csv")

#Add group and blockcode variables to the data frame:
mod4_brm_preds <- mod4_brm_preds %>%
  mutate( II = mod3_d$II,
  EDAQ = mod3_d$EDAQ,
  neurotype = mod3_d$neurotype
)

#Create the plot:
ggplot(mod4_brm_preds, aes(x = II, y = EDAQ, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate, fill = group)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = group), alpha = 0.3, color = NA) +
  labs(x = "II (std)", y = "EDA-QA scores (std)", color = "Neurotype", fill = "Neurotype") +
  ggtitle("Simulated data demonstrating the predicted relationship between II and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  scale_fill_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  theme_minimal()
```

Above are two plots; one II predicting EDA-QA scores (model 3), while the other depicts II predicting EDA-QA scores by neurotype (model 4). Based on parameter values plugged into the simulation above, model 3 suggests a strong relationship between II and EDA-QA scores; without considering differences related to neurotype, insensitivity to changing probabilistic contingencies is strongly associated with PDA behaviours. However, the second plot (model 4) suggested that ASC participants should overall have higher EDA-QA scores than NTs (representing greater prevalence of PDA in the autistic population compared to the neurotypical population), and II values that centre around 0 (representing insensitivity to changing probabilistic contingencies). NTs should have II values that (mostly) sit below 0 (representing sensitivity to changing probabilistic contingencies). In both neurotypes, II values get larger with EDA-QA score (suggesting that insensitivity to changing probabilistic contingencies is associated with a greater number of PDA behaviours), but this association is weaker when considering each neurotype separately - this is owing to differences in average EDA-QA scores and II between neurotypes.

# Prior predictive checks

"A prior predictive check displays simulated data that are generated from parameter values in the prior distribution. The simulated data from the mathematically specified prior should show trends that match the trends assumed by prior knowledge" (Kruschke, 2021)

We want to know that our choice of priors accurately represents our assumptions about our parameters. To do this, we can simulate priors from our model and relate them to observed values (in this case, simulated observations).

We have chosen weakly informative priors to begin with- because RTs are standardized, we can start with the priors 0,1.

### RTs predicted by group × probability condition

We use our simulated data to model RTs by group and probability condition (model 1):

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between RTs and group, blockcode:
mod1_brm_prior <- brm(RTs_std ~ 0 + group:blockcode,
                      data = mod1_data,
                      backend = "cmdstan",
                      prior = prior(normal(0, 1), class = "b"),
                      sample_prior = "only")

#Save the model output to a file:
#saveRDS(mod1_brm_prior, file = "mod1_brm_prior.rds")
```

We can inspect the output here:

```{r}
#Load the model output from a file:
mod1_brm_prior <- readRDS("mod1_brm_prior.rds")

#Return model output:
summary(mod1_brm_prior)
```

Draw samples from the prior predictive simulation above:

```{r}
#Draw prior samples:
mod1_brm_prior_PPC <- add_linpred_draws(
  mod1_brm_prior, 
  newdata = tibble(group = mod1_data$group, blockcode = mod1_data$blockcode),
  ndraws = 100,
  value = 'Predicted_RT'
) |> 
  ungroup() |> 
  select(Predicted_RT, .draw, group, blockcode) |> 
  group_by(group, blockcode) |> 
  summarise(
    mean_RT = mean(Predicted_RT),
    sd_RT = sd(Predicted_RT),
    se_RT = sd_RT / sqrt(n()),
    .groups = "drop"
  )

#Print:
mod1_brm_prior_PPC
```

Plot a prior predictive simulation:

```{r}
#Plot prior predictive simulation:
ggplot(mod1_brm_prior_PPC, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Group", y = "Prior predicted RTs", color = "Group") +
  ggtitle("Prior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
mod1_data$RT_std <- standardize(mod1_data$latency)
mod1_data_means <- mod1_data %>%
  group_by(group, blockcode) %>%
  summarise(
    mean_RT = mean(RT_std, na.rm = TRUE),
    sd_RT = sd(RT_std, na.rm = TRUE)
  )

#Plot simulated data:
ggplot(mod1_data_means, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Group", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod1_brm_prior, nsamples = 100)
```

For clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed (simulated) data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### RTs predicted by group × probability condition × trial type

We also need to check our prior predictions for model 2. We use our simulated data to model RTs by group, probability condition, and trial type; load in model and inspect output:

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between RTs and group, blockcode:
mod2_brm_prior <- brm(RTs_std ~ 0 + group:blockcode,
                      data = mod1_data,
                      backend = "cmdstan",
                      prior = prior(normal(0, 1), class = "b"),
                      sample_prior = "only")

#Save the model output to a file:
#saveRDS(mod2_brm_prior, file = "mod2_brm_prior.rds")
```

```{r}
#Load the model output from file:
mod2_brm_prior <- readRDS("mod2_brm_prior.rds")

#Return model output:
summary(mod2_brm_prior)
```

Draw samples from the prior predictive simulation above:

```{r}
#Draw prior samples:
mod2_brm_PPC <- add_linpred_draws(
  mod2_brm_prior, 
  newdata = tibble(group = mod1_data$group, blockcode = mod1_data$blockcode, label = mod1_data$label),
  ndraws = 100,
  value = 'Predicted_RT'
) |> 
  ungroup() |> 
  select(Predicted_RT, .draw, group, blockcode, label) |> 
  group_by(group, blockcode, label) |> 
  summarise(
    mean_RT = mean(Predicted_RT),
    sd_RT = sd(Predicted_RT),
    se_RT = sd_RT / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod2_brm_PPC)
```

Plot a prior predictive simulation:

```{r}
#Plot prior predictive simulation:
ggplot(mod2_brm_PPC, aes(x = label, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "Prior predicted RTs", color = "Group") +
  ggtitle("Prior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
mod1_data$RT_std <- standardize(mod1_data$latency)
mod2_data_means <- mod1_data %>%
  group_by(group, blockcode, label) %>%
  summarise(
    mean_RT = mean(RT_std, na.rm = TRUE),
    sd_RT = sd(RT_std, na.rm = TRUE)
  )

#Plot:
ggplot(mod2_data_means, aes(x = label, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod2_brm_prior, nsamples = 100)
```

Again, for clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed (simulated) data, while the 'y_rep' lines represent the distribution of data simulated from the model priors (based on a 100 draws from the prior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### EDA-QA predicted by II

We also need to check our prior predictions for model 3 and 4. We use our simulated data to model EDA-QA scores as predicted by II for each group (model3), and EDA-QA scores as predicted by II for each neurotype (model 4).

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between II and EDA-QA scores:
mod3_brm_prior <- brm(EDAQ ~ II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
    sample_prior = "only",
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod3_brm_prior, file = "mod3_brm_prior.rds")

#A model with broad priors that acommodate uncertainty pertaining to the relationship between II and EDA-QA scores by neurotype:
mod4_brm_prior <- brm(EDAQ ~ neurotype * II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
    sample_prior = "only",
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod4_brm_prior, file = "mod4_brm_prior.rds")
```

Load in models and inspect output:

```{r}
#Load the model output from file:
mod3_brm_prior <- readRDS("mod3_brm_prior.rds")
mod4_brm_prior <- readRDS("mod4_brm_prior.rds")

#Return model output:
summary(mod3_brm_prior)
summary(mod4_brm_prior)
```

Draw samples from the prior predictive simulations above:

```{r}
#Draw prior samples for model 3:
mod3_brm_PPC <- add_linpred_draws(
  mod3_brm, 
  newdata = tibble(II = mod3_d$II),
  ndraws = 100,
  value = 'Predicted_EDAQ'
) |> 
  ungroup() |> 
  select(Predicted_EDAQ, .draw, II) |> 
  group_by(II) |> 
  summarise(
    mean_Predicted_EDAQ = mean(Predicted_EDAQ),
    sd_Predicted_EDAQ = sd(Predicted_EDAQ),
    se_Predicted_EDAQ = sd_Predicted_EDAQ / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod3_brm_PPC)

#And again for model 4; draw prior samples:
mod4_brm_PPC <- add_linpred_draws(
  mod4_brm, 
  newdata = tibble(neurotype = mod3_d$neurotype, II = mod3_d$II),
  ndraws = 100,
  value = 'Predicted_EDAQ'
) |> 
  ungroup() |> 
  select(Predicted_EDAQ, .draw, neurotype, II) |> 
  group_by(neurotype, II) |> 
  summarise(
    mean_Predicted_EDAQ = mean(Predicted_EDAQ),
    sd_Predicted_EDAQ = sd(Predicted_EDAQ),
    se_Predicted_EDAQ = sd_Predicted_EDAQ / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod4_brm_PPC)
```

Plot a prior predictive simulation for model 3:

```{r}
#Plot prior predictive simulation:
ggplot(mod3_brm_PPC, aes(x = II, y = mean_Predicted_EDAQ)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  geom_ribbon(aes(ymin = mean_Predicted_EDAQ - sd_Predicted_EDAQ, ymax = mean_Predicted_EDAQ + sd_Predicted_EDAQ), alpha = 0.3, color = NA) +
  geom_point(data = mod3_d, aes(x = II, y = EDAQ)) +
  labs(x = "II (std)", y = "Prior predicted EDA-QA scores (std)") +
  ggtitle("Prior predicted EDA-QA scores") +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod3_brm_prior, ndraws = 100)
```

Once again, for clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed (simulated) data, while the 'y_rep' lines represent the distribution of data simulated from the model priors (based on a 100 draws from the prior distribution of the model parameters). The simulated data is made up of two Gaussian distributions, one centered around -0.5 (NT) and the other around 0 (ASC). Prior predicted values seemingly overlap well with our simulated data; prior predictions assume the bulk of estimates to be approx. within the -2 to 2 range, which fits well with our simulated data values.

Now plot a prior predictive simulation for model 4:

```{r}
#Plot prior predictive simulation:
ggplot(mod4_brm_PPC, aes(x = II, y = mean_Predicted_EDAQ, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(fill = neurotype)) +
  geom_ribbon(aes(ymin = mean_Predicted_EDAQ - sd_Predicted_EDAQ, ymax = mean_Predicted_EDAQ + sd_Predicted_EDAQ, fill = neurotype), alpha = 0.3, color = NA) +
  geom_point(data = mod3_d, aes(x = II, y = EDAQ, color = neurotype)) +
  labs(x = "II (std)", y = "Prior predicted EDA-QA scores (std)", color = "Neurotype", fill = "Neurotype") +
  ggtitle("Prior predicted EDA-QA scores by neurotype") +
  scale_color_manual(values = c("ASC" = "#336666", "NT" = "#cc99ff")) +
  scale_fill_manual(values = c("ASC" = "#336666", "NT" = "#cc99ff")) +
  theme_minimal()


#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod4_brm_prior, ndraws = 100)
```

Again, the simulated data is made up of two Gaussian distributions, one centered around -0.5 (NT) and the other around 0 (ASC). Again, prior predictions assume the bulk of estimates to be approx. within the -2 to 2 range, which fits well with our simulated data values.

# Computation and posterior distribution

## Details of the computation

All computations to derive posterior distributions for all three models were conducted using the brms() package (source details available in Package install_20.05.24.r- see packages and libraries); this includes ˆR and effective sample size (ESS).

Again, chapter 12 of the following webpage provides some useful guidance on how brms() models are specified: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

We must check that the MCMC chains for every parameter have converged and are long enough to provide stable estimates.

Convergence here is indicated by ˆR, which must be near 1.0 to indicate convergence. When ˆR is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn't trust the samples.

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain not to the sample size of the data. When n_eff is much lower than the actual number of iterations (minus warm-up) of your chains, it means the chains are inefficient, but possibly still okay.

### RTs predicted by group × probability condition

Once again, here is the output for model 1:

```{r}
#Load the model output from a file:
mod1_brm <- readRDS("mod1_brm_output.rds")

#Return model output:
summary(mod1_brm)

#We can also get model parameters:
get_prior(mod1_brm)
```

Both ˆR and n_eff (ESS) look ok; ˆR is around 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot. A trace plot plots the samples in sequential order, joined by a line- we look for three things in these trace plots: (1) stationarity, (2) good mixing, and (3) convergence. Stationarity refers to the path of each chain staying within the same high-probability portion of the posterior distribution. Good mixing means that the chain rapidly explores the full region. It doesn't slowly wander, but rather rapidly zig-zags around. Convergence means that multiple, independent chains stick around the same region of high probability.

```{r}
mcmc_trace( mod1_brm )
```

All chains look good.

### RTs predicted by group × probability condition × trial type

And again for model 2:

```{r}
#Load the model output from file:
mod2_brm <- readRDS("mod2_brm_output.rds")

#Return model output:
summary(mod2_brm)

#We can also get model parameters:
get_prior(mod2_brm)
```

Both ˆR and n_eff (ESS) look ok; ˆR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod2_brm )
```

Again, all chains look good.

### EDA-QA predicted by II

And once more for model 3:

```{r}
#Load the model output from file:
mod3_brm <- readRDS("mod3_brm_output.rds")

#Return model output:
summary(mod3_brm)

#We can also get model parameters:
get_prior(mod3_brm)
```

Both ˆR and n_eff (ESS) look ok; ˆR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod3_brm )
```

All chains look good.

And finally for model 4:

```{r}
#Load the model output from file:
mod4_brm <- readRDS("mod4_brm_output.rds")

#Return model output:
summary(mod4_brm)

#We can also get model parameters:
get_prior(mod4_brm)
```

Both ˆR and n_eff (ESS) look ok; ˆR is 1 for each parameter, and n_eff is \> than the actual number of iterations (minus warm-up) of the chain.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod3_brm )
```

All chains look good.

## Posterior predictive check

### RTs predicted by group × probability condition

Below we view the posterior predictions plotted against our (simulated) data for model 1.

```{r}
#Perform a graphical posterior predictive check
pp_check(mod1_brm, ndraws = 100)
```

As with out prior predictive checks, the 'y' line of the pp_check output represents the distribution of our observed (simulated) data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the posterior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### RTs predicted by group × probability condition × trial type

Below we view the posterior predictions plotted against our (simulated) data for model 2.

```{r}
#Perform a graphical posterior predictive check:
pp_check(mod2_brm, ndraws = 100)
```

Again, both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### EDA-QA predicted by II

Below we view the posterior predictions plotted against our (simulated) data for model 3 and 4.

```{r}
#Perform a graphical posterior predictive check:
pp_check(mod3_brm, ndraws = 100)
pp_check(mod4_brm, ndraws = 100)
```

Comparing the pp_checks for models 3 and 4, we can see that model 3 does a poor job of providing predictive estimates of the posterior. The posterior predictive estimates fairly to accurately represent the simulated data. This is because model 3 does not account for by group variance, which can be seen by the model predicts a Gaussian distribution of estimates, while the simulated data is in fact made up of two Gaussian distributions (one for each neurotype). Model 4 (that does consider differences between neurotypes) does a much better job of predicting the simulated data; both distributions closely overlap each other, in the shape of two Gaussian distributions (one centered around -0.5 (NT) and the other around 0 (ASC), suggesting that model predictions are consistent with the observed data.

## Marginal posterior distribution

### RTs predicted by group × probability condition

Once again, we can inspect the output for model 1, this time we will consider the marginal posterior distribution of each parameter (central tendency and credible intervals):

```{r}
#Return model output:
summary(mod1_brm)

#Extract posterior samples:
#mod1_brm_posterior <- posterior_samples(mod1_brm)

#To visualise later, save to .csv:
#write.table(mod1_brm_posterior, file="mod1_brm_posterior_13.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod1_brm_posterior <- read.csv("mod1_brm_posterior_13.06.24.csv")

plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod1_brm_posterior,
           pars =  c("b_groupASC.blockcode1", "b_groupNT.blockcode1", "b_groupPDA.blockcode1", 
                     "b_groupASC.blockcode2", "b_groupNT.blockcode2", "b_groupPDA.blockcode2", 
                     "b_groupASC.blockcode3", "b_groupNT.blockcode3", "b_groupPDA.blockcode3", 
                     "b_groupASC.blockcode4", "b_groupNT.blockcode4", "b_groupPDA.blockcode4", 
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Note that we are primarily interested in the relationship between groups for each condition. We will consider the marginal posterior distribution of each parameter by condition (blockcode).

For condition 1, there is overlap between NT and ASC groups, and between ASC and PDA groups, all sitting well below 0. This suggests that all groups perform similarly in condition 1 (100% probability)- that these estimates are the smallest indicates that this is the condition with the fastest RTs.

For condition 2, there is overlap between ASC and PDA groups, both groups sitting above zero. The NT group sits far from the other two, below 0. This suggests that while all groups performed slower in condition 2 (84% probability) compared to condition 1 (100% probability), the pattern of performance is different between groups in this condition; these estimates suggests that the NT group generated faster RTs than both the ASC and PDA groups (who performed comparatively).

For condition 3, there is overlap between NT and PDA groups, and between PDA and ASC groups, all sitting just above 0. Notably, estimates for the ASC and PDA group in condition 3 overlap with the estimates for the ASC and PDA estimates in condition 2. This suggests that ASC and PDA groups performed comparatively in both conditions.

For condition 4, there is overlap between NT and ASC groups, and between NT and PDA groups, all groups sitting well above zero. This suggests that all groups performed slowest in condition 4 (33% probability). The proximity of estimates suggests that all groups generated similar RTs in this condition.

Because we are specifically interested in PDA group performance in 67% and 84% probability conditions compared to the ASC and NT groups, we can compare group means:

```{r}
# Define a function to create the data frame
create_df <- function(param1, param2, group) {
  df <- tibble(
    Value = c(mod1_brm_posterior[[param1]], mod1_brm_posterior[[param2]], mod1_brm_posterior[[param1]] - mod1_brm_posterior[[param2]]),
    Parameter = rep(c(param1, param2, "Difference"), each = length(mod1_brm_posterior[[param1]])),
    Group = group
  )
  
  # Add a new column with simplified parameter names
  df <- df %>%
    mutate(SimpleParameter = case_when(
      Parameter == param1 ~ "TT",
      Parameter == param2 ~ "DT",
      Parameter == "Difference" ~ "Difference"
    ))
  
  return(df)
}

# Create the data frames
df1 <- create_df('b_groupPDA.blockcode2', 'b_groupPDA.blockcode3', "PDA")
df2 <- create_df('b_groupASC.blockcode2', 'b_groupASC.blockcode3', "ASC")
df3 <- create_df('b_groupNT.blockcode2', 'b_groupNT.blockcode3', "NT")

# Combine the data frames
df <- bind_rows(df1, df2, df3)

# Plot the distributions
ggplot(df, aes(x=Value, fill=SimpleParameter)) +
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(x="Value", y="Count", title="Distributions of parameter estimates showing differences between probability conditions by group") +
  theme_minimal() +
  scale_fill_manual(values=c("Difference" = "#336666", "TT" = "#66cccc", "DT" = "#cc99ff"),
                    labels=c("Difference", "84%", "67%"),
                    name="Condition") + 
  facet_wrap(~ Group, ncol = 3)
```

Visually comparing the differences between 84% and 67% probability conditions for each group, we can see that both ASC and PDA have a small difference of around 0.05 and 0.1, respectively. This suggests that both group performed comparably in both conditions, as expected. For the NT group, the difference between 84% and 67% probability conditions is much larger, just above 0.4. This pattern of differences between groups is indicative of our preditions that autistic and PDA individuals will demonstrate diminished differentiation between probability conditions compared to neurotypical individuals.

We can also re-plot the posterior predicted RT values:

```{r}
#First, extract predicted values:
#mod1_brm_preds <- data.frame(fitted(mod1_brm))

#As above, to visualise later, save to .csv:
#write.table(mod1_brm_preds, file="mod1_brm_preds_13.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod1_brm_preds <- read.csv("mod1_brm_preds_13.06.24.csv")

#Add group and blockcode variables to the data frame:
mod1_brm_preds <- mod1_brm_preds  %>%
  mutate(group = mod1_data$group,
         blockcode = mod1_data$blockcode)

#Create plot:
ggplot(mod1_brm_preds, aes(x = blockcode, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  #geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +
  labs(x = "Group", y = "Posterior predicted RTs", color = "Group") +
  ggtitle("Predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%")) +
  theme_minimal()
```

The plot above depicts posterior predicted RTs for each probability condition by group. For the NT group, as specified in our simulated data, the model predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions). Again, as specified in our simulated data, the ASC and PDA groups show a similar pattern of diminishing RTs as a function of probability, but the difference between 84% and 67% probability conditions is small/non-existent. We might conclude that because our model predictions reflect our simulated data, our model is functioning as intended.

### RTs predicted by group × probability condition × trial type

Repeat the process for model 2; we will consider the marginal posterior distribution of each parameter (central tendency and credible intervals).

```{r}
#Return model output:
summary(mod2_brm)

#Read in .csv to data frame:
mod2_brm_posterior <- read.csv("mod2_brm_posterior_24.06.24.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod2_brm_posterior,
           pars =  c("b_groupASC.blockcode1.labelit", "b_groupNT.blockcode1.labelit", 
                     "b_groupPDA.blockcode1.labelit", "b_groupASC.blockcode2.labelit",
                     "b_groupNT.blockcode2.labelit", "b_groupPDA.blockcode2.labelit", 
                     "b_groupASC.blockcode3.labelit", "b_groupNT.blockcode3.labelit",
                     "b_groupPDA.blockcode3.labelit", "b_groupASC.blockcode4.labelit",
                     "b_groupNT.blockcode4.labelit", "b_groupPDA.blockcode4.labelit",
                     "b_groupASC.blockcode1.labeltt", "b_groupNT.blockcode1.labeltt",
                     "b_groupPDA.blockcode1.labeltt", "b_groupASC.blockcode2.labeltt", 
                     "b_groupNT.blockcode2.labeltt", "b_groupPDA.blockcode2.labeltt", 
                     "b_groupASC.blockcode3.labeltt", "b_groupNT.blockcode3.labeltt", 
                     "b_groupPDA.blockcode3.labeltt", "b_groupASC.blockcode4.labeltt", 
                     "b_groupNT.blockcode4.labeltt", "b_groupPDA.blockcode4.labeltt",
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Here, we are primarily interested in the relationship between each trial type across conditions for each group.

The difference in RTs between DT and TT trials in condition 1 is large for each group; parameter estimates sit well below -1 for all groups and it parameter estimates sit around 0 for all groups. This is because there are very few DT trials in the 100% probability conditions (they can only arise as the first trial of the conditions for participants who complete the 100% condition after one of the other probability conditions). In additions, there is a fair amount of overlap between all groups for both trial types, suggesting that, for the 100% condition, each group performed similarly.

For the NT group, the difference between parameter estimates for each TT-DT combination gets smaller as a function of probability conditions. In conditions 2 (84% probability), the it parameter estimates sits just above 0, while the TT parameter estimates sits just above -1. In conditions 3 (67% probability), the it parameter estimates sits just above 0, while the TT parameter estimates sits just below 0. Finally, in conditions 4 (33% probability), both the DT and TT parameter estimates sit above 0, with considerable overlap. This incremental decrease in difference between TT and DT parameter estimates indicates effective learning; learning that in high probability conditions a target is more likely to be preceded by a target. Thus, having seen a target, one should expect another target. Because a distractor trial is uncommon in a high probability condition, seeing an distractor target should be surprising and thus lead to slower RTs for overall (including for TT trials). Thus, RTs are faster for TT trials and slower for DT trials comparatively for the 84% probability condition, then less so for the 67% compared to the 84% probability condition, and finally, in the 33% probability condition, we see similar RTs for both DT and TT trials (i.e., there are more distractor than target trials here; both distractor and target trials are probably followed by a distractor trial).

For the ASC and PDA groups, the difference between parameter estimates for each TT-DT combination remains static across probability conditions. In conditions 2 (84% probability), the DT and TT parameter estimates sit just above 0- this is true for both groups. There is substantial overlap between DT and TT trials and between TT and DT trials between groups- this indicates that there is little difference in estimates for tt and it parameter estimates and that both groups returned comparable RTs for both trial types in this condition. In conditions 3 (67% probability), the DT and TT parameter estimates for both groups sit above 0. Again, there is overlap between groups here, suggesting that ASC and PDA groups performed comparably in this condition. There is no overlap between DT and TT estimates, but the distance between parameter values is small, suggesting again that there is little difference in estimates for TT and DT parameter estimates. Finally, in conditions 4 (33% probability), the DT parameter estimates sit just below 1, while the TT parameter estimates sit between 0 and 1- this is true for both groups. Once again, there is overlap between groups here, suggesting that ASC and PDA groups performed comparably in this condition. There is no overlap between DT and TT estimates, but the distance between parameter values is small, suggesting again that there is little difference in estimates for TT and DT parameter estimates. Taken together, this pattern of responses demonstrate less variation in RTs between DT and TT trials throughout each probability condition. On each trial, only the previous trial is used to predict the current trial outcome (i.e., a distractor trial leads to the prediction of another distractor trial). Thus, in each probability condition, RTs for TT trials are faster than DT trials (because target trials are always assumed to lead to target trials, while distractor trials are always assumed to lead to distractor trials).

Here we are interested in the difference between trial types (TT and DT) in each probability condition for each group. Thus, we compare RT means for each trial type by condition and group:

```{r}
# Define a function to create the data frame
create_df <- function(param1, param2, group, condition) {
  df <- tibble(
    Value = c(mod2_brm_posterior[[param1]], mod2_brm_posterior[[param2]], mod2_brm_posterior[[param1]] - mod2_brm_posterior[[param2]]),
    Parameter = rep(c(param1, param2, "Difference"), each = length(mod2_brm_posterior[[param1]])),
    Group = group,
    Condition = condition
  )
  
  # Add a new column with simplified parameter names
  df <- df %>%
    mutate(SimpleParameter = case_when(
      Parameter == param1 ~ "TT",
      Parameter == param2 ~ "DT",
      Parameter == "Difference" ~ "Difference"
    ))
  
  return(df)
}

# Create the data frames
df1 <- create_df('b_groupNT.blockcode2.labeltt', 'b_groupNT.blockcode2.labelit', "NT", "84%")
df2 <- create_df('b_groupASC.blockcode2.labeltt', 'b_groupASC.blockcode2.labelit', "ASC", "84%")
df3 <- create_df('b_groupPDA.blockcode2.labeltt', 'b_groupPDA.blockcode2.labelit', "PDA", "84%")
df4 <- create_df('b_groupNT.blockcode3.labeltt', 'b_groupNT.blockcode3.labelit', "NT", "67%")
df5 <- create_df('b_groupASC.blockcode3.labeltt', 'b_groupASC.blockcode3.labelit', "ASC", "67%")
df6 <- create_df('b_groupPDA.blockcode3.labeltt', 'b_groupPDA.blockcode3.labelit', "PDA", "67%")
df7 <- create_df('b_groupNT.blockcode4.labeltt', 'b_groupNT.blockcode4.labelit', "NT", "33%")
df8 <- create_df('b_groupASC.blockcode4.labeltt', 'b_groupASC.blockcode4.labelit', "ASC", "33%")
df9 <- create_df('b_groupPDA.blockcode4.labeltt', 'b_groupPDA.blockcode4.labelit', "PDA", "33%")

# Combine the data frames
df <- bind_rows(df1, df2, df3, df4, df5, df6, df7, df8, df9)

# Plot the distributions
ggplot(df, aes(x=Value, fill=SimpleParameter)) +
  geom_histogram(position="identity", alpha=0.5, bins=30) +
  labs(x="Value", y="Count", title="Distributions of parameter estimates showing differences between trial types in each condition by group") +
  theme_minimal() +
  scale_fill_manual(values=c("Difference" = "#336666", "TT" = "#66cccc", "DT" = "#cc99ff"),
                    labels=c("Difference", "TT", "DT"),
                    name="Trial Type") + 
  facet_grid(Group ~ Condition)
```

Visually comparing the differences between trial types in each condition, we can see that for the NT group, the difference between TT and DT gets gradually smaller as probability diminishes; in the 84% probability condition, the difference is around 0.9, for the 67% probability condition, it drops to around 0.5, and for the 33% probability condition, the difference is around approximately 0. This pattern suggests that the NT group demonstrate effective learning; learning that in high probability conditions a target is more likely to be preceded by a target.

Both ASC and PDA groups demonstrate more stable differences between trial type across probability conditions; in all conditions, the difference between TT and DT remains between 0.5 and 0. This suggests that both group performed comparably across all conditions, as expected.

We can also re-plot the posterior predicted RT values:

```{r}
#First, extract predicted values:
#mod2_brm_preds <- data.frame(fitted(mod2_brm))

#As above, to visualise later, save to .csv:
#write.table(mod2_brm_preds, file="mod2_brm_preds_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod2_brm_preds <- read.csv("mod2_brm_preds_24.06.24.csv")

#Add group and blockcode variables to the data frame:
mod2_brm_preds <- mod2_brm_preds  %>%
  mutate(group = mod1_data$group,
         blockcode = mod1_data$blockcode,
         label = mod1_data$label)

#Create plot  showing predicted RTs for each trial type by probability condition and group:
ggplot(mod2_brm_preds, aes(x = label, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("1" = "100%", "2" = "84%", "3" = "67%", "4" = "33%"))) +
  theme_minimal()
```

The plot above depicts posterior predicted RTs for each trial type in each probability condition by group. For the NT group, as specified in our simulated data, the difference between RTs for DT and TT trials diminishes as a function of probability condition; participants are faster in TT compared to DT trials while the probability of a cue indicating a target is high (e.g., 84% probability condition) and diminishing differences in RTs between TT and DT trials when probability of a cue indicating a target is low (e.g., 33% probability condition). As demonstrated by model 1, this model also predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions).

Again, specified in our simulated data, the ASC and PDA groups show similar differences between RTs for DT and TT trials across probability conditions; in each probability condition, RTs for TT trials are faster than DT trials. Again, as demostrated by model 1, this model predicts a pattern of diminishing RTs as a function of probability comparable to the NT group, but the difference between 84% and 67% probability conditions is small. We might conclude that because our model predictions reflect our simulated data, our model is functioning as intended.

### EDA-QA predicted by II

Plot parameter distributions with means and credible intervals for model 3:

```{r}
#Extract posterior samples:
#mod3_brm_posterior <- posterior_samples(mod3_brm)

#To visualise later, save to .csv:
#write.table(mod3_brm_posterior, file="mod3_brm_posterior_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_posterior <- read.csv("mod3_brm_posterior_24.06.24.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod3_brm_posterior,
           pars =  c("b_Intercept", "b_II", 
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

The intercept represents the expected value of EDA-QA scores when II is zero. II is the estimated effect of II on EDA-QA scores, It is the expected change in EDA-QA scores for a one-unit increase in II, assuming all other predictors are held constant. Specifically, for each one-unit increase in II, we expect EDA-QA scores to increase by approx. 1.32, assuming all other variables are held constant.

All of the above can be observed by plotting the marginal posterior distributions against the simulated data:

```{r}
#First, extract predicted values:
#mod3_brm_preds <- data.frame(fitted(mod3_brm))

#As above, to visualise later, save to .csv:
#write.table(mod3_brm_preds, file="mod3_brm_preds_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_preds <- read.csv("mod3_brm_preds_24.06.24.csv")

#Add group and blockcode variables to the data frame:
mod3_brm_preds <- mod3_brm_preds %>%
  mutate( II = mod3_d$II,
  EDAQ = mod3_d$EDAQ,
  neurotype = mod3_d$neurotype
)

#Create the plot:
ggplot(mod3_brm_preds, aes(x = II, y = EDAQ)) +
  geom_point(color = "#339999") +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate), color = "#339999") +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, fill = "#339999") +
  labs(x = "II (std)", y = "EDA-QA scores (std)") +
  ggtitle("Simulated data demonstrating the predicted relationship between II and EDA-QA scores") +
  theme_minimal()
```

Model 3 suggests a strong positive trend; larger II predict higher EAD-QA scores. However, this model does not account for any between neurotype differences in II or EDA-QA scores. To rectify this omission, let us turn to model 4.

Now, plot parameter distributions with means and credible intervals for model 4:

```{r}
#Extract posterior samples:
#mod4_brm_posterior <- posterior_samples(mod4_brm)

#To visualise later, save to .csv:
#write.table(mod4_brm_posterior, file="mod4_brm_posterior_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_posterior <- read.csv("mod4_brm_posterior_24.06.24.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod4_brm_posterior,
           pars =  c("b_Intercept", "b_neurotypeNT", 
                     "b_II", "b_neurotypeNT.II",
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

The intercept represents the expected value of EDA-QA scores when all predictors are zero. In this case, it's the expected value of EDA-QA scores for ASC when II is zero. II is the expected change in EDA-QA scores for a one-unit increase in II, assuming neurotype is at its reference level (ASC). In other words, if II increases by one unit, EDA-QA scores is expected to increase by 0.52 units, given that neurotype is ASC. neurotypeNT represents the expected difference in EDA-QA scores between the ASC and NT individuals, assuming II is zero. Specifically, EDA-QA scores is expected to be 1.00 units lower for NT compared to ASC when II is zero. Finally, neurotypeNT.IIrepresents the interaction term between II and neurotypeNT. This is the expected change in the relationship between II and EDA-QA scores between ASC and NT. Specifically, the effect of II on EDA-QA scores is expected to decrease by 0.19 units for NT compared to ASC.

All of the above can be observed by plotting the marginal posterior distributions against the simulated data:

```{r}
#First, extract predicted values:
#mod4_brm_preds <- data.frame(fitted(mod4_brm))

#As above, to visualise later, save to .csv:
#write.table(mod4_brm_preds, file="mod4_brm_preds_24.06.24.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_preds <- read.csv("mod4_brm_preds_24.06.24.csv")

#Add group and blockcode variables to the data frame:
mod4_brm_preds <- mod4_brm_preds %>%
  mutate( II = mod3_d$II,
  EDAQ = mod3_d$EDAQ,
  neurotype = mod3_d$neurotype
)

#Create the plot:
ggplot(mod4_brm_preds, aes(x = II, y = EDAQ, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate, fill = neurotype)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = neurotype), alpha = 0.3, color = NA) +
  labs(x = "II (std)", y = "EDA-QA scores (std)", color = "Neurotype", fill = "Neurotype") +
  ggtitle("Simulated data demonstrating the predicted relationship between II and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  scale_fill_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  theme_minimal()
```

Both groups demonstrate a positive trend; larger II values predict higher EAD-QA scores. Here, we have chosen to specify NT and ASC separately, this is to capture any differences between neurotypes. EDA-QA scores are higher in ASC, the model accurately captures our simulated assumption that PDA is more prevalent in the autistic population. Additionally, the model accurately captures our assumption that average II would be smaller in NT. ASC have average II values that sit around around 0; in line with Reisli et al. (2023), larger II values are indicative of insensitivity to changing probabilistic contingencies. Overall, the strength of association between II and EDA-QA scores is comparable in both neurotypes.

# Sensitivity analysis

"A sensitivity analysis explores how changes in assumptions influence inference. If none of the alternative assumptions you consider have much impact on inference, that's worth reporting. Likewise, if the alternatives you consider do have an important impact on inference, that's also worth reporting. The same sort of advice follows for other modeling assumptions: likelihoods, linear models, priors, and even how the model is fit to data" - McElreath (2020).

### RTs predicted by group × probability condition

First, read in the data:

```{r}
#Read in saved data file:
mod1_data <- read.csv("Data_Mod1_24.06.24.csv") %>%
  mutate(RTs_std = standardize(latency),
         group = factor(group),
         label = factor(label),
         blockcode = factor(blockcode))
```

We will begin with three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod1_brm_default <- brm(RTs_std ~ 0 + group:blockcode,
            data = mod1_data,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
saveRDS(mod1_brm_default, file = "mod1_brm_default_output.rds")

#Fit the model with narrow priors:
mod1_brm_narrow <- brm(RTs_std ~ 0 + group:blockcode,
            data = mod1_data,
            backend = "cmdstan",
            prior = prior(normal(0, 0.2), class = "b"))

#Save the model output to a file:
saveRDS(mod1_brm_narrow, file = "mod1_brm_narrow_output.rds")

#Fit the model with broad priors:
mod1_brm_broad <- brm(RTs_std ~ 0 + group:blockcode,
            data = mod1_data,
            backend = "cmdstan",
            prior = prior(normal(0, 2), class = "b"))

#Save the model output to a file:
saveRDS(mod1_brm_broad, file = "mod1_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod1_brm_default <- readRDS("mod1_brm_default_output.rds")
mod1_brm_narrow <- readRDS("mod1_brm_narrow_output.rds")
mod1_brm_broad <- readRDS("mod1_brm_broad_output.rds")

#Return model output:
models <- list(mod1_brm_default, mod1_brm_narrow, mod1_brm_broad)

#Iterate summary over the models:
for(i in 1:length(models)) {
  # Print model summary
  print(summary(models[[i]]))
}
```

```{r}
#Visualise model output:

# Put models in a list
models <- list(Default = mod1_brm_default, Narrow = mod1_brm_narrow, Broad = mod1_brm_broad)

# Create a data frame to hold coefficients
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

# Extract coefficients and add to data frame
for (model_name in names(models)) {
  coefs <- fixef(models[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

# Order coefficients by the order they appear in the first model, then reverse the order
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(models[[1]]))))

# Plot using ggplot
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()

```

All three models show comparable estimates for each combination of group and condition (blockcode); we can conclude that our default priors are acceptable to capture our expected range of parameter estimates.

### RTs predicted by group × probability condition × trial type

Repeat for model 2. First, read in the data:

```{r}
#Read in saved data file:
mod2_data <- read.csv("Data_Mod1_14.06.24.csv") %>%
  mutate(RTs_std = standardize(latency),
         group = factor(group),
         label = factor(label),
         blockcode = factor(blockcode))
```

We will begin with three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod2_brm_default <- brm(RTs_std ~ 0 + group:blockcode:label,
            data = modttit_data,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_default, file = "mod2_brm_default_output.rds")

#Fit the model with narrow priors:
mod2_brm_narrow <- brm(RTs_std ~ 0 + group:blockcode:label,
            data = modttit_data,
            backend = "cmdstan",
            prior = prior(normal(0, 0.2), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_narrow, file = "mod2_brm_narrow_output.rds")

#Fit the model with broad priors:
mod2_brm_broad <- brm(RTs_std ~ 0 + group:blockcode:label,
            data = modttit_data,
            backend = "cmdstan",
            prior = prior(normal(0, 2), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_broad, file = "mod2_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod2_brm_default <- readRDS("mod2_brm_default_output.rds")
mod2_brm_narrow <- readRDS("mod2_brm_narrow_output.rds")
mod2_brm_broad <- readRDS("mod2_brm_broad_output.rds")

#Return model output:
model2s <- list(mod2_brm_default, mod2_brm_narrow, mod2_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model2s)) {
  # Print model summary
  print(summary(model2s[[i]]))
}
```

```{r}
#Visualise model output:

# Put models in a list
model2s <- list(Default = mod2_brm_default, Narrow = mod2_brm_narrow, Broad = mod2_brm_broad)

# Create a data frame to hold coefficients
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

# Extract coefficients and add to data frame
for (model_name in names(model2s)) {
  coefs <- fixef(model2s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

# Order coefficients by the order they appear in the first model, then reverse the order
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model2s[[1]]))))

# Plot using ggplot
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()

```

All three models show comparable estimates for each combination of group, condition (blockcode), and trial type (label); we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.

### EDA-QA predicted by II

Repeat for model 3 and 4. First, read in the data:

```{r}
#Read in the simulated data:
mod3_d <- read.csv("Data_Mod3_24.06.24.csv") 
```

We will begin with model 3, constructing three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod3_brm_default <- brm(EDAQ ~ II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)
#Save the model output to a file:
saveRDS(mod3_brm_default, file = "mod3_brm_default_output.rds")

#Fit the model with narrow priors:
mod3_brm_narrow <- brm(EDAQ ~ II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 0.2), class = "Intercept"),
      prior(normal(0, 0.2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod3_brm_narrow, file = "mod3_brm_narrow_output.rds")

#Fit the model with broad priors:
mod3_brm_broad <- brm(EDAQ ~ II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 2), class = "Intercept"),
      prior(normal(0, 2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod3_brm_broad, file = "mod3_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod3_brm_default <- readRDS("mod3_brm_default_output.rds")
mod3_brm_narrow <- readRDS("mod3_brm_narrow_output.rds")
mod3_brm_broad <- readRDS("mod3_brm_broad_output.rds")

#Return model output:
model3s <- list(mod3_brm_default, mod3_brm_narrow, mod3_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model3s)) {
  # Print model summary
  print(summary(model3s[[i]]))
}
```

```{r}
#Visualise model output:
# Put models in a list
model3s <- list(Default = mod3_brm_default, Narrow = mod3_brm_narrow, Broad = mod3_brm_broad)

# Create a data frame to hold coefficients
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

# Extract coefficients and add to data frame
for (model_name in names(model3s)) {
  coefs <- fixef(model3s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

# Order coefficients by the order they appear in the first model, then reverse the order
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model3s[[1]]))))

# Plot using ggplot
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

Models with broad (normal(0,2)) and default (normal(0,1)) priors show comparable parameter estimates, while narrow (normal(0,0.2)) generates estimates closer to 0 - these priors are overly confident that observed values should sit closer to 0. The limited knowledge available regarding the relationship between PDA and sensitivity to changes in probability lead us to choose priors that would be accepting of a broad range of estimates; we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.

Now repeat the process for model 4 by constructing three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod4_brm_default <- brm(EDAQ ~ neurotype * II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)
#Save the model output to a file:
saveRDS(mod4_brm_default, file = "mod4_brm_default_output.rds")

#Fit the model with narrow priors:
mod4_brm_narrow <- brm(EDAQ ~ neurotype * II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 0.2), class = "Intercept"),
      prior(normal(0, 0.2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod4_brm_narrow, file = "mod4_brm_narrow_output.rds")

#Fit the model with broad priors:
mod4_brm_broad <- brm(EDAQ ~ neurotype * II,
    data = mod3_d,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 2), class = "Intercept"),
      prior(normal(0, 2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
saveRDS(mod4_brm_broad, file = "mod4_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod4_brm_default <- readRDS("mod4_brm_default_output.rds")
mod4_brm_narrow <- readRDS("mod4_brm_narrow_output.rds")
mod4_brm_broad <- readRDS("mod4_brm_broad_output.rds")

#Return model output:
model4s <- list(mod4_brm_default, mod4_brm_narrow, mod4_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model4s)) {
  # Print model summary
  print(summary(model4s[[i]]))
}
```

```{r}
#Visualise model output:
# Put models in a list
model4s <- list(Default = mod4_brm_default, Narrow = mod4_brm_narrow, Broad = mod4_brm_broad)

# Create a data frame to hold coefficients
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

# Extract coefficients and add to data frame
for (model_name in names(model3s)) {
  coefs <- fixef(model4s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

# Order coefficients by the order they appear in the first model, then reverse the order
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model4s[[1]]))))

# Plot using ggplot
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

As with the output above from model 3, models with broad (normal(0,2)) and default (normal(0,1)) priors show comparable parameter estimates, while narrow (normal(0,0.2)) generates estimates closer to 0 - these priors are overly confident that observed values should sit closer to 0. The limited knowledge available regarding the relationship between PDA and sensitivity to changes in probability lead us to choose priors that would be accepting of a broad range of estimates; again, we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.
