---
title: "BARG analysis_probabilistic learning_23.07.24"
format: html
editor: visual
---

# Package installation

```{r}
#Install packages and libraries:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot",'HDInterval', 'ggthemes')

#Install packages if not already installed:
packages_to_install <- packages[!packages %in% installed.packages()]
if(length(packages_to_install)) install.packages(packages_to_install, dependencies = TRUE)

#If not already installed, install rethinking() separately:
#install.packages("rethinking", 
#                 repos=c(cran="https://cloud.r-project.org",
#                         rethinking="http://xcelab.net/R"))

#Rstan might need a bit of extra attention. If it doesn't install with the above code, remove any existing RStan via:
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")

#Set up compiler flags:
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXX17FLAGS=-O3 -march=native -mtune=native -fPIC",
    "CXX17=g++", # or clang++ but you may need a version postfix
    file = M, sep = "\n", append = TRUE)

#This code is for the development version of rstan- I've been told that this might function better than the up-to-date version:
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

#To verify your installation, you can run the RStan example/test model:
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

```{r}
#Once installed, load libraries using groundhog():
library(groundhog)

#Specify packages:
packages <- c("groundhog", "tidyr", "INLA", "tidyverse", "plotrix", 
              "rstatix", "gridExtra", "tidybayes", "modelsummary", 
              "rstatix", "brms", "coda", "mvtnorm", "devtools", "dagitty", "StanHeaders", 
              "rstan", "V8", "bayesplot",'HDInterval', 'ggthemes')

#Load packages:
groundhog.library(packages, "2024-06-28", tolerate.R.version='4.4.0')
```

```{r}
#If that doesn't work, load packages manually:
library(rstan)
library(cmdstanr)
library(devtools)
library(rethinking) #Add download above
library(V8)
library(brms)
library(tidyverse) 
library(plotrix)
library(gridExtra)
library(tidybayes)
library(modelsummary)
library(bayesplot)
library(HDInterval)
library(ggthemes)
```

# Preamble

## Why Bayesian?

Global autism prevalence is estimated at approx. 1 in 100 (Zeidan et al., 2022). Little is known about Pathological Demand Avoidance (PDA) and its prevalence. However, it seems reasonable to assume that if PDA represents a proportion of the autistic population, prevalence of PDA is likely \<1 in 100. Additionally, the percentage of PDA and/or autistic individuals willing and able to participate in research is likely smaller still. Compared to frequentist methods that require large samples to produce precise, well-powered results, Bayesian approaches rely on estimates that have a clear and valid interpretation, no matter the sample size- though it is important to note that Bayesian estimates are dependent upon the initial plausibilities assigned to them (i.e., priors), which is especially true for estimates derived from small sample sizes. In this regard, Bayesian modelling offers us a powerful tool with which to better understand individuals from underrepresented groups, providing the priors we fit to our model are well justified- this will be explained and exemplified in our prior predictive checks and sensitivity analysis.

Furthermore, the autistic population is renowned for its heterogeneity (for an overview of heterogeneity in autism, see Masi et al., 2017), which often produces noisy data that is difficult to interpret. Though data generated by PDA individuals (not third-party report) is limited, its potential relationship to autism means that PDA data may be noisy, too. Again, Bayesian analyses allow us to incorporate prior knowledge about the parameters in the form of priors. This can be particularly useful when dealing with noisy data, as prior information can help regularize the estimates and improve model stability. In addition, Bayesian methods provide a way to quantify uncertainty in parameter estimates through posterior distributions. This is especially important when dealing with noisy data, as it allows us to assess the reliability of estimates and make more informed decisions.

## Goals of analysis

We are interested in how PDA, autism, and neurotypicallity learn about probabilistic contingencies. We utilised a probabilistic learning task in which a cue (a shape) was indicative of a target (same shape rotated 90°) 100%, 84%, 67%, and 33% of the time. Participants completed four randomly presented blocks, one for each probability condition; 100%, 84%, 67% and 33%. This study had two primary aims:

1\) Gain insight into whether PDA and autistic individuals differ from neurotypical individuals in how they learn about probabilistic associations. Specifically, we wanted to know if performance of PDA and autistic individuals differ less between similar probability contexts (e.g., 84% and 67% conditions) compared to neurotypical individuals. We also wanted to know if PDA and autistic individuals perform similarly across various probability conditions (i.e., if PDA and autistic individuals produce similar RTs for each condition) – this would help to clarify if/how PDA and autism are related.

2\) Examine the relationship between sensitivity to changing probabilistic associations and PDA behaviours. We wanted to know if an insensitivity to changing probabilistic associations is related to PDA behaviours beyond the influence of an autism diagnosis – potentially offering a broader understanding of how uncertainty relates to PDA.

## Causal model

Below is a directed acyclical graphic (DAG) representation of our causal model. "SensoryProcessing" refers to sensory processing differences thought to underpin perception, cognition, and behaviour in autism (Palmer et al., 2017; Van de Cruys et al., 2014); probabilistic learning is thought to represent one facet of these sensory processing differences, and the facet we attempt to probe in this study. "Autism" here refers to a binary category- possessing a formal, clinical diagnosis of autism, or not. Finally, "PDA" refers to a spectrum of behavioural characteristics thought to be associated with PDA.

```{r}
dag_m <- dagitty( "dag {
    Autism -> EDAQ
    SensoryProcessing -> EDAQ
    SensoryProcessing -> Autism
}")
coordinates( dag_m ) <- list( x=c(Autism=0, EDAQ=1, SensoryProcessing=-1) , y=c(Autism=-0.5, EDAQ=1, SensoryProcessing=1) )
drawdag(dag_m)
```

Previous research suggests that aberrant probability learning is associated with autism (Lawson et al., 2017; Reisli et al., 2023). This compliments predictive processing theories that suggest aberrant handling of sensory information relative to prior knowledge might underpin autistic development (for review, see Cannon et al., 2021 and Chrysaitis & Series, 2023). Thus, a latent measure of probabilistic learning should causally influence an autism diagnosis. The relationship between PDA and autism is contentious- some view PDA as an autism subgroup (Christie, 2007), others posit that PDA is not different from autism (Milton, 2013; Moore, 2020), while others argue that PDA is a common mental health condition prevalent in the general population (Woods, 2018). Given that autism might underpin the development of PDA behaviours, we include a causal link between autism and PDA. The relationship between probabilistic learning (under the umbrella of sensory processing differences, labelled in the DAG above as SensoryProcessing) and PDA has yet to be empirically questioned- this relationship represents our primary interest.

# Data preparation

## Load data

Load and inspect structure of summary and raw data frames:

```{r}
#Load in both summary and raw data frames:
df_BM <- read.csv("BlockMedians.csv") %>%
  mutate(II_std = standardize(II),
         EDAQ_std = standardize(EDAQ))
df_RT <- read.csv("RT_Data.csv") %>%
  mutate(RTs_std = standardize(latency),
         group = factor(group),
         blockcode = factor(blockcode))
df_TT <- read.csv("TrialTypeData.csv") %>%
  mutate(RTs_std = standardize(latency),
         group = factor(group),
         blockcode = factor(blockcode),
         type = factor(type))

#Check simulated data structure:
#str(df_BM)
#str(df_RT)
#str(df_TT)
```

Check demographic information:

```{r}
#Check demographics by neurotype:
df_BM %>%
  dplyr::group_by(diagnosis) %>%
  dplyr::summarize(
    mean_EDAQ = mean(EDAQ, na.rm = TRUE),
    sd_EDAQ = sd(EDAQ, na.rm = TRUE),
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    male = sum(gender == "Male", na.rm = TRUE),
    female = sum(gender == "Female", na.rm = TRUE),
    other = sum(gender == "Other", na.rm = TRUE)
  )

#And by group:
df_BM %>%
  dplyr::group_by(group) %>%
  dplyr::summarize(
    mean_EDAQ = mean(EDAQ, na.rm = TRUE),
    sd_EDAQ = sd(EDAQ, na.rm = TRUE),
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    male = sum(gender == "Male", na.rm = TRUE),
    female = sum(gender == "Female", na.rm = TRUE),
    other = sum(gender == "Other", na.rm = TRUE)
  )
```

## Visualisation

### RTs predicted by group × probability condition

We plot the expected relationship between condition and group:

```{r}
#Plot raw data:
ggplot(df_RT, aes(group, latency, fill = blockcode)) + 
  geom_boxplot() +
  labs(x = "Group", y = "RTs", fill = "Condition") +
  ggtitle("RTs plotted for each group by probability condition") +
  scale_fill_manual(values = c("block1" = "#66cccc", "block2" = "#336666", "block3" = "#cc99ff", "block4" = "#336999"),
                     labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  scale_x_discrete(labels = c("1" = "NT", "2" = "ASC", "3" = "PDA")) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
mod1_data_means <- df_RT %>%
  group_by(group, blockcode) %>%
  summarise(
    mean_RT = mean(latency, na.rm = TRUE),
    sd_RT = sd(latency, na.rm = TRUE)
  )

ggplot(mod1_data_means, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Condition", y = "RTs", color = "Group") +
  ggtitle("RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  theme_minimal()
```

As a reminder, Reisli et al., (2023) found that CNV amplitudes for 84%, 67%, and 33% probability conditions were reduced in their autistic cohort compared to their neurotypical cohort. The autistic group also showed larger CNV amplitude differences between 100% and 80% probability conditions. RT data showed overall faster RTs in response to greater probability in both autistic and neurotypical groups, mirroring Lawson et al. (2017) who also found a similar trend in both groups' RTs. Additionally, RTs demonstrated a pattern similar to CNV amplitudes; for the neurotypical group, RTs scaled with probability condition (i.e., quicker for 100%, slower for 33%), for the autistic group, RTs did not differ between 84% and 67% conditions. Reisli et al. (2023) conclude that this pattern of diminished differentiation between probability conditions in autism is indicative of a binary classification of unceratinty. Here, instead of inferring the degree to which an environment is uncertain based upon probabilistic information, autistic individuals are thought to categorise an environment as either certain or not.

Based on Reisli et al., (2023), we predicted that the neurotypical group (NT) would demonstrate incrementally slower RTs as a function of condition. That is, participants would perform faster when probability was high than when probability was low. Given that PDA is categorically associated with autism (either as a subgroup; Christie, 2007; or a form of autistic self advocacy; Milton, 2013; Moore, 2020) we anticipated a pattern of RTs conforming to a binary classification of volatility for both the autistic (ASC) and PDA groups; RTs that do not differ between 84% and 67% conditions. Visual inspection of the data suggests that the ASC group show a pattern of RT responses to probability contingencies comparable to that demonstrated by Reisli et al. (2023). Specifically, the ASC group seemingly show little difference in RTs for 67% and 84% probability conditions. However, in contrast to our prediction that the PDA group would show a pattern of RTs comparable to the ASC group, the PDA group demonstrate an evenly spaced, incremental decrease in RTs as a function of probability. This pattern echoes the NT group's performance. Importantly, this inference is made based solely on visual inspection of the data; analyse of the data will offer great insight.

### RTs predicted by group × probability condition × trial type

We also plot the expected relationship between group, condition, and trial type:

```{r}
#We should also derive means for each tiral type:
mod2_data_means <- df_TT %>%
  group_by(group, blockcode, type) %>%
  summarise(
    mean_RT = mean(latency, na.rm = TRUE),
    sd_RT = sd(latency, na.rm = TRUE)
  )

#Plot:
ggplot(mod2_data_means, aes(x = type, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%"))) +
  theme_minimal()
```

Again, it is argued that a pattern of incrementally longer RTs as a function of reduced probability (i.e., short RTs for high probability conditions, and long RTs for low probability conditions) represents appropriate volatility estimation. However, this pattern could emerge via two mechanisms: 1) where participants learn probabilistic contingencies to form expectations about cue-outcome associations (i.e., estimate certainty that a cue will predict an outcome), RTs should be shorter when probability is high (i.e., more certain), and longer when probability is low (i.e., less certain), or 2) as an artifact of task design. In the latter case, the proportion of target and distractor trials within each condition may influence median RTs.

Assuming appropriate learning of probabilistic contingencies, participants should form expectations regarding cue-target probabilities. For example, in the 84% condition, a target should be expected on approximately 84% of trials. Deviations from expectations (e.g., the occurrence of a distractor trial in the 84% condition) should slow responses. As such, participants who are sensitive to changing probabilistic contingencies are expected to respond faster to target trials when they are frequent (e.g., high-probability conditions) compared to when they occur less frequently (e.g., low-probability conditions). Conversely, when distractor trials are rare (e.g., in a high probability conditions), their occurrence should slow participant responses to subsequent target trials more than when they occur frequently (e.g., low probability conditions).

In contrast, relying on the previous trial (i.e., *n* - 1) to inform expectations of the current trial outcome might represent difficulties learning probabilistic contingencies – which theoretically could be indicative of individuals who treat probabilistic outcomes as wholly uncertain. In this instance, targets preceded by targets (TT) would be responded to faster than targets preceded by distractors (DT). This would lead to a pattern of median RTs differ between TT and DT trials within each probability condition, but not between probability condition – indicating that probability contingencies had not been learnt.

However, target and distractor trials are unevenly distributed in each probability condition (i.e., conditions in order of most target and fewest distractor trials: 100%, 84%, 67%, then 33% – note that 100% contains no distractor trials). As such, median RTs might be unduly influenced by the proportion of target and distractor trials within each condition. For example, conditions with more target/TT trials might have shorter median RTs compared to conditions with fewer target trials. Conversely, conditions with more distractor/DT trials might have longer median RTs compared to conditions with fewer distractor trials. This could artificially create a pattern of incrementally slower RTs as a function of reduced probability.

### EDA-QA predicted by II

Plot data:

```{r}
#Plot:
ggplot(data = df_BM, aes(x = II_std, y = EDAQ_std, color = diagnosis)) +
  geom_point() +
  labs(x = "II (std)", y = "EDA-QA scores (std)", color = "Neurotype") +
  ggtitle("Relationship between II and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  theme_minimal()
```

Again, II simply refers to slopes calculated for median RTs across 84%, 67%, and 33% probability conditions for each participant, providing a measure of sensitivity to changing probabilistic contingencies. As such, larger II values indicate insensitivity to changing probabilistic contingencies, while smaller II values suggest sensitivity to changing probabilistic contingencies. We predicted that for both neurotypical and autistic data, II would predict scores of a descriptive measure of PDA behaviour (EDA-QA). Though an overall positive trend for both neurotypical and autistic data was anticipated, neurotypical and autistic data were specified separately. This was to capture any potential difference between neurotypes. For example, because PDA is thought to have greater prevalence in the autistic population, it was expected that autistic data would include higher EDA-QA scores. It was expected neurotypical individuals would on average generate smaller II values compared to autistic individuals, indicating greater sensitivity to changing probabilistic contingencies. Autistic individuals were expected to have II values around 0, in line with Reisli et al. (2023), where 0 is indicative of insensitivity to changing probabilistic contingencies. Potential differences in how sensitivity to changing probabilistic contingencies relates to PDA in neurotypical and autistic individuals were of particular interest. For example, it might be that the relationship is more pronounced for the autistic group, due to insensitivity to changing probabilistic contingencies and more instances of PDA behaviours. By considering both neurotypical and autistic data separately, these potential variations could be captured.

Upon first glance of our plotted data, it seems that, as expected, ASC on average demonstrate higher total EDA-QA scores compared to NT. This is in keeping with the notion that PDA is categorically linked to ASC, with higher prevalence of PDA in the autistic population. In contrast to our prediction that ASC would on average demonstrate larger II values compared to NT, it seems that ASC and NT performed similarly across 84%, 67%, and 33% conditions (despite the ASC group seemingly demonstrating less change in RTs between 84% and 67% conditions- see model 1 plot above). Once again, this inference is derived from visual inspection of the data; analysis of the data will provide greater insight into the relationship between II and EDA-QA scores.

# Model checking

For those unfamiliar with brms() (specifically with brms() syntax), chapter 12 of the following webpage provides some useful guidance on how to specify a brms() model: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

### RTs predicted by group × probability condition

To better understand if the reaction times (RTs) of PDA and autistic individuals differ less for adjacent conditions (84%, 67%, and 33% conditions) compared to neurotypical individuals, and if PDA and autistic individuals preform similarly (i.e., PDA and autistic individuals perform similarly across 100%, 84%, 67%, and 33% conditions), we run a Bayesian regression model where participant responses (RTs) to probabilistic learning task are predicted by a group, condition interaction:

$$
RT_i ∼ Normal( μ_i, σ ) \\
μ_i = α_{group, condition​}  \\
α_{group, condition​} ∼ ​Normal(0, 1) \\
σ ∼ Exponential(1)
$$Our choice of priors was justified by simulation and model testing- for more information, see BARG simulation.qmd.

Now fit the model:

```{r}
#Fit the model:
mod1_brm <- brm(RTs_std ~ 0 + group:blockcode,
            data = df_RT,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(mod1_brm, file = "mod1_brm_output.rds")

#Load the model output from a file:
mod1_brm <- readRDS("mod1_brm_output.rds")

#Return model output:
summary(mod1_brm)
```

Plot parameter distributions with means and credible intervals:

```{r}
#Extract posterior samples:
#mod1_brm_posterior <- posterior_samples(mod1_brm)

#To visualise later, save to .csv:
#write.table(mod1_brm_posterior, file="mod1_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod1_brm_posterior <- read.csv("mod1_brm_posterior.csv")

plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod1_brm_posterior,
           pars =  c("b_groupASC.blockcodeblock1", "b_groupNT.blockcodeblock1", "b_groupPDA.blockcodeblock1", "b_groupASC.blockcodeblock2", "b_groupNT.blockcodeblock2", "b_groupPDA.blockcodeblock2", "b_groupASC.blockcodeblock3", "b_groupNT.blockcodeblock3", "b_groupPDA.blockcodeblock3", "b_groupASC.blockcodeblock4", "b_groupNT.blockcodeblock4", "b_groupPDA.blockcodeblock4",
                     "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")

```

Finally, we can visualise our model-predicted RT values for each group by condition:

```{r}
#First, extract predicted values:
#mod1_brm_preds <- data.frame(fitted(mod1_brm))

#As above, to visualise later, save to .csv:
#write.table(mod1_brm_preds, file="mod1_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod1_brm_preds <- read.csv("mod1_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod1_brm_preds <- mod1_brm_preds  %>%
  mutate(group = df_RT$group,
         blockcode = df_RT$blockcode)

#Create plot, the first showing predicted RTs for each group by probability condition, the second showing predicted RTs for each probability condition by group:
ggplot(mod1_brm_preds, aes(x = group, y = Estimate, color = blockcode, group = blockcode)) +
  geom_point() +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  labs(x = "Group", y = "RTs", color = "Condition") +
  ggtitle("Predicted RTs for each group by probability condition") +
  scale_color_manual(values = c("block1" = "#66cccc", "block2" = "#336666", "block3" = "#cc99ff", "block4" = "#336999"),
                     labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  scale_x_discrete(labels = c("1" = "NT", "2" = "ASC", "3" = "PDA")) +
  theme_minimal()

ggplot(mod1_brm_preds, aes(x = blockcode, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  labs(x = "Condition", y = "RTs", color = "Group") +
  ggtitle("Predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  theme_minimal()
```

The plots above depict predicted RTs in two different ways; the first shows predicted RTs for each group by probability condition, the second shows predicted RTs for each probability condition by group. For the NT and PDA groups, our model predicts a pattern of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions). While the ASC group show a similar pattern of diminishing RTs as a function of probability, the difference between 84% and 67% probability conditions is smaller comparatively. This is in line with Reisli et al. (2023) who found that autistic individuals generated similar RTs for 84% and 67% probability conditions in a similar task. Here, we replicate Reisli et al.'s original finding that autistic individuals show less difference between probability conditions (specifically, 84% and 67%). Unexpectedly, despite the categorical link between autism and PDA, we find that the PDA group demonstrate a pattern of RTs similar to that of the NT group; diminishing RTs that are distributed in evenly spaced increments (between the 33% an d84% probability conditions) as a function of probability.

### RTs predicted by group × probability condition × trial type

To further investigate how PDA and autistic individuals learn about probbailistic contingecies compared to neurotypical individuals, we also run a Bayesian regression model where participant reponses (RTs) to probabilistic learning task are predicted by a group, condition, label interaction:

$$
RT_i ∼ Normal( μ_i, σ ) \\
μ_i = α_{group, condition​, trial type}  \\
α_{group, condition, trial type​} ∼ ​Normal(0, 1) \\
σ ∼ Exponential(1)
$$

Our choice of priors was justified by simulation and model testing- for more information, see BARG simulation.qmd.

Now fit the model:

```{r}
#Fit the model:
mod2_brm <- brm(RTs_std ~ 0 + group:blockcode:type,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm, file = "mod2_brm_output.rds")

#Load the model output from a file:
mod2_brm <- readRDS("mod2_brm_output.rds")

#Return model output:
summary(mod2_brm)
```

Plot parameter distributions with means and credible intervals:

```{r}
#Extract posterior samples:
#mod2_brm_posterior <- posterior_samples(mod2_brm)

#To visualise later, save to .csv:
#write.table(mod2_brm_posterior, file="mod2_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod2_brm_posterior <- read.csv("mod2_brm_posterior.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod2_brm_posterior,
           pars =  c("b_groupASC.blockcodeblock2.typedt", "b_groupNT.blockcodeblock2.typedt", "b_groupPDA.blockcodeblock2.typedt", "b_groupASC.blockcodeblock3.typedt", "b_groupNT.blockcodeblock3.typedt", "b_groupPDA.blockcodeblock3.typedt", "b_groupASC.blockcodeblock4.typedt", "b_groupNT.blockcodeblock4.typedt", "b_groupPDA.blockcodeblock4.typedt", "b_groupASC.blockcodeblock2.typett", "b_groupNT.blockcodeblock2.typett", "b_groupPDA.blockcodeblock2.typett", "b_groupASC.blockcodeblock3.typett", "b_groupNT.blockcodeblock3.typett", "b_groupPDA.blockcodeblock3.typett", "b_groupASC.blockcodeblock4.typett", "b_groupNT.blockcodeblock4.typett", "b_groupPDA.blockcodeblock4.typett", "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Finally, we can visualise our model-predicted RT values for each group by condition:

```{r}
#First, extract predicted values:
#mod2_brm_preds <- data.frame(fitted(mod2_brm))

#As above, to visualise later, save to .csv:
#write.table(mod2_brm_preds, file="mod2_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod2_brm_preds <- read.csv("mod2_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod2_brm_preds <- mod2_brm_preds  %>%
  mutate(group = df_TT$group,
         blockcode = df_TT$blockcode,
         type = df_TT$type)

#Create plot  showing predicted RTs for each trial type by probability condition and group:
ggplot(mod2_brm_preds, aes(x = type, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%"))) +
  theme_minimal()
```

The plot above depicts predicted RTs for each trial type (DT and TT) in each probability condition by group. For the NT group, our model predicts a patterns of diminishing RTs where by the difference between RTs for DT and TT trials is predicted as a function of probability condition; participants are faster in TT compared to DT trials while the probability of a cue indicating a target is high (e.g., 84% probability condition) and diminishing differences in RTs between TT and DT trials when probability of a cue indicating a target is low (e.g., 33% probability condition). The ASC group also demonstrates a pattern of diminishing RTs as a function of probability, however, the model predicts less variation in RTs between DT and TT trials across each probability condition; in each probability condition, RTs for TT trials are faster than DT trials, irrespective of the relative quantity of target and distractor trials in each probability contingency. In contrast to our prediction, the PDA group did not demonstrate a pattern of RTs for TT and DT trials across probability conditions comparable to the ASC group. Instead, the PDA group showed a pattern similar to the NT group; participants were faster in TT compared to DT trials while the probability of a cue indicating a target was high (e.g., 84% probability condition) and diminishing differences in RTs between TT and DT trials when probability of a cue indicating a target was low (e.g., 33% probability condition).

### EDA-QA predicted by II

We are also interested in the relationship between probabilistic learning and PDA behaviours; we run a Bayesian regression where EDAQ scores (a self-report descriptive measure of PDA behaviours) are predicted by II (an experimentally derived measure of sensitivity to changing probabilistic contingencies). $$
EDAQ_i ∼ Normal( μ_i, σ ) \\
μ_i = \alpha + \beta_{II} \\
α ∼ ​Normal( 0 , 1 ) \\
β ∼ Normal(0, 1) \\
σ ∼ Exponential(1)
$$

It is possible that there are differences in how sensitivity to changing probabilistic contingencies relates to PDA in neurotypical and autistic individuals. For example, it might be that the relationship is more pronounced for the autistic group, due to insensitivity to changing probabilistic contingencies and more instances of PDA behaviours. A model was constructed to explore potential effects of autism diagnosis: $$
EDAQ_i ∼ Normal( μ_i, σ ) \\
μ_i = \alpha + \beta_{neurotype_{NT}} + \beta_{II} + \beta_{{neurotype}_{{NT}} \times \ II} \\
α ∼ ​Normal( 0 , 1 ) \\
β ∼ Normal(0, 1) \\
σ ∼ Exponential(1)
$$

Our choice of priors was justified by simulation and model testing- for more information, see BARG simulation.qmd.

Now fit both models:

```{r}
#Fit the model 3:
mod3_brm <- brm(EDAQ_std ~ II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod3_brm, file = "mod3_brm_output.rds")

#Fit the model 4:
mod4_brm <- brm(EDAQ_std ~ neurotype * II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod4_brm, file = "mod4_brm_output.rds")

#Load the model output from file:
mod3_brm <- readRDS("mod3_brm_output.rds")
mod4_brm <- readRDS("mod4_brm_output.rds")

#Return model output:
summary(mod3_brm)
summary(mod4_brm)

#Get priors (if interested):
#get_prior(mod3_brm)
#get_prior(mod4_brm)
```

Plot parameter distributions with means and credible intervals for model 3:

```{r}
#Extract posterior samples:
#mod3_brm_posterior <- posterior_samples(mod3_brm)

#To visualise later, save to .csv:
#write.table(mod3_brm_posterior, file="mod3_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_posterior <- read.csv("mod3_brm_posterior.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod3_brm_posterior,
           pars =  c("b_Intercept", "b_II_std", "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

And again, plot parameter distributions with means and credible intervals for model 4:

```{r}
#Extract posterior samples:
#mod4_brm_posterior <- posterior_samples(mod4_brm)

#To visualise later, save to .csv:
#write.table(mod4_brm_posterior, file="mod4_brm_posterior.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_posterior <- read.csv("mod4_brm_posterior.csv")

#Plot:
plot_title <- ggtitle("Posterior distributions",
                      "with means and 89% compatibility intervals")
color_scheme_set("mix-teal-pink")
mcmc_areas(mod4_brm_posterior,
           pars =  c("b_Intercept", "b_II_std", "b_neurotypeNT", "b_neurotypeNT.II_std", "sigma"), #"lprior" and "lp__" can be included but they're much smaller values and mess up the plot making it hard to see.
           point_est = "mean",
           border_size = 0.1,
           prob = 0.89) + plot_title + xlab("Parameter estimates") + ylab("Parameters")
```

Plot the observed data points and predicted lines:

```{r}
#First, extract predicted values for model 3:
#mod3_brm_preds <- data.frame(fitted(mod3_brm))

#As above, to visualise later, save to .csv:
#write.table(mod3_brm_preds, file="mod3_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod3_brm_preds <- read.csv("mod3_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod3_brm_preds <- mod3_brm_preds %>%
  mutate( II = df_BM$II_std,
  EDAQ = df_BM$EDAQ_std,
  neurotype = df_BM$diagnosis
)

#Create the plot:
ggplot(mod3_brm_preds, aes(x = II, y = EDAQ)) +
  geom_point(color = "#339999") +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate), color = "#339999") +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, fill = "#339999") +
  labs(x = "II (std)", y = "EDA-QA scores (std)") +
  ggtitle("Relationship between II and EDA-QA scores") +
  theme_minimal()

#Repeat for model 4, extract predicted values:
#mod4_brm_preds <- data.frame(fitted(mod4_brm))

#As above, to visualise later, save to .csv:
#write.table(mod4_brm_preds, file="mod4_brm_preds.csv",sep=",",row.names=F)

#Read in .csv to data frame:
mod4_brm_preds <- read.csv("mod4_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod4_brm_preds <- mod4_brm_preds %>%
  mutate( II = df_BM$II_std,
  EDAQ = df_BM$EDAQ_std,
  neurotype = df_BM$diagnosis
)

#Create the plot:
ggplot(mod4_brm_preds, aes(x = II, y = EDAQ, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate, fill = neurotype)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = neurotype), alpha = 0.3, color = NA) +
  labs(x = "II (std)", y = "EDA-QA scores (std)", color = "Group", fill = "Group") +
  ggtitle("Simulated data demonstrating the relationship between II and EDA-QA scores by neurotype") +
  scale_color_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  scale_fill_manual(values = c("NT" = "#cc99ff", "ASC" = "#336999")) +
  theme_minimal()
```

Above are two plots; one depicts II predicting EDA-QA scores (model 3), while the other depicts II predicting EDA-QA scores by neurotype (model 4). Model 3 suggest no relationship between II and EDA-QA scores; without considering neurotype differences, insensitivity to changing probabilistic contingencies is not associated with PDA behaviours. The second plot (model 4) suggested that ASC participants on average have higher EDA-QA scores than NTs (representing greater prevalence of PDA in the autistic population compared to the neurotypical population). Again, neither neurotype's II were predictive of EDA-QA scores, suggesting no relationship between II and EDA-QA scores in either neurotype.

# Prior predictive checks

"A prior predictive check displays simulated data that are generated from parameter values in the prior distribution. The simulated data from the mathematically specified prior should show trends that match the trends assumed by prior knowledge" (Kruschke, 2021)

We want to know that our choice of priors accurately represents our assumptions about our parameters. To do this, we can simulate priors from our models and relate them to observed values.

### RTs predicted by group × probability condition

Here, we model RTs by group and probability condition (model 1):

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between RTs and group, blockcode:
mod1_brm_prior <- brm(RTs_std ~ 0 + group:blockcode,
                      data = df_RT,
                      backend = "cmdstan",
                      prior = prior(normal(0, 1), class = "b"),
                      sample_prior = "only")

#Save the model output to a file:
#saveRDS(mod1_brm_prior, file = "mod1_brm_prior.rds")
```

We can inspect the output here:

```{r}
#Load the model output from a file:
mod1_brm_prior <- readRDS("mod1_brm_prior.rds")

#Return model output:
summary(mod1_brm_prior)
```

Draw samples from the prior predictive simulation above:

```{r}
#Draw prior samples:
mod1_brm_prior_PPC <- add_linpred_draws(
  mod1_brm_prior, 
  newdata = tibble(group = df_RT$group, blockcode = df_RT$blockcode),
  ndraws = 100,
  value = 'Predicted_RT'
) |> 
  ungroup() |> 
  select(Predicted_RT, .draw, group, blockcode) |> 
  group_by(group, blockcode) |> 
  summarise(
    mean_RT = mean(Predicted_RT),
    sd_RT = sd(Predicted_RT),
    se_RT = sd_RT / sqrt(n()),
    .groups = "drop"
  )

#Print:
mod1_brm_prior_PPC
```

Plot a prior predictive simulation:

```{r}
#Plot prior predictive simulation:
ggplot(mod1_brm_prior_PPC, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Group", y = "Prior predicted RTs", color = "Group") +
  ggtitle("Prior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
mod1_data_means <- df_RT %>%
  group_by(group, blockcode) %>%
  summarise(
    mean_RT = mean(RTs_std, na.rm = TRUE),
    sd_RT = sd(RTs_std, na.rm = TRUE)
  )

#Plot simulated data:
ggplot(mod1_data_means, aes(x = blockcode, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Group", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  scale_x_discrete(labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod1_brm_prior, nsamples = 100)
```

For clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the prior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### RTs predicted by group × probability condition × trial type

We also need to check our prior predictions for model 2. Here, we model RTs by group, probability condition, and trial type; load in model and inspect output:

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between RTs and group, blockcode:
mod2_brm_prior <- brm(RTs_std ~ 0 + group:blockcode,
                      data = df_TT,
                      backend = "cmdstan",
                      prior = prior(normal(0, 1), class = "b"),
                      sample_prior = "only")

#Save the model output to a file:
#saveRDS(mod2_brm_prior, file = "mod2_brm_prior.rds")
```

```{r}
#Load the model output from file:
mod2_brm_prior <- readRDS("mod2_brm_prior.rds")

#Return model output:
summary(mod2_brm_prior)
```

Draw samples from the prior predictive simulation above:

```{r}
#Draw prior samples:
mod2_brm_PPC <- add_linpred_draws(
  mod2_brm_prior, 
  newdata = tibble(group = df_TT$group, blockcode = df_TT$blockcode, type = df_TT$type),
  ndraws = 100,
  value = 'Predicted_RT'
) |> 
  ungroup() |> 
  select(Predicted_RT, .draw, group, blockcode, type) |> 
  group_by(group, blockcode, type) |> 
  summarise(
    mean_RT = mean(Predicted_RT),
    sd_RT = sd(Predicted_RT),
    se_RT = sd_RT / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod2_brm_PPC)
```

Plot a prior predictive simulation:

```{r}
#Plot prior predictive simulation:
ggplot(mod2_brm_PPC, aes(x = type, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "Prior predicted RTs", color = "Group") +
  ggtitle("Prior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%"))) +
  theme_minimal()

#Compare to the simulated data, standardise RTs and derive means:
df_TT$RT_std <- standardize(df_TT$latency)
mod2_data_means <- df_TT %>%
  group_by(group, blockcode, type) %>%
  summarise(
    mean_RT = mean(RTs_std, na.rm = TRUE),
    sd_RT = sd(RTs_std, na.rm = TRUE)
  )

#Plot:
ggplot(mod2_data_means, aes(x = type, y = mean_RT, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_RT - sd_RT, ymax = mean_RT + sd_RT), width = 0.2) +
  labs(x = "Trial type", y = "RTs", color = "Group") +
  ggtitle("Simulated RTs for each probability condition by group") +
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#336666", "PDA" = "#cc99ff")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%"))) +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod2_brm_prior, nsamples = 100)
```

Again, for clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model priors (based on a 100 draws from the prior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### EDA-QA predicted by II

We also need to check our prior predictions for model 3 and 4. Here, we model EDA-QA scores as predicted by II for each neurotype (model3), and EDA-QA scores as predicted by II for each neurotype (model 4).

```{r}
#A model with broad priors that acommodate uncertainty pertaining to the relationship between II and EDA-QA scores:
mod3_brm_prior <- brm(EDAQ_std ~ II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
    sample_prior = "only",
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod3_brm_prior, file = "mod3_brm_prior.rds")

#A model with broad priors that acommodate uncertainty pertaining to the relationship between II and EDA-QA scores by neurotype:
mod4_brm_prior <- brm(EDAQ_std ~ neurotype * II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
    sample_prior = "only",
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod4_brm_prior, file = "mod4_brm_prior.rds")
```

Load in models and inspect output:

```{r}
#Load the model output from file:
mod3_brm_prior <- readRDS("mod3_brm_prior.rds")
mod4_brm_prior <- readRDS("mod4_brm_prior.rds")

#Return model output:
summary(mod3_brm_prior)
summary(mod4_brm_prior)
```

Draw samples from the prior predictive simulations above:

```{r}
#Draw prior samples for model 3:
mod3_brm_PPC <- add_linpred_draws(
  mod3_brm, 
  newdata = tibble(II_std = df_BM$II_std),
  ndraws = 100,
  value = 'Predicted_EDAQ'
) |> 
  ungroup() |> 
  select(Predicted_EDAQ, .draw, II_std) |> 
  group_by(II_std) |> 
  summarise(
    mean_Predicted_EDAQ = mean(Predicted_EDAQ),
    sd_Predicted_EDAQ = sd(Predicted_EDAQ),
    se_Predicted_EDAQ = sd_Predicted_EDAQ / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod3_brm_PPC)

#And again for model 4; draw prior samples:
mod4_brm_PPC <- add_linpred_draws(
  mod4_brm, 
  newdata = tibble(neurotype = df_BM$diagnosis, II_std = df_BM$II_std),
  ndraws = 100,
  value = 'Predicted_EDAQ'
) |> 
  ungroup() |> 
  select(Predicted_EDAQ, .draw, neurotype, II_std) |> 
  group_by(neurotype, II_std) |> 
  summarise(
    mean_Predicted_EDAQ = mean(Predicted_EDAQ),
    sd_Predicted_EDAQ = sd(Predicted_EDAQ),
    se_Predicted_EDAQ = sd_Predicted_EDAQ / sqrt(n()),
    .groups = "drop"
  )

#Print:
print(mod4_brm_PPC)
```

Plot a prior predictive simulation for model 3:

```{r}
#Plot prior predictive simulation:
ggplot(mod3_brm_PPC, aes(x = II_std, y = mean_Predicted_EDAQ)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  geom_ribbon(aes(ymin = mean_Predicted_EDAQ - sd_Predicted_EDAQ, ymax = mean_Predicted_EDAQ + sd_Predicted_EDAQ), alpha = 0.3, color = NA) +
  geom_point(data = df_BM, aes(x = II_std, y = EDAQ_std)) +
  labs(x = "II (std)", y = "Prior predicted EDA-QA scores (std)") +
  ggtitle("Prior predicted EDA-QA scores") +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod3_brm_prior, ndraws = 100)
```

Once again, for clarity, the pp_check() output displays a 'y' line that represents the distribution of our observed data, while the 'y_rep' lines represent the distribution of data simulated from the model priors (based on a 100 draws from the prior distribution of the model parameters). The observed data is made up of two Gaussian distributions, one centered around just below 0 (the NT group) and the other just above 0 (the ASC group). Prior predicted values seemingly overlap well with our simulated data; prior predictions assume the bulk of estimates to be approx. within the -3 to 3 range, which fits well with our simulated data values.

Now plot a prior predictive simulation for model 4:

```{r}
#Plot prior predictive simulation:
ggplot(mod4_brm_PPC, aes(x = II_std, y = mean_Predicted_EDAQ, color = neurotype)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, aes(fill = neurotype)) +
  geom_ribbon(aes(ymin = mean_Predicted_EDAQ - sd_Predicted_EDAQ, ymax = mean_Predicted_EDAQ + sd_Predicted_EDAQ, fill = neurotype), alpha = 0.3, color = NA) +
  geom_point(data = df_BM, aes(x = II_std, y = EDAQ_std, color = neurotype)) +
  labs(x = "II (std)", y = "Prior predicted EDA-QA scores (std)", color = "Neurotype", fill = "Neurotype") +
  ggtitle("Prior predicted EDA-QA scores by neurotype") +
  scale_color_manual(values = c("ASC" = "#336666", "NT" = "#cc99ff")) +
  scale_fill_manual(values = c("ASC" = "#336666", "NT" = "#cc99ff")) +
  theme_minimal()

#We can also run a prior predictive check that compare the distribution of prior predicted values to the distribution of observed values:
pp_check(mod4_brm_prior, ndraws = 100)
```

Again, the observed data is made up of two Gaussian distributions, one centered just below 0 (NT) and the other just above 0 (ASC). Again, prior predictions assume the bulk of estimates to be approx. within the -3 to 3 range, which fits well with our simulated data values.

# Computation and posterior distribution

## Details of the computation

All computations to derive posterior distributions for all four models were conducted using the brms() package (source details available in Package install_20.05.24.r- see packages and libraries); this includes ˆR and effective sample size (ESS).

Again, chapter 12 of the following webpage provides some useful guidance on how brms() models are specified: [Chapter 12 Bayesian estimation with brms \| An R companion to Statistics: data analysis and modelling (mspeekenbrink.github.io)](https://mspeekenbrink.github.io/sdam-r-companion/bayesian-estimation-with-brms.html)

We must check that the MCMC chains for every parameter have converged and are long enough to provide stable estimates.

Convergence here is indicated by ˆR, which must be near 1.0 to indicate convergence. When ˆR is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn't trust the samples.

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain not to the sample size of the data. The Bulk_ESS provides an estimate of the effective sample size for rank normalized values using split chains. Bulk-ESS is useful measure for sampling efficiency in the bulk of the distribution (related e.g. to efficiency of mean and median estimates), and is well defined even if the chains do not have finite mean or variance. The Tail_ESS estimates effective sample size of the tail by computing the minimum of effective sample sizes for 5% and 95% quantiles. Tail_ESS is useful measure for sampling efficiency in the tails of the distribution (related e.g. to efficiency of variance and tail quantile estimates). According to [Convergence and efficiency diagnostics for Markov Chains --- Rhat • rstan (mc-stan.org)](https://mc-stan.org/rstan/reference/Rhat.html), both Bulk_ESS and Tail_ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable.

### RTs predicted by group × probability condition

Once again, here is the output for model 1:

```{r}
#Load the model output from a file:
mod1_brm <- readRDS("mod1_brm_output.rds")

#Return model output:
summary(mod1_brm)

#We can also get model parameters:
get_prior(mod1_brm)
```

Both ˆR and Bulk_ESS and Tail_ESS (ESS) look ok; ˆR is around 1 for each parameter, and Bulk_ESS and Tail_ESS is \> 100.

We can also inspect the MCMC with a trace plot. A trace plot plots the samples in sequential order, joined by a line- we look for three things in these trace plots: (1) stationarity, (2) good mixing, and (3) convergence. Stationarity refers to the path of each chain staying within the same high-probability portion of the posterior distribution. Good mixing means that the chain rapidly explores the full region. It doesn't slowly wander, but rather rapidly zig-zags around. Convergence means that multiple, independent chains stick around the same region of high probability.

```{r}
mcmc_trace( mod1_brm )
```

All chains look good.

### RTs predicted by group × probability condition × trial type

And again for model 2:

```{r}
#Load the model output from file:
mod2_brm <- readRDS("mod2_brm_output.rds")

#Return model output:
summary(mod2_brm)

#We can also get model parameters:
get_prior(mod2_brm)
```

Both ˆR and Bulk_ESS and Tail_ESS (ESS) look ok; ˆR is around 1 for each parameter, and Bulk_ESS and Tail_ESS is \> 100.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod2_brm )
```

Again, all chains look good.

### EDA-QA predicted by II

And once more for model 3:

```{r}
#Load the model output from file:
mod3_brm <- readRDS("mod3_brm_output.rds")

#Return model output:
summary(mod3_brm)

#We can also get model parameters:
get_prior(mod3_brm)
```

Both ˆR and Bulk_ESS and Tail_ESS (ESS) look ok; ˆR is around 1 for each parameter, and Bulk_ESS and Tail_ESS is \> 100.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod3_brm )
```

All chains look good.

And finally for model 4:

```{r}
#Load the model output from file:
mod4_brm <- readRDS("mod4_brm_output.rds")

#Return model output:
summary(mod4_brm)

#We can also get model parameters:
get_prior(mod4_brm)
```

Both ˆR and Bulk_ESS and Tail_ESS (ESS) look ok; ˆR is around 1 for each parameter, and Bulk_ESS and Tail_ESS is \> 100.

We can also inspect the MCMC with a trace plot.

```{r}
mcmc_trace( mod4_brm )
```

All chains look good.

## Posterior predictive check

### RTs predicted by group × probability condition

Below we view the posterior predictions plotted against our data for model 1.

```{r}
#Perform a graphical posterior predictive check
pp_check(mod1_brm, ndraws = 100)
```

As with our prior predictive checks, the 'y' line of the pp_check output represents the distribution of our observed (simulated) data, while the 'y_rep' lines represent the distribution of data simulated from the model (based on a 100 draws from the posterior distribution of the model parameters). Both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### RTs predicted by group × probability condition × trial type

Below we view the posterior predictions plotted against our data for model 2.

```{r}
#Perform a graphical posterior predictive check:
pp_check(mod2_brm, ndraws = 100)
```

Again, both distributions appear to be normally distributed and centered around 0. Importantly, they closely overlap each other, which suggests that model predictions are consistent with the observed data.

### EDA-QA predicted by II

Below we view the posterior predictions plotted against our data for model 3 and 4.

```{r}
#Perform a graphical posterior predictive check:
pp_check(mod3_brm, ndraws = 100)
pp_check(mod4_brm, ndraws = 100)
```

Comparing the pp_checks for models 3 and 4, we can see that both models predict a range of values that are consistent with the observed data; in both model 3 and 4, both sets of distributions centre approx. around 0, and closely overlap each other, which suggests that model predictions are consistent with the observed data.

## Marginal posterior distribution

### RTs predicted by group × probability condition

Once again, we can inspect the output for model 1, this time we will consider the marginal posterior distribution of each parameter (central tendency and credible intervals):

```{r}
#Return model output:
summary(mod1_brm)

#Read in .csv to data frame:
mod1_brm_posterior <- read.csv("mod1_brm_posterior.csv")

#Create a new data frame to house the renamed posterior distributions:
mod1_brm_posterior_copy <- mod1_brm_posterior

#Rename parameters:
colnames(mod1_brm_posterior_copy) <- c("ASC:100%","NT:100%","PDA:100%","ASC:84%","NT:84%","PDA:84%","ASC:67%","NT:67%", "PDA:67%","ASC:33%","NT:33%","PDA:33%", "sigma", "priorlp", "lp")

#Convert the data frame to long format:
mod1_brm_posterior_long <- pivot_longer(mod1_brm_posterior_copy, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Filter out the variables lp and priorlp:
mod1_brm_posterior_long <- mod1_brm_posterior_long %>%
  filter(!Parameter %in% c("lp", "priorlp"))

#Define colors for groups:
colors <- c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666")
#colors <- c("NT" = "#B2E5E5", "ASC" = "#99B2B2", "PDA" = "#E5CCFF")

#Extract group information from parameter names:
mod1_brm_posterior_long$Group <- sub(":.*", "", mod1_brm_posterior_long$Parameter)

#:Reorder the parameters and flip the order:
mod1_brm_posterior_long$Parameter <- factor(mod1_brm_posterior_long$Parameter, 
                                            levels = rev(c("NT:100%","ASC:100%", "PDA:100%", 
                                                           "NT:84%", "ASC:84%", "PDA:84%", 
                                                           "NT:67%", "ASC:67%", "PDA:67%", 
                                                           "NT:33%", "ASC:33%", "PDA:33%", 
                                                           "sigma")))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

# Update Group assignment: Leave sigma as NA for Group
mod1_brm_posterior_long$Group <- case_when(
  grepl("^NT", mod1_brm_posterior_long$Parameter) ~ "NT",
  grepl("^ASC", mod1_brm_posterior_long$Parameter) ~ "ASC",
  grepl("^PDA", mod1_brm_posterior_long$Parameter) ~ "PDA",
  TRUE ~ NA_character_  # Keep sigma as NA
)

# Set the factor levels for Group (NT, ASC, PDA in the specified order)
mod1_brm_posterior_long$Group <- factor(mod1_brm_posterior_long$Group, 
                                         levels = c("NT", "ASC", "PDA"))

# Plot the data with the correct legend order and remove the NA (sigma) from the legend
ggplot(mod1_brm_posterior_long, aes(x = Estimate, y = Parameter, fill = Group)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, height = 3) +  
  scale_fill_manual(values = colors, 
                    guide = guide_legend(override.aes = list(fill = colors))) +  # Custom colors
  labs(x = "Model Estimated RTs (std)", y = "Probability Condition by Group", fill = "Group") +
  plot_title +
  theme_minimal() +  
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  
        panel.grid.minor = element_blank(),  
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12),
        plot.title = element_text(size = 16),
        legend.title = element_text(size = 14),
        legend.text = element_text(size = 12)) +  
  geom_vline(xintercept = 0, linetype = "dashed")  + 
  geom_hline(yintercept = c(4.5, 7.5, 10.5), linetype = "dashed", color = "gray50", size = 0.5, alpha = 0.5) +
  # Use scale_fill_manual with exclude NA from legend (use guide_legend() to remove NA)
  scale_fill_manual(values = colors, 
                    breaks = c("NT", "ASC", "PDA"),  # This excludes NA from the legend
                    guide = guide_legend(order = 1))  # Ensure correct ordering in the legend

ggsave('group_Cond_dist.png', plot = last_plot(), dpi = 300)
```

Note that we are primarily interested in the relationship between groups for each condition. We will consider the marginal posterior distribution of each parameter by condition (blockcode).

For condition 1, there is overlap between NT and PDA groups, and between ASC and PDA groups, all sitting well below 0. This suggests that all groups perform similarly in condition 1 (100% probability)- that these estimates are the smallest indicates that this is the condition with the fastest RTs.

For condition 2, there is overlap between NT and PDA groups, both groups sitting around zero. The ASC group sits just above the NT group overlapping the PDA group, just above 0. This suggests that while all groups performed slower in condition 2 (84% probability) compared to condition 1 (100% probability), the pattern of performance is different between groups in this condition; these estimates suggests that the NT and PDA groups performed comparatively and generated faster RTs than both the ASC group.

For condition 3, there is overlap between all three groups, all sitting above 0. Notably, estimates for the ASC group in condition 3 overlap with the estimates for the ASC estimates in condition 2. This suggests that the ASC group performed comparatively in both conditions.

For condition 4, there is overlap between all three groups again, all groups sitting well above zero. This suggests that all groups performed slowest in condition 4 (33% probability). The proximity of estimates suggests that all groups generated similar RTs in this condition.

Because we are specifically interested in PDA group performance in 67% and 84% probability conditions compared to the ASC and NT groups, we can compare group means:

```{r}
# Define a function to create the data frame
create_df <- function(param1, param2, group) {
  df <- tibble(
    Value = c(mod1_brm_posterior[[param1]], mod1_brm_posterior[[param2]], mod1_brm_posterior[[param1]] - mod1_brm_posterior[[param2]]),
    Parameter = rep(c(param1, param2, "Difference"), each = length(mod1_brm_posterior[[param1]])),
    Group = group
  )
  
  # Add a new column with simplified parameter names
  df <- df %>%
    mutate(SimpleParameter = case_when(
      Parameter == param1 ~ "TT",
      Parameter == param2 ~ "DT",
      Parameter == "Difference" ~ "Difference"
    ))
  
  return(df)
}

# Create the data frames
df1 <- create_df('b_groupPDA.blockcodeblock2', 'b_groupPDA.blockcodeblock3', "PDA")
df2 <- create_df('b_groupASC.blockcodeblock2', 'b_groupASC.blockcodeblock3', "ASC")
df3 <- create_df('b_groupNT.blockcodeblock2', 'b_groupNT.blockcodeblock3', "NT")

# Combine the data frames
df <- bind_rows(df1, df2, df3)

# Plot the distributions
ggplot(df, aes(x = Value, fill = SimpleParameter)) +
  geom_density(alpha = 0.5, color = NA) +  # Using geom_density for smooth distribution
  labs(x = "Model Estimated RTs (std)", y = "Density", 
       title = "Distributions of parameter estimates for 84% and 67% conditions
and their difference by group") +
  theme_minimal() +
  scale_fill_manual(values = c("Difference" = "#D7A8D2", "TT" = "#A3C4FC", "DT" = "#A3D8D1"),
                    labels = c("84%-67%", "67%", "84%"),
                    name = "Condition") + 
  facet_wrap(~ Group, ncol = 1, nrow = 3) +
  scale_y_continuous() +  # Density plots usually don't need custom y breaks
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )
ggsave('diff_Cond_dist.png', plot = last_plot(), dpi = 300)


#Also, return means and SDs for each group:
df %>%
  filter(Parameter == "Difference") %>%
  group_by(Group) %>%
  summarise(
    mean = mean(Value, na.rm = TRUE),
    sd = sd(Value, na.rm = TRUE),
    lower_hdi = hdi(Value, credMass = 0.95)[1],
    upper_hdi = hdi(Value, credMass = 0.95)[2]
  )
```

Visually comparing the differences between 84% and 67% probability conditions for each group, we can see that the ASC group has a slightly small difference of around 0.1 compared to PDA and NT groups (with differences in estimates of approx. 0.25 and 0.3, respectively). This suggests that the ASC group performed comparably in both conditions, as expected. For the NT and PDA groups, the difference between 84% and 67% probability conditions is larger, between 0.2 and 0.3. This pattern of differences between groups is contrary to our predictions that autistic and PDA individuals would demonstrate comparable diminished differentiation between probability conditions compared to neurotypical individuals.

We can also re-plot the posterior predicted RT values:

```{r}
#Read in .csv to data frame:
mod1_brm_preds <- read.csv("mod1_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod1_brm_preds <- mod1_brm_preds  %>%
  mutate(group = df_RT$group,
         blockcode = df_RT$blockcode)

#Create plot:
ggplot(mod1_brm_preds, aes(x = blockcode, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  geom_line() +
  #geom_smooth(method = "lm", se = FALSE, lwd = 0.25) +
  #geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +
  labs(x = "Probability of cue-target pair", y = "Posterior predicted RTs (std)", color = "Group") +
  ggtitle("Posterior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666"),  breaks = c("NT", "ASC", "PDA")) +
  scale_x_discrete(labels = c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%")) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )
ggsave('Cond_plot.png', plot = last_plot(), dpi = 300)
```

In summary, the plot above depicts posterior predicted RTs for each probability condition by group. For the NT group, as expected, the model predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions). Also as expected, the ASC group show a similar pattern of diminishing RTs as a function of probability, but the difference between 84% and 67% probability conditions is smaller than both the NT and PDA groups. Despite categorical links between PDA and ASC (and the disproportionate representation of PDA in our ASC group), the PDA group did not demonstrate a pattern of RTs characterised by a reduced difference between probability contingencies (specifically, between 67% and 84% conditions). Instead, much like for the NT group, the model predicts a patterns of diminishing RTs that are distributed in evenly spaced increments as a function of probability (i.e., faster RTs for high probability conditions and slower for low probability conditions).

### RTs predicted by group × probability condition × trial type

Repeat the process for model 2; we will consider the marginal posterior distribution of each parameter (central tendency and credible intervals).

```{r}
#Return model output:
summary(mod2_brm)

#Read in .csv to data frame:
mod2_brm_posterior <- read.csv("mod2_brm_posterior.csv")

#Create a new data frame to house the renamed posterior distributions:
mod2_brm_posterior_copy <- mod2_brm_posterior

#Rename parameters:
colnames(mod2_brm_posterior_copy) <- c("ASC:84%:DT","NT:84%:DT","PDA:84%:DT","ASC:67%:DT","NT:67%:DT", "PDA:67%:DT","ASC:33%:DT","NT:33%:DT","PDA:33%:DT","ASC:84%:TT","NT:84%:TT","PDA:84%:TT","ASC:67%:TT","NT:67%:TT", "PDA:67%:TT","ASC:33%:TT","NT:33%:TT","PDA:33%:TT","sigma")

#Filter out 'lprior' and 'lp__':
mod2_brm_posterior_copy <- mod2_brm_posterior_copy[, c("ASC:84%:DT","NT:84%:DT","PDA:84%:DT","ASC:67%:DT","NT:67%:DT", "PDA:67%:DT","ASC:33%:DT","NT:33%:DT","PDA:33%:DT","ASC:84%:TT","NT:84%:TT","PDA:84%:TT","ASC:67%:TT","NT:67%:TT", "PDA:67%:TT","ASC:33%:TT","NT:33%:TT","PDA:33%:TT","sigma")]

#Convert the data frame to long format:
mod2_brm_posterior_long <- pivot_longer(mod2_brm_posterior_copy, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Reorder the parameters and flip the order:
mod2_brm_posterior_long$Parameter <- factor(mod2_brm_posterior_long$Parameter, 
                                            levels = rev(c("NT:84%:TT","NT:84%:DT","ASC:84%:TT","ASC:84%:DT","PDA:84%:TT","PDA:84%:DT",
                                                           "NT:67%:TT","NT:67%:DT","ASC:67%:TT","ASC:67%:DT", "PDA:67%:TT","PDA:67%:DT",
                                                           "NT:33%:TT", "NT:33%:DT","ASC:33%:TT","ASC:33%:DT","PDA:33%:TT","PDA:33%:DT",
                                                            "sigma")))

#Define colors for groups:
colors <- c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666")
#colors <- c("NT" = "#B2E5E5", "ASC" = "#99B2B2", "PDA" = "#E5CCFF")

#Extract group information from parameter names:
mod2_brm_posterior_long$Group <- sub(":.*", "", mod2_brm_posterior_long$Parameter)

# Set the factor levels for Group (NT, ASC, PDA in the specified order)
mod2_brm_posterior_long$Group <- factor(mod2_brm_posterior_long$Group, 
                                         levels = c("NT", "ASC", "PDA"))

#Plot marginal posterior distributions with specified colors:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(mod2_brm_posterior_long, aes(x = Estimate, y = Parameter, fill = Group)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, , height = 3) +  # Adjusting the size of the points
  labs(x = "Model Estimated RTs (std)", y = "Probability Condition and 
TT/DT trials by Group", fill = "Group") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "right",
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)) +  # Increase font size of axis titles) +  # Add a single line at 0 on the x-axis
  geom_vline(xintercept = 0, linetype = "dashed") +  # Add a dashed line at x = 0
  geom_hline(yintercept = c(7.5, 13.5), linetype = "dashed", color = "gray50", size = 0.5, alpha = 0.5) +
  scale_fill_manual(values = colors, 
                    breaks = c("NT", "ASC", "PDA"),  # This excludes NA from the legend
                    guide = guide_legend(order = 1))  # Ensure correct ordering in the legend
ggsave('group_Cond_TTDT_dist.png', plot = last_plot(), dpi = 300)
```

Here, we are primarily interested in the relationship between each trial type across conditions for each group.

For the NT and PDA groups, the difference between parameter estimates for each TT-DT combination gets smaller as a function of probability conditions. In conditions 2 (84% probability), the DT parameter estimates sit above 0 for both groups, while the TT parameter estimates sits just below and straddling 0, respectively. In conditions 3 (67% probability), the DT and TT parameter estimates overlap sitting above above 0 for both groups. Finally, in conditions 4 (33% probability), both the DT and TT parameter estimates sit around 0.5 for both groups.

For the ASC group, the difference between parameter estimates for each TT-DT combination remains relatively static across probability conditions. In conditions 2 (84% probability), the DT and TT parameter estimates sit just above 0. There is substantial overlap between DT and TT trials, indicative that there is little difference in estimates for TT and DT parameter estimates. In conditions 3 (67% probability), the DT and TT parameter estimates overlap again, sitting further above 0. Finally, in conditions 4 (33% probability), the DT and TT overlap, both estimates sitting around 0.5.

Here we are interested in the difference between trial types (TT and DT) in each probability condition for each group. Thus, we compare RT means for each trial type by condition and group:

Visually comparing the differences between trial types in each condition, we can see that for the NT and PDA groups, the difference between tt and dt gets gradually smaller as probability diminishes; in the 84% probability condition, the difference is around 0.25, for the 67% probability condition, it drops to around 0.2, and for the 33% probability condition, the difference is around 0.125. This pattern suggests that the NT and PDA groups demonstrate effective learning; using global statistics to estimate the probabilistic contingencies of a cue-target relationship.

The ASC group demonstrates more stable differences between trial type across probability conditions; in all conditions, the difference between tt and dt remains around 0.125. This suggests that the ASC group may have employed a pattern of learning characterised by the use of only the previous trial to predict the current trial outcome (e.g., a distractor trial leads to the prediction of another distractor trial).

\[No difference in 33% - likely due to ceiling effect? But expected differences between 67%-84%; AC show the smallest difference (albeit negative, which is a bit funky), while PDA showing the same difference but positive (I don't know how to reconcile this with the plot above), and the NT show a slightly bigger difference compared to the PDA and ASC groups - not sure what to make of this\].

We can also re-plot the posterior predicted RT values:

```{r}
#Read in .csv to data frame:
mod2_brm_preds <- read.csv("mod2_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod2_brm_preds <- mod2_brm_preds  %>%
  mutate(group = df_TT$group,
         blockcode = df_TT$blockcode,
         type = df_TT$type)

#Create plot  showing predicted RTs for each trial type by probability condition and group:
ggplot(mod2_brm_preds, aes(x = type, y = Estimate, color = group, group = group)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  labs(x = "Trial type", y = "Posterior predicted RTs (std)", color = "Group") +
  ggtitle("Posterior predicted RTs for each probability condition by group") +
  scale_color_manual(values = c( "NT" = "#66cccc", "ASC" = "#cc99ff", "PDA" = "#336666"),  breaks = c("NT", "ASC", "PDA")) +
  facet_wrap(~blockcode, nrow = 1, labeller = as_labeller(c("block1" = "100%", "block2" = "84%", "block3" = "67%", "block4" = "33%"))) +
  scale_x_discrete(labels = c("dt" = "DT", "tt" = "TT")) + 
  theme_minimal()  +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )
ggsave('Cond_TTDT_plot.png', plot = last_plot(), dpi = 300)
```

In summary, the plot above depicts posterior predicted RTs for each trial type in each probability condition by group. For the NT and PDA groups, the difference between RTs for DT and TT trials diminishes as a function of probability condition; participants are faster in TT compared to DT trials while the probability of a cue indicating a target is high (e.g., 84% probability condition) and diminishing differences in RTs between TT and DT trials when probability of a cue indicating a target is low (e.g., 33% probability condition).

The ASC group shows relatively static differences between RTs for DT and TT trials across probability conditions; the difference between TT and DT trials remains around 0.125 irrespective of probability contingency and the relative number of corresponding TT and DT trials.

### EDA-QA predicted by II

Plot parameter distributions with means and credible intervals for model 3:

```{r}
#Return model output:
summary(mod3_brm)

#Read in .csv to data frame:
mod3_brm_posterior <- read.csv("mod3_brm_posterior.csv")

#Filter out 'lprior' and 'lp__':
mod3_brm_posterior <- mod3_brm_posterior[, c("b_Intercept", "b_II_std", "sigma")]

#Convert the data frame to long format:
mod3_brm_posterior_long <- pivot_longer(mod3_brm_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'II_std', and 'sigma':
mod3_brm_posterior_long$Parameter <- recode(mod3_brm_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_II_std" = "II_std",
                                            "sigma" = "sigma")

#Reorder the parameters and flip the order:
mod3_brm_posterior_long$Parameter <- factor(mod3_brm_posterior_long$Parameter, 
                                            levels = rev(c("Intercept", "II_std", "sigma")))

#Plot marginal posterior distributions:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(mod3_brm_posterior_long, aes(x = Estimate, y = Parameter)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, height = 0.8, fill = "#9ED4D1") +  # Use one color for all distributions
  labs(x = "Parameter estimates", y = "Parameters") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "none",  # Remove the legend
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black")  # Add a dashed line at x = 0
ggsave('SI_PDA_dist.png', plot = last_plot(), dpi = 300)
```

The intercept represents the expected value of EDA-QA scores when II is zero. Because both II and EDA-QA scores are standardised here, this can be interpreted as when II values are at their mean, EDA-QA scores are expected to be at their mean.

II is the estimated effect of II on EDA-QA scores, it is the expected change in EDA-QA scores for a one-unit increase in II, assuming all other predictors are held constant. Specifically, for each one-unit increase in II, we expect EDA-QA scores to increase by approx. 0, assuming all other variables are held constant. This suggests there is little evidence of a relationship between EDA-QA scores and II. The 95% compatibility interval for this estimate includes zero (-0.17 to 0.15), further suggesting that the effect could plausibly be zero.

All of the above can be observed by plotting the marginal posterior distributions against the simulated data:

```{r}
#Read in .csv to data frame:
mod3_brm_preds <- read.csv("mod3_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod3_brm_preds <- mod3_brm_preds %>%
  mutate( II = df_BM$II_std,
  EDAQ = df_BM$EDAQ_std
)

#Plot:
ggplot(mod3_brm_preds, aes(x = II, y = EDAQ)) +
  geom_point(color = "#339999") +
  geom_smooth(method = "lm", se = TRUE, aes(y = Estimate), color = "#339999") +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, fill = "#339999") +
  labs(x = "Insensitivity Index (std)", y = "EDA-QA scores (std)") +
  ggtitle("Relationship between insensitivity to probability contingencies 
and EDA-QA scores") +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
ggsave('SI_PDA_plot.png', plot = last_plot(), dpi = 300)
```

In summary, model 3 suggests that there is no relationship between II and EDA-QA scores. The credible intervals for the II parameter seem to suggests that the model is confident in its estimation. However, this model does not account for any between differences in II or EDA-QA scores between neurotype. To rectify this omission, let us turn to model 4.

Now, plot parameter distributions with means and credible intervals for model 4:

```{r}
#Return model output:
summary(mod4_brm)

#Read in .csv to data frame:
mod4_brm_posterior <- read.csv("mod4_brm_posterior.csv")

#Filter out 'lprior' and 'lp__':
mod4_brm_posterior <- mod4_brm_posterior[, c("b_Intercept", "b_II_std", "b_neurotypeNT", "b_neurotypeNT.II_std", "sigma")]

#Convert the data frame to long format:
mod4_brm_posterior_long <- pivot_longer(mod4_brm_posterior, 
                                        cols = everything(), 
                                        names_to = "Parameter", 
                                        values_to = "Estimate")

#Rename parameters to 'Intercept', 'II_std', and 'sigma':
mod4_brm_posterior_long$Parameter <- recode(mod4_brm_posterior_long$Parameter,
                                            "b_Intercept" = "Intercept",
                                            "b_II_std" = "II_std",
                                            "b_neurotypeNT" = "NeurotypeNT",
                                            "b_neurotypeNT.II_std" = "Interaction",
                                            "sigma" = "sigma")

#Reorder the parameters and flip the order:
mod4_brm_posterior_long$Parameter <- factor(mod4_brm_posterior_long$Parameter, 
                                            levels = rev(c("Intercept", "II_std", "NeurotypeNT", "Interaction", "sigma")))

#Plot marginal posterior distributions:
plot_title <- ggtitle("Posterior distributions",
                      subtitle = "with means and 95% HDI")

ggplot(mod4_brm_posterior_long, aes(x = Estimate, y = Parameter)) +
  stat_halfeye(point_interval = mean_hdi, .width = c(0.95), size = 0.5, height = 0.8, fill = "#9ED4D1") +  # Use one color for all distributions
  labs(x = "Parameter estimates", y = "Parameters") +
  plot_title +
  theme_minimal() +  # Using theme_minimal() instead of theme_clean()
  theme(legend.position = "none",  # Remove the legend
        panel.grid.major = element_blank(),  # Remove major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)) +
  geom_vline(xintercept = 0, linetype = "dashed")  # Add a dashed line at x = 0
ggsave('SI_PDA_group_dist.png', plot = last_plot(), dpi = 300)
```

The intercept represents the expected value of EDA-QA scores when II are at their mean (which is 0 for standardized variables) and neurotype is at its reference level (ASC). The estimate is 0.5, indicating that for ASC, if II is at its mean, EDA-QA scores are expected to be 0.5 standard deviations above its mean.

II is the estimated change in EDA-QA scores for a one standard deviation increase in II, when neurotype is ASC. The estimate is -0.06, which is close to zero, suggesting a weak relationship between EDA-QA scores and II for ASC. However, the 95% compatibility interval for this estimate includes zero (-0.22 to 0.1), indicating that the data does not provide strong evidence for an interaction effect.

neurotypeNT represents the estimated difference in EDA-QA scores between NT and ASC, when II are at their mean. The estimate is -1.09, suggesting that NT is associated with a 1.09 standard deviation decrease in EDA-QA scores compared to ASC.

Finally, neurotypeNT.II represents the interaction term, representing the difference in the effect of II on EDA-QA scores between NT and ASC. The estimate is 0.05, suggesting that the relationship between II and EDA-QA scores is slightly stronger for NT compared to ASC. However, the 95% compatibility interval for this estimate includes zero (-0.23 to 0.33), indicating that the data does not provide strong evidence for an interaction effect.

All of the above can be observed by plotting the marginal posterior distributions against the simulated data:

```{r}
#Read in .csv to data frame:
mod4_brm_preds <- read.csv("mod4_brm_preds.csv")

#Add group and blockcode variables to the data frame:
mod4_brm_preds <- mod4_brm_preds %>%
  mutate( II = df_BM$II_std,
  EDAQ = df_BM$EDAQ_std,
  neurotype = df_BM$diagnosis
)

# Ensure neurotype is a factor with levels in the desired order (NT, ASC)
mod4_brm_preds$neurotype <- factor(mod4_brm_preds$neurotype, levels = c("NT", "ASC"))

ggplot(mod4_brm_preds, aes(x = II, y = EDAQ)) +
  # Points and lines use color aesthetic
  geom_point(aes(color = neurotype)) +  # color points by neurotype
  geom_smooth(method = "lm", se = TRUE, aes(color = neurotype), fill = NA) +  # color the line by neurotype
  # Ribbon uses fill aesthetic
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = neurotype), alpha = 0.3, color = NA) +  # fill the ribbon by neurotype
  labs(x = "Insensitivity Index (std)", 
       y = "EDA-QA scores (std)", 
       fill = "Neurotype", color = "Neurotype") +  # Adding labels for color and fill
  ggtitle("Relationship between insensitivity to probability contingencies 
and EDA-QA scores by neurotype") +
  scale_fill_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff")) +  # Set colors for NT and ASC
  scale_color_manual(values = c("NT" = "#66cccc", "ASC" = "#cc99ff")) +  # Set colors for NT and ASC
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 16)
  )
ggsave('SI_PDA_group_plot.png', plot = last_plot(), dpi = 300)
```

In summary, we failed to find a relationship between II and EDA-QA scores in either ASC or NT. Furthermore, credible intervals for the II parameter seem to suggests that the model is confident in its estimation. In addition, as expected, ASC demonstrated higher EDA-QA scores compared to NT. This is in keeping with the notion that the prevalence of PDA is likely higher in the the autistic population compared to the general population.

# Sensitivity analysis

"A sensitivity analysis explores how changes in assumptions influence inference. If none of the alternative assumptions you consider have much impact on inference, that's worth reporting. Likewise, if the alternatives you consider do have an important impact on inference, that's also worth reporting. The same sort of advice follows for other modeling assumptions: likelihoods, linear models, priors, and even how the model is fit to data" - McElreath (2020).

### RTs predicted by group × probability condition

We will begin with three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod1_brm_default <- brm(RTs_std ~ 0 + group:blockcode,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(mod1_brm_default, file = "mod1_brm_default_output.rds")

#Fit the model with narrow priors:
mod1_brm_narrow <- brm(RTs_std ~ 0 + group:blockcode,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 0.2), class = "b"))

#Save the model output to a file:
#saveRDS(mod1_brm_narrow, file = "mod1_brm_narrow_output.rds")

#Fit the model with broad priors:
mod1_brm_broad <- brm(RTs_std ~ 0 + group:blockcode,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 2), class = "b"))

#Save the model output to a file:
#saveRDS(mod1_brm_broad, file = "mod1_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod1_brm_default <- readRDS("mod1_brm_default_output.rds")
mod1_brm_narrow <- readRDS("mod1_brm_narrow_output.rds")
mod1_brm_broad <- readRDS("mod1_brm_broad_output.rds")

#Return model output:
models <- list(mod1_brm_default, mod1_brm_narrow, mod1_brm_broad)

#Iterate summary over the models:
for(i in 1:length(models)) {
  # Print model summary
  print(summary(models[[i]]))
}
```

Visualise model output:

```{r}
#Put models in a list:
models <- list(Default = mod1_brm_default, Narrow = mod1_brm_narrow, Broad = mod1_brm_broad)

#Create a data frame to hold coefficients:
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(models)) {
  coefs <- fixef(models[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

#Order coefficients by the order they appear in the first model, then reverse the order:
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(models[[1]]))))

#Plot using ggplot:
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()

```

All three models show comparable estimates for each combination of group and condition (blockcode); we can conclude that our priors are acceptable to capture our expected range of parameter estimates.

### RTs predicted by group × probability condition × trial type

Again, begin with three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod2_brm_default <- brm(RTs_std ~ 0 + group:blockcode:type,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 1), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_default, file = "mod2_brm_default_output.rds")

#Fit the model with narrow priors:
mod2_brm_narrow <- brm(RTs_std ~ 0 + group:blockcode:type,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 0.2), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_narrow, file = "mod2_brm_narrow_output.rds")

#Fit the model with broad priors:
mod2_brm_broad <- brm(RTs_std ~ 0 + group:blockcode:type,
            data = df_TT,
            backend = "cmdstan",
            prior = prior(normal(0, 2), class = "b"))

#Save the model output to a file:
#saveRDS(mod2_brm_broad, file = "mod2_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod2_brm_default <- readRDS("mod2_brm_default_output.rds")
mod2_brm_narrow <- readRDS("mod2_brm_narrow_output.rds")
mod2_brm_broad <- readRDS("mod2_brm_broad_output.rds")

#Return model output:
model2s <- list(mod2_brm_default, mod2_brm_narrow, mod2_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model2s)) {
  # Print model summary
  print(summary(model2s[[i]]))
}
```

Visualise model output:

```{r}
#Put models in a list:
model2s <- list(Default = mod2_brm_default, Narrow = mod2_brm_narrow, Broad = mod2_brm_broad)

#Create a data frame to hold coefficients:
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(model2s)) {
  coefs <- fixef(model2s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

#Order coefficients by the order they appear in the first model, then reverse the order:
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model2s[[1]]))))

#Plot using ggplot:
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()

```

All three models show comparable estimates for each combination of group, condition (blockcode), and trial type (label); we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.

### EDA-QA predicted by II

We will begin with model 3, constructing three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod3_brm_default <- brm(EDAQ_std ~ II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)
#Save the model output to a file:
#saveRDS(mod3_brm_default, file = "mod3_brm_default_output.rds")

#Fit the model with narrow priors:
mod3_brm_narrow <- brm(EDAQ_std ~ II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 0.2), class = "Intercept"),
      prior(normal(0, 0.2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod3_brm_narrow, file = "mod3_brm_narrow_output.rds")

#Fit the model with broad priors:
mod3_brm_broad <- brm(EDAQ_std ~ II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 2), class = "Intercept"),
      prior(normal(0, 2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod3_brm_broad, file = "mod3_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod3_brm_default <- readRDS("mod3_brm_default_output.rds")
mod3_brm_narrow <- readRDS("mod3_brm_narrow_output.rds")
mod3_brm_broad <- readRDS("mod3_brm_broad_output.rds")

#Return model output:
model3s <- list(mod3_brm_default, mod3_brm_narrow, mod3_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model3s)) {
  # Print model summary
  print(summary(model3s[[i]]))
}
```

Visualise model output:

```{r}
#Put models in a list:
model3s <- list(Default = mod3_brm_default, Narrow = mod3_brm_narrow, Broad = mod3_brm_broad)

#Create a data frame to hold coefficients:
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(model3s)) {
  coefs <- fixef(model3s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

#Order coefficients by the order they appear in the first model, then reverse the order:
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model3s[[1]]))))

#Plot using ggplot:
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All three models show comparable parameter estimates for intercept and II. The limited knowledge available regarding the relationship between PDA and sensitivity to changes in probability lead us to choose priors that would be accepting of a broad range of estimates. Taken together, we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.

Now repeat the process for model 4 by constructing three models that consider different priors for b; default (the prior we used in our model), narrow, and broad:

```{r}
#Fit the model with default priors:
mod4_brm_default <- brm(EDAQ_std ~ neurotype * II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 1), class = "Intercept"),
      prior(normal(0, 1), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)
#Save the model output to a file:
#saveRDS(mod4_brm_default, file = "mod4_brm_default_output.rds")

#Fit the model with narrow priors:
mod4_brm_narrow <- brm(EDAQ_std ~ neurotype * II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 0.2), class = "Intercept"),
      prior(normal(0, 0.2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod4_brm_narrow, file = "mod4_brm_narrow_output.rds")

#Fit the model with broad priors:
mod4_brm_broad <- brm(EDAQ_std ~ neurotype * II_std,
    data = df_BM,
    backend = "cmdstan", 
    family = gaussian(),
    prior = c(
      prior(normal(0, 2), class = "Intercept"),
      prior(normal(0, 2), class = "b"),
      prior(exponential(1), class = "sigma")
    ),
  control = list(adapt_delta = 0.99)
)

#Save the model output to a file:
#saveRDS(mod4_brm_broad, file = "mod4_brm_broad_output.rds")
```

We can visualise the output of the above models to compare the impact of differing prior assumptions for b:

```{r}
#Load models output from files:
mod4_brm_default <- readRDS("mod4_brm_default_output.rds")
mod4_brm_narrow <- readRDS("mod4_brm_narrow_output.rds")
mod4_brm_broad <- readRDS("mod4_brm_broad_output.rds")

#Return model output:
model4s <- list(mod4_brm_default, mod4_brm_narrow, mod4_brm_broad)

#Iterate summary over the models:
for(i in 1:length(model4s)) {
  # Print model summary
  print(summary(model4s[[i]]))
}
```

Visualise model output:

```{r}
#Put models in a list:
model4s <- list(Default = mod4_brm_default, Narrow = mod4_brm_narrow, Broad = mod4_brm_broad)

#Create a data frame to hold coefficients:
df <- data.frame(Model = character(), Parameters = character(), Estimate = numeric(), SE = numeric())

#Extract coefficients and add to data frame:
for (model_name in names(model3s)) {
  coefs <- fixef(model4s[[model_name]])
  temp_df <- data.frame(Model = model_name, Parameters = rownames(coefs), Estimate = coefs[, 1], SE = coefs[, 2])
  df <- rbind(df, temp_df)
}

#Order coefficients by the order they appear in the first model, then reverse the order:
df$Parameters <- factor(df$Parameters, levels = rev(rownames(fixef(model4s[[1]]))))

#Plot using ggplot:
ggplot(df, aes(x = Estimate, y = Parameters, color = Model)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Estimate - SE, xmax = Estimate + SE), height = 0.2) +
  labs(title = "Comparison of Coefficients Across Models", x = "Estimate", y = "Parameters") +
  scale_color_manual(values = c("Broad" = "#66cccc", "Default" = "#336666", "Narrow" = "#cc99ff")) +
  theme_minimal()
```

All three models show comparable parameter estimates for II and the interaction parameter, neurotypeNT:II_std. The narrow priors model estimates for intercept and the difference in EDA-QA scores between NT and ASC (i.e., the parameter neurotypeNT) sit a little closer to 0 than do the default and broad prior models. This isn't surprising; the narrow priors are constraining estimates, pulling them closer to 0. Again, the limited knowledge available regarding the relationship between PDA and sensitivity to changes in probability lead us to choose priors that would be accepting of a broad range of estimates; we can conclude that our default priors are appropriate to capture our expected range of parameter estimates.
